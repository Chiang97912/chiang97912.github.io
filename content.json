{"meta":{"title":"Peter's Blog","subtitle":null,"description":"Stay hungry, stay foolish!","author":"Peter Chiang","url":"http://chiang97912.github.io","root":"/"},"pages":[{"title":"关于","date":"2017-08-25T10:34:44.000Z","updated":"2025-12-13T12:29:09.992Z","comments":true,"path":"about/index.html","permalink":"http://chiang97912.github.io/about/index.html","excerpt":"","text":"简介 主业是算法工程师，每天和 NLP、大模型 “斗智斗勇”，致力于让 AI 更懂人类语言；副业是读书爱好者、旅行打卡选手、野生摄影师。既能写得动复杂代码，也能扛得动相机走四方，还能在书本里蹲一下午。博客主打一个 “技术硬核 + 生活柔软”，欢迎来我的小天地一起唠技术、聊生活！ 联系方式 E-Mail: chiang97912@gmail.com Github: https://github.com/Chiang97912"},{"title":"分类","date":"2017-08-20T06:56:53.000Z","updated":"2020-08-01T03:20:40.000Z","comments":false,"path":"categories/index.html","permalink":"http://chiang97912.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-08-25T10:29:35.000Z","updated":"2020-08-01T03:20:40.000Z","comments":true,"path":"tags/index.html","permalink":"http://chiang97912.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"大模型位置编码笔记","slug":"大模型位置编码笔记","date":"2026-01-04T15:21:35.000Z","updated":"2026-01-16T02:30:31.893Z","comments":true,"path":"2026/01/04/大模型位置编码笔记/","permalink":"http://chiang97912.github.io/2026/01/04/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%AC%94%E8%AE%B0/","excerpt":"旋转位置编码 论文：RoFormer: Enhanced Transformer with Rotary Position Embedding 代码：https://github.com/ZhuiyiTechnology/roformer","text":"旋转位置编码 论文：RoFormer: Enhanced Transformer with Rotary Position Embedding 代码：https://github.com/ZhuiyiTechnology/roformer RoPE旋转位置编码的出发点是“通过绝对位置编码的方式实现相对位置编码”，Transformer位置编码是加性的正弦位置编码（位置向量和词向量是直接相加的），然而RoPE采用的是乘性的正弦位置编码，因为自注意力机制计算过程中需要通过query词向量qmq_mqm​和key词向量knk_nkn​点乘计算注意力得分，因此想到可以先用绝对位置编码表示qmq_mqm​和knk_nkn​的位置信息，需要找到函数g(xm,xn,m−n)g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n)g(xm​,xn​,m−n)使得qmq_mqm​和knk_nkn​点乘之后可以计算得到qmq_mqm​和knk_nkn​的相对位置信息m-n： &lt;fq(xm,m),fk(xn,n)&gt;=g(xm,xn,m−n)&lt; f_{q} \\left( \\boldsymbol{x}_{m} , m \\right) , f_{k} \\left( \\boldsymbol{x}_{n} , n \\right) &gt;=g \\left( \\boldsymbol{x}_{m} , \\boldsymbol{x}_{n} , m-n \\right) &lt;fq​(xm​,m),fk​(xn​,n)&gt;=g(xm​,xn​,m−n) 为了实现这个目标，需要引入复数和旋转矩阵（复数在空间上可以表示旋转），假定现在词嵌入向量的维度是两维d = 2 ，然后RoPE利用2维度平面上的向量的几何性质，再结合复数的性质，神奇般的找到了满足上述等式的 f 和 g ，其形式如下： fq(xm,m)=(Wqxm)eimθfk(xn,n)=(Wkxn)einθg(xm,xn,m−n)=Re⁡[(Wqxm)(Wkxn)∗ei(m−n)θ]\\begin{aligned} &amp; f_q\\left(\\boldsymbol{x}_m, m\\right)=\\left(\\boldsymbol{W}_q \\boldsymbol{x}_m\\right) e^{i m \\theta} \\\\ &amp; f_k\\left(\\boldsymbol{x}_n, n\\right)=\\left(\\boldsymbol{W}_k \\boldsymbol{x}_n\\right) e^{i n \\theta} \\\\ &amp; g\\left(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n\\right)=\\operatorname{Re}\\left[\\left(\\boldsymbol{W}_q \\boldsymbol{x}_m\\right)\\left(\\boldsymbol{W}_k \\boldsymbol{x}_n\\right)^* e^{i(m-n) \\theta}\\right] \\end{aligned} ​fq​(xm​,m)=(Wq​xm​)eimθfk​(xn​,n)=(Wk​xn​)einθg(xm​,xn​,m−n)=Re[(Wq​xm​)(Wk​xn​)∗ei(m−n)θ]​ 为了实现这个目标，需要引入复数和旋转矩阵（复数在空间上可以表示旋转），假定现在词嵌入向量的维度是两维d = 2 ，然后RoPE利用2维度平面上的向量的几何性质，再结合复数的性质，神奇般的找到了满足上述等式的 f 和 g ，其形式如下： fq(xm,m)=(Wqxm)eimθfk(xn,n)=(Wkxn)einθg(xm,xn,m−n)=Re⁡[(Wqxm)(Wkxn)∗ei(m−n)θ]\\begin{aligned} &amp; f_q\\left(\\boldsymbol{x}_m, m\\right)=\\left(\\boldsymbol{W}_q \\boldsymbol{x}_m\\right) e^{i m \\theta} \\\\ &amp; f_k\\left(\\boldsymbol{x}_n, n\\right)=\\left(\\boldsymbol{W}_k \\boldsymbol{x}_n\\right) e^{i n \\theta} \\\\ &amp; g\\left(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n\\right)=\\operatorname{Re}\\left[\\left(\\boldsymbol{W}_q \\boldsymbol{x}_m\\right)\\left(\\boldsymbol{W}_k \\boldsymbol{x}_n\\right)^* e^{i(m-n) \\theta}\\right] \\end{aligned} ​fq​(xm​,m)=(Wq​xm​)eimθfk​(xn​,n)=(Wk​xn​)einθg(xm​,xn​,m−n)=Re[(Wq​xm​)(Wk​xn​)∗ei(m−n)θ]​ 其中 eimθe^{imθ}eimθ是复数的指数形式（可以通过旋转矩阵表示），WqW_qWq​和WkW_kWk​表示q和k输入注意力线性层权重，Re 表示复数的实部。 RoPE将θ设置为： θ={θi=10000−2id,i∈[1,2,…,d2]}\\theta=\\left\\{\\theta_{i}=1 0 0 0 0^{\\frac{-2 i} {d}} , i \\in[ 1 , 2 , \\ldots, \\frac{d} {2} ] \\right\\} θ={θi​=10000d−2i​,i∈[1,2,…,2d​]} fq(xm,m)f_q(\\boldsymbol{x}_m, m)fq​(xm​,m)可以表示成下面的式子： fq(xm,m)=(cos⁡mθ−sin⁡mθsin⁡mθcos⁡mθ)(Wq(1,1)Wq(1,2)Wq(2,1)Wq(2,2))(xm(1)xm(2))=(cos⁡mθ−sin⁡mθsin⁡mθcos⁡mθ)(qm(1)qm(2))\\begin{aligned} f_q\\left(\\boldsymbol{x}_m, m\\right) &amp; =\\left(\\begin{array}{cc} \\cos m \\theta &amp; -\\sin m \\theta \\\\ \\sin m \\theta &amp; \\cos m \\theta \\end{array}\\right)\\left(\\begin{array}{ll} W_q^{(1,1)} &amp; W_q^{(1,2)} \\\\ W_q^{(2,1)} &amp; W_q^{(2,2)} \\end{array}\\right)\\binom{x_m^{(1)}}{x_m^{(2)}} \\\\ &amp; =\\left(\\begin{array}{cc} \\cos m \\theta &amp; -\\sin m \\theta \\\\ \\sin m \\theta &amp; \\cos m \\theta \\end{array}\\right)\\binom{q_m^{(1)}}{q_m^{(2)}} \\end{aligned} fq​(xm​,m)​=(cosmθsinmθ​−sinmθcosmθ​)(Wq(1,1)​Wq(2,1)​​Wq(1,2)​Wq(2,2)​​)(xm(2)​xm(1)​​)=(cosmθsinmθ​−sinmθcosmθ​)(qm(2)​qm(1)​​)​ 而前面说的能让乘性正弦位置编码绝对位置编码在q和k向量点乘之后具有相对位置信息m-n的就是旋转矩阵： Rθ=[cos⁡θ−sin⁡θsin⁡θcos⁡θ]\\mathbf{R}_\\theta=\\left[\\begin{array}{cc} \\cos \\theta &amp; -\\sin \\theta \\\\ \\sin \\theta &amp; \\cos \\theta \\end{array}\\right] Rθ​=[cosθsinθ​−sinθcosθ​] 复数旋转矩阵证明：https://chatgpt.com/s/t_6948207f1b7c8191935745beadbc563b fk(xn,n)f_k(\\boldsymbol{x}_n, n)fk​(xn​,n)可以表示成下面的式子： fk(xm,m)=(cos⁡mθ−sin⁡mθsin⁡mθcos⁡mθ)(Wk(1,1)Wk(1,2)Wk(2,1)Wk(2,2))(xm(1)xm(2))=(cos⁡mθ−sin⁡mθsin⁡mθcos⁡mθ)(km(1)km(2))\\begin{gathered} f_k\\left(\\boldsymbol{x}_m, m\\right)=\\left(\\begin{array}{cc} \\cos m \\theta &amp; -\\sin m \\theta \\\\ \\sin m \\theta &amp; \\cos m \\theta \\end{array}\\right)\\left(\\begin{array}{ll} W_k^{(1,1)} &amp; W_k^{(1,2)} \\\\ W_k^{(2,1)} &amp; W_k^{(2,2)} \\end{array}\\right)\\binom{x_m^{(1)}}{x_m^{(2)}} \\\\ =\\left(\\begin{array}{cc} \\cos m \\theta &amp; -\\sin m \\theta \\\\ \\sin m \\theta &amp; \\cos m \\theta \\end{array}\\right)\\binom{k_m^{(1)}}{k_m^{(2)}} \\end{gathered} fk​(xm​,m)=(cosmθsinmθ​−sinmθcosmθ​)(Wk(1,1)​Wk(2,1)​​Wk(1,2)​Wk(2,2)​​)(xm(2)​xm(1)​​)=(cosmθsinmθ​−sinmθcosmθ​)(km(2)​km(1)​​)​ g(xm,xn,m−n)g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n)g(xm​,xn​,m−n)可以表示如下： g(xm,xn,m−n)=(qm(1)qm(2))(cos⁡((m−n)θ)−sin⁡((m−n)θ)sin⁡((m−n)θ)cos⁡((m−n)θ))(kn(1)kn(2))g\\left(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n\\right)=\\left(\\begin{array}{cc} q_m^{(1)} &amp; q_m^{(2)} \\end{array}\\right)\\left(\\begin{array}{cc} \\cos ((m-n) \\theta) &amp; -\\sin ((m-n) \\theta) \\\\ \\sin ((m-n) \\theta) &amp; \\cos ((m-n) \\theta) \\end{array}\\right)\\binom{k_n^{(1)}}{k_n^{(2)}} g(xm​,xn​,m−n)=(qm(1)​​qm(2)​​)(cos((m−n)θ)sin((m−n)θ)​−sin((m−n)θ)cos((m−n)θ)​)(kn(2)​kn(1)​​) g(xm,xn,m−n)g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n)g(xm​,xn​,m−n)公式证明： 将fk(xn,n)f_k(\\boldsymbol{x}_n, n)fk​(xn​,n)和fk(xn,n)f_k(\\boldsymbol{x}_n, n)fk​(xn​,n)写成向量形式： fq(xm,m)=[qm(1)cos⁡(mθ)−qm(2)sin⁡(mθ),qm(2)cos⁡(mθ)+qm(1)sin⁡(mθ)]fk(xn,n)=[kn(1)cos⁡(nθ)−kn(2)sin⁡(nθ),kn(2)cos⁡(nθ)+kn(1)sin⁡(nθ)]\\begin{aligned} f_q\\left(x_m, m\\right) &amp; =\\left[q_m^{(1)} \\cos (m \\theta)-q_m^{(2)} \\sin (m \\theta), q_m^{(2)} \\cos (m \\theta)+q_m^{(1)} \\sin (m \\theta)\\right] \\\\ f_k\\left(x_n, n\\right) &amp; =\\left[k_n^{(1)} \\cos (n \\theta)-k_n^{(2)} \\sin (n \\theta), k_n^{(2)} \\cos (n \\theta)+k_n^{(1)} \\sin (n \\theta)\\right] \\end{aligned} fq​(xm​,m)fk​(xn​,n)​=[qm(1)​cos(mθ)−qm(2)​sin(mθ),qm(2)​cos(mθ)+qm(1)​sin(mθ)]=[kn(1)​cos(nθ)−kn(2)​sin(nθ),kn(2)​cos(nθ)+kn(1)​sin(nθ)]​ g(xm,xn,m−n)g(\\boldsymbol{x}_m, \\boldsymbol{x}_n, m-n)g(xm​,xn​,m−n)可以表示如下向量形式： &lt;fq(xm,m),fk(xn,n)&gt;=(qm(1)cos⁡(mθ)−qm(2)sin⁡(mθ))(kn(1)cos⁡(nθ)−kn(2)sin⁡(nθ))+(qm(2)cos⁡(mθ)+qm(1)sin⁡(mθ))(kn(2)cos⁡(nθ)+kn(1)sin⁡(nθ))=qm(1)cos⁡(mθ)kn(1)cos⁡(nθ)−qm(1)cos⁡(mθ)kn(2)sin⁡(nθ)−qm(2)sin⁡(mθ)kn(1)cos⁡(nθ)+qm(2)sin⁡(mθ)kn(2)sin⁡(nθ)+qm(2)cos⁡(mθ)kn(2)cos⁡(nθ)+qm(2)cos⁡(mθ)kn(1)sin⁡(nθ)+qm(1)sin⁡(mθ)kn(2)cos⁡(nθ)+qm(1)sin⁡(mθ)kn(1)sin⁡(nθ)\\begin{gathered} &lt;f_q\\left(x_m, m\\right), f_k\\left(x_n, n\\right)&gt; \\\\ =\\left(q_m^{(1)} \\cos (m \\theta)-q_m^{(2)} \\sin (m \\theta)\\right)\\left(k_n^{(1)} \\cos (n \\theta)-k_n^{(2)} \\sin (n \\theta)\\right) \\\\ +\\left(q_m^{(2)} \\cos (m \\theta)+q_m^{(1)} \\sin (m \\theta)\\right)\\left(k_n^{(2)} \\cos (n \\theta)+k_n^{(1)} \\sin (n \\theta)\\right) \\\\ =q_m^{(1)} \\cos (m \\theta) k_n^{(1)} \\cos (n \\theta)-q_m^{(1)} \\cos (m \\theta) k_n^{(2)} \\sin (n \\theta) \\\\ -q_m^{(2)} \\sin (m \\theta) k_n^{(1)} \\cos (n \\theta)+q_m^{(2)} \\sin (m \\theta) k_n^{(2)} \\sin (n \\theta) \\\\ +q_m^{(2)} \\cos (m \\theta) k_n^{(2)} \\cos (n \\theta)+q_m^{(2)} \\cos (m \\theta) k_n^{(1)} \\sin (n \\theta) \\\\ +q_m^{(1)} \\sin (m \\theta) k_n^{(2)} \\cos (n \\theta)+q_m^{(1)} \\sin (m \\theta) k_n^{(1)} \\sin (n \\theta) \\end{gathered} &lt;fq​(xm​,m),fk​(xn​,n)&gt;=(qm(1)​cos(mθ)−qm(2)​sin(mθ))(kn(1)​cos(nθ)−kn(2)​sin(nθ))+(qm(2)​cos(mθ)+qm(1)​sin(mθ))(kn(2)​cos(nθ)+kn(1)​sin(nθ))=qm(1)​cos(mθ)kn(1)​cos(nθ)−qm(1)​cos(mθ)kn(2)​sin(nθ)−qm(2)​sin(mθ)kn(1)​cos(nθ)+qm(2)​sin(mθ)kn(2)​sin(nθ)+qm(2)​cos(mθ)kn(2)​cos(nθ)+qm(2)​cos(mθ)kn(1)​sin(nθ)+qm(1)​sin(mθ)kn(2)​cos(nθ)+qm(1)​sin(mθ)kn(1)​sin(nθ)​ 背景知识-三角函数恒等变换公式： sin⁡(a+b)=sin⁡acos⁡b+cos⁡asin⁡b,sin⁡(a−b)=sin⁡acos⁡b−cos⁡asin⁡b,cos⁡(a+b)=cos⁡acos⁡b−sin⁡asin⁡b,cos⁡(a−b)=cos⁡acos⁡b+sin⁡asin⁡b.\\begin{aligned} \\sin(a+b) &amp;= \\sin a \\cos b + \\cos a \\sin b, \\\\ \\sin(a-b) &amp;= \\sin a \\cos b - \\cos a \\sin b, \\\\ \\cos(a+b) &amp;= \\cos a \\cos b - \\sin a \\sin b, \\\\ \\cos(a-b) &amp;= \\cos a \\cos b + \\sin a \\sin b. \\end{aligned} sin(a+b)sin(a−b)cos(a+b)cos(a−b)​=sinacosb+cosasinb,=sinacosb−cosasinb,=cosacosb−sinasinb,=cosacosb+sinasinb.​ 首先，把上面第二点的式子整理一下，总计8项，为了把qk相关的项提取出来，第1项和8项合并处理、第2项和7项合并处理、第3项和6项合并处理、第4项和5项合并处理： &lt;fq(xm,m),fk(xn,n)&gt;=qm(1)kn(1)(cos⁡(mθ)cos⁡(nθ)+sin⁡(mθ)sin⁡(nθ))+qm(1)kn(2)(−cos⁡(mθ)sin⁡(nθ)+sin⁡(mθ)cos⁡(nθ))+qm(2)kn(1)(−sin⁡(mθ)cos⁡(nθ)+cos⁡(mθ)sin⁡(nθ))+qm(2)kn(2)(sin⁡(mθ)sin⁡(nθ)+cos⁡(mθ)cos⁡(nθ))=qm(1)kn(1)cos⁡((m−n)θ)+qm(1)kn(2)sin⁡((m−n)θ)−qm(2)kn(1)sin⁡((m−n)θ)+qm(2)kn(2)cos⁡((m−n)θ)=(qm(1)kn(1)+qm(2)kn(2))cos⁡((m−n)θ)+(qm(1)kn(2)−qm(2)kn(1))sin⁡((m−n)θ)=(qm(1)kn(1)+qm(2)kn(2))cos⁡((m−n)θ)−(qm(2)kn(1)−qm(1)kn(2))sin⁡((m−n)θ)=g(xm,xn,m−n)\\begin{gathered} &lt;f_q\\left(x_m, m\\right), f_k\\left(x_n, n\\right)&gt; \\\\ =q_m^{(1)} k_n^{(1)}(\\cos (m \\theta) \\cos (n \\theta)+\\sin (m \\theta) \\sin (n \\theta)) \\\\ +q_m^{(1)} k_n^{(2)}(-\\cos (m \\theta) \\sin (n \\theta)+\\sin (m \\theta) \\cos (n \\theta)) \\\\ +q_m^{(2)} k_n^{(1)}(-\\sin (m \\theta) \\cos (n \\theta)+\\cos (m \\theta) \\sin (n \\theta)) \\\\ +q_m^{(2)} k_n^{(2)}(\\sin (m \\theta) \\sin (n \\theta)+\\cos (m \\theta) \\cos (n \\theta)) \\\\ =q_m^{(1)} k_n^{(1)} \\cos ((m-n) \\theta) \\\\ +q_m^{(1)} k_n^{(2)} \\sin ((m-n) \\theta) \\\\ -q_m^{(2)} k_n^{(1)} \\sin ((m-n) \\theta) \\\\ +q_m^{(2)} k_n^{(2)} \\cos ((m-n) \\theta) \\\\ =\\left(q_m^{(1)} k_n^{(1)}+q_m^{(2)} k_n^{(2)}\\right) \\cos ((m-n) \\theta)+\\left(q_m^{(1)} k_n^{(2)}-q_m^{(2)} k_n^{(1)}\\right) \\sin ((m-n) \\theta) \\\\ =\\left(q_m^{(1)} k_n^{(1)}+q_m^{(2)} k_n^{(2)}\\right) \\begin{array}{c} \\cos ((m-n) \\theta)-\\left(q_m^{(2)} k_n^{(1)}-q_m^{(1)} k_n^{(2)}\\right) \\sin ((m-n) \\theta) \\\\ =g\\left(x_m, x_n, m-n\\right) \\end{array} \\end{gathered} &lt;fq​(xm​,m),fk​(xn​,n)&gt;=qm(1)​kn(1)​(cos(mθ)cos(nθ)+sin(mθ)sin(nθ))+qm(1)​kn(2)​(−cos(mθ)sin(nθ)+sin(mθ)cos(nθ))+qm(2)​kn(1)​(−sin(mθ)cos(nθ)+cos(mθ)sin(nθ))+qm(2)​kn(2)​(sin(mθ)sin(nθ)+cos(mθ)cos(nθ))=qm(1)​kn(1)​cos((m−n)θ)+qm(1)​kn(2)​sin((m−n)θ)−qm(2)​kn(1)​sin((m−n)θ)+qm(2)​kn(2)​cos((m−n)θ)=(qm(1)​kn(1)​+qm(2)​kn(2)​)cos((m−n)θ)+(qm(1)​kn(2)​−qm(2)​kn(1)​)sin((m−n)θ)=(qm(1)​kn(1)​+qm(2)​kn(2)​)cos((m−n)θ)−(qm(2)​kn(1)​−qm(1)​kn(2)​)sin((m−n)θ)=g(xm​,xn​,m−n)​​ 至此证明了位置m的query向量和位置n的key向量的内积就是函数g(xm,xn,m−n)g(x_m, x_n, m-n)g(xm​,xn​,m−n)。 RoPE位置编码只作用于q和k，不直接作用于v，但注意力权重（由 RoPE 的 q/k 计算而来）包含位置信息，最终 v 的加权输出会间接包含位置信息。 由于内积满足线性叠加性，因此任意偶数维的RoPE，我们都可以表示为二维情形的拼接 由于Rm的稀疏性，所以直接用矩阵乘法来实现会很浪费算力，所以在计算时采用逐位相乘再相加的方式进行： (q0q1q2q3⋮qd−2qd−1)⊗(cos⁡mθ0cos⁡mθ0cos⁡mθ1cos⁡mθ1⋮cos⁡mθd/2−1cos⁡mθd/2−1)+(−q1q0−q3q2⋮−qd−1qd−2)⊗(sin⁡mθ0sin⁡mθ1sin⁡mθ1sin⁡mθ1⋮sin⁡mθd/2−1sin⁡mθd/2−1)\\left(\\begin{array}{c} q_0 \\\\ q_1 \\\\ q_2 \\\\ q_3 \\\\ \\vdots \\\\ q_{d-2} \\\\ q_{d-1} \\end{array}\\right) \\otimes\\left(\\begin{array}{c} \\cos m \\theta_0 \\\\ \\cos m \\theta_0 \\\\ \\cos m \\theta_1 \\\\ \\cos m \\theta_1 \\\\ \\vdots \\\\ \\cos m \\theta_{d / 2-1} \\\\ \\cos m \\theta_{d / 2-1} \\end{array}\\right)+\\left(\\begin{array}{c} -q_1 \\\\ q_0 \\\\ -q_3 \\\\ q_2 \\\\ \\vdots \\\\ -q_{d-1} \\\\ q_{d-2} \\end{array}\\right) \\otimes\\left(\\begin{array}{c} \\sin m \\theta_0 \\\\ \\sin m \\theta_1 \\\\ \\sin m \\theta_1 \\\\ \\sin m \\theta_1 \\\\ \\vdots \\\\ \\sin m \\theta_{d / 2-1} \\\\ \\sin m \\theta_{d / 2-1} \\end{array}\\right) ⎝⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎛​q0​q1​q2​q3​⋮qd−2​qd−1​​⎠⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎞​⊗⎝⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎛​cosmθ0​cosmθ0​cosmθ1​cosmθ1​⋮cosmθd/2−1​cosmθd/2−1​​⎠⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎞​+⎝⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎛​−q1​q0​−q3​q2​⋮−qd−1​qd−2​​⎠⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎞​⊗⎝⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎛​sinmθ0​sinmθ1​sinmθ1​sinmθ1​⋮sinmθd/2−1​sinmθd/2−1​​⎠⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎞​ 其中⊗是逐位对应相乘。 参考文章： Transformer升级之路：2、博采众长的旋转式位置编码 Transformer升级之路：4、二维位置的旋转式位置编码 Transformer升级之路：17、多模态位置编码的简单思考 一文通透位置编码：从标准位置编码、复数、欧拉公式到旋转位置编码RoPE(含其推导与代码实现) 一文通透位置编码：从标准位置编码、复数、欧拉公式到旋转位置编码RoPE(含其推导与代码实现)","categories":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"位置编码","slug":"位置编码","permalink":"http://chiang97912.github.io/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"},{"name":"相对位置编码","slug":"相对位置编码","permalink":"http://chiang97912.github.io/tags/%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"},{"name":"旋转位置编码","slug":"旋转位置编码","permalink":"http://chiang97912.github.io/tags/%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"},{"name":"RoPE","slug":"RoPE","permalink":"http://chiang97912.github.io/tags/RoPE/"}]},{"title":"大模型强化学习笔记","slug":"大模型强化学习笔记","date":"2025-12-29T11:21:42.000Z","updated":"2026-01-11T17:03:23.386Z","comments":true,"path":"2025/12/29/大模型强化学习笔记/","permalink":"http://chiang97912.github.io/2025/12/29/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"PPO 论文：Proximal Policy Optimization Algorithms 代码：https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py","text":"PPO 论文：Proximal Policy Optimization Algorithms 代码：https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py 背景知识 策略梯度方法 策略梯度方法直接对策略进行参数化，并通过最大化预期回报来优化策略参数。常用的策略梯度估计器为： g^=E^t[∇θlog⁡πθ(at∣st)A^t]{\\hat{g}}={\\hat{\\mathbb{E}}}_{t} \\Big[ \\nabla_{\\theta} \\operatorname{l o g} \\pi_{\\theta} ( a_{t} \\mid s_{t} ) {\\hat{A}}_{t} \\Big] g^​=E^t​[∇θ​logπθ​(at​∣st​)A^t​] 其中： E^t[⋅]\\hat{\\mathbb{E}}_t[⋅]E^t​[⋅] 表示对采样数据的经验平均； π表示随机策略； A^t\\hat{A}_tA^t​ 是优势函数（Advantage Function）的估计，衡量动作ata_tat​相对于基准策略在状态sts_tst​ 下的好坏。 信赖域方法（TRPO） 为了解决策略更新不稳定的问题，信赖域策略优化（Trust Region Policy Optimization，TRPO） 被提出，核心思想是在策略更新时加入约束，限制新旧策略之间的差异。 TRPO解决以下优化问题： maximize⁡θE^t[πθ(at∣st)πθold (at∣st)A^t] subject to E^t[KL⁡[πθold (⋅∣st),πθ(⋅∣st)]]≤δ\\begin{array}{cl} \\underset{\\theta}{\\operatorname{maximize}} &amp; \\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t\\right] \\\\ \\text { subject to } &amp; \\hat{\\mathbb{E}}_t\\left[\\operatorname{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid s_t\\right), \\pi_\\theta\\left(\\cdot \\mid s_t\\right)\\right]\\right] \\leq \\delta \\end{array} θmaximize​ subject to ​E^t​[πθold ​​(at​∣st​)πθ​(at​∣st​)​A^t​]E^t​[KL[πθold ​​(⋅∣st​),πθ​(⋅∣st​)]]≤δ​ 其中： πθoldπ_{θ_{old}}πθold​​是旧的策略； KL[⋅|⋅] 表示两个策略在状态st 下的KL散度（Kullback-Leibler Divergence）； δ 是预设的阈值，限制策略更新的幅度。 TRPO通过复杂的二阶优化方法（如共轭梯度算法）求解上述约束优化问题。虽然TRPO在实践中表现稳定，但其实现较为复杂，且不易与一些神经网络结构（如共享参数的网络、包含随机噪声的网络）结合。 重要性采样是一种数据利用技巧。首先用策略模型πθ去和环境交互，产生一系列轨迹，然后用新的相同策略模型πθ′和环境交互，产生一系列轨迹。然后用这些轨迹去多次更新之前的策略模型πθ。但是我们并不是直接用这些轨迹对我们需要求取的策略模型进行迭代，而是需要对采样动作的概率进行比例放缩。重要性采样的优势在于隔离采样的轨迹和目标策略模型的关系。 参考文章：https://zhuanlan.zhihu.com/p/1895169683969274027 近端策略优化（PPO） PPO旨在保留TRPO性能稳定的优点，同时简化实现，并提高数据利用率。 截断的代理目标（Clipped Surrogate Objective） PPO引入了截断的代理目标函数，限制策略更新的幅度，避免过大的策略变化。 首先，定义概率比率（Probability Ratio）： rt(θ)=πθ(at∣st)πθold(at∣st)r_t(\\theta)=\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\mathrm{old}}}\\left(a_t \\mid s_t\\right)} rt​(θ)=πθold​​(at​∣st​)πθ​(at​∣st​)​ 在TRPO中，优化的代理目标函数为： LCPI(θ)=E^t[πθ(at∣st)πθold (at∣st)A^t]=E^t[rt(θ)A^t].L^{C P I}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t\\right]=\\hat{\\mathbb{E}}_t\\left[r_t(\\theta) \\hat{A}_t\\right] . LCPI(θ)=E^t​[πθold ​​(at​∣st​)πθ​(at​∣st​)​A^t​]=E^t​[rt​(θ)A^t​]. 上标 CPI （conservative policy iteration）指的是保守策略迭代。 为了避免策略的过度更新，PPO引入了截断函数，定义新的目标函数： LCLIP(θ)=E^t[min⁡(rt(θ)A^t,clip⁡(rt(θ),1−ϵ,1+ϵ)A^t)]L^{C L I P}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\operatorname{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right] LCLIP(θ)=E^t​[min(rt​(θ)A^t​,clip(rt​(θ),1−ϵ,1+ϵ)A^t​)] 其中： clip(rt(θ),1−ϵ,1+ϵ)clip(r_t(θ), 1−ϵ, 1+ϵ)clip(rt​(θ),1−ϵ,1+ϵ) 表示将rt(θ)r_t(θ)rt​(θ) 限制在[1−ϵ, 1+ϵ] 区间内； ϵ 是超参数，控制截断的幅度，通常取值为0.1或0.2。 截断目标函数的作用： 限制策略更新幅度：通过截断rt(θ)r_t(θ)rt​(θ) ，防止策略在一次更新中发生过大的变化。 保留优化灵活性：当优势AtA^tAt 为正时，允许rt(θ)r_t(θ)rt​(θ) 增加，但不超过 1+ϵ ；当AtA^tAt 为负时，允许rt(θ)r_t(θ)rt​(θ) 减少，但不低于 1−ϵ 。 提高数据利用率：由于有了稳定的目标函数，可以对同一批数据进行多次更新，而不会导致策略崩溃。 自适应的KL散度惩罚（Adaptive KL Penalty Coefficient） 另一种控制策略更新的方法是加入KL散度惩罚项，并自适应调整惩罚系数。 定义目标函数： LKLPEN(θ)=E^t[πθ(at∣st)πθold (at∣st)A^t−βKL⁡[πθold (⋅∣st),πθ(⋅∣st)]]L^{K L P E N}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t-\\beta \\operatorname{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid s_t\\right), \\pi_\\theta\\left(\\cdot \\mid s_t\\right)\\right]\\right] LKLPEN(θ)=E^t​[πθold ​​(at​∣st​)πθ​(at​∣st​)​A^t​−βKL[πθold ​​(⋅∣st​),πθ​(⋅∣st​)]] 其中： β 是KL散度的惩罚系数； 通过调整β ，控制策略更新的幅度。 自适应调整β： 当实际KL散度d 小于目标值d_targ 的1.5倍时，减小β ，允许更大的策略更新； 当d 大于d_targ 的1.5倍时，增大β ，限制策略更新的幅度。 这样，β 会根据策略更新的情况，自适应地调整，维持策略变化在合适的范围内。 算法（Algorithm） PPO算法的主要步骤如下： 采样数据：使用当前策略πθoldπ_{θ_{old}}πθold​​ 与环境交互，收集数据（状态、动作、奖励等）。 计算优势函数：使用 广义优势估计（Generalized Advantage Estimation，GAE） 等方法，计算优势函数A^t\\hat{A}_tA^t​。 构建目标函数：根据选择，构建LCLIP(θ) 或LKLPEN(θ) 。 多轮优化：使用随机梯度上升法（如Adam），对目标函数进行K 轮优化。 更新策略：将θoldθ_{old}θold​ 更新为优化后的θ 。 重复上述步骤：迭代进行，直到达到训练目标。 优势估计量是**广义优势估计（Generalized Advantage Estimation，GAE）**的截断版本,即 A^t=δt+(γλ)δt+1+⋯+⋯+(γλ)T−t+1δT−1, where δt=rt+γV(st+1)−V(st)\\begin{aligned} &amp; \\hat{A}_t=\\delta_t+(\\gamma \\lambda) \\delta_{t+1}+\\cdots+\\cdots+(\\gamma \\lambda)^{T-t+1} \\delta_{T-1}, \\\\ &amp; \\text { where } \\quad \\delta_t=r_t+\\gamma V\\left(s_{t+1}\\right)-V\\left(s_t\\right) \\end{aligned} ​A^t​=δt​+(γλ)δt+1​+⋯+⋯+(γλ)T−t+1δT−1​, where δt​=rt​+γV(st+1​)−V(st​)​ 其中： rtr_trt​：当前奖励。 γ：折扣因子。 V(st)V(s_t)V(st​)：当前状态价值估计。 V(st+1)V(s_t+1)V(st​+1)：下一状态价值估计。 δt=rt+γV(st+1)−V(st)δ_t=r_t + γV(s_t+1) - V(s_t)δt​=rt​+γV(st​+1)−V(st​) 是TD-error（Temporal Difference error，时序差分误差）。 价值函数 t时刻状态s的总收益 = 身处状态s能带来的即时收益 + 从状态s出发后能带来的未来收益 Vt=Rt+γVt+1V_t=R_t+\\gamma V_{t+1} Vt​=Rt​+γVt+1​ 其中： VtV_tVt​ ： t 时刻的总收益，注意这个收益蕴涵了“即时”和“未来”的概念 RtR_tRt​ ： t 时刻的即时收益 Vt+1V_{t+1}Vt+1​ ： t+1 时刻的总收益，注意这个收益蕴涵了“即时”和“未来”的概念。而 Vt+1 对 Vt 来说就是“未来”。 γ ：折扣因子。它决定了我们在多大程度上考虑将“未来收益”纳入“当下收益”。 TD-error（Temporal Difference error，时序差分误差） 是强化学习中衡量当前价值估计与一步引导（bootstrap）目标之间偏差的核心量，用来指导参数更新。 TD误差有两种主要形式： ‌1-step TD误差‌：只考虑一步的未来奖励和价值，如上述公式所示。 ‌n-step TD误差‌：考虑多步的未来奖励和价值，例如n-step TD(λ)算法，可以更平滑地估计长期价值。 广义优势估计 优势就是在当前状态下，这次选这个动作，是不是比平常更赚。 优势 = 实际表现 − 预测表现 rt+γV(st+1)r_t + γV(s_t+1)rt​+γV(st​+1)表示实际表现，它表示当前奖励 + 折扣后的未来价值，V(st)V(s_t)V(st​) 表示预测表现 优势计算： def get_advantages_and_returns(self, values, rewards, start): &quot;&quot;&quot; Adopted from https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py#L134 没有引入GAE前的t时刻的优势值： detal_t = r_t + gamma * V_t+1 - V_t 其中： - r_t表示t时刻的即时收益 - V_t+1表示未来时刻的预期收益 - r_t + gamma * V_t+1可理解成t时刻的实际预期收益 - V_t可理解成t时刻的预估预期收益（是模型，例如critic model自己估算出来的） 引入GAE后的t时刻的优势值： A_t = delta_t + gamma * lambda * A_t+1 粗暴理解为在t时刻时，不仅考虑当下优势，还考虑了未来的优势 为了知道A_t, 我们得知道A_t+1，所以在本算法中采取了从后往前做动态规划求解的方法，也即： 假设T是最后一个时刻，则有A_T+1 = 0, 所以有: A_T = delta_T 知道了A_T, 就可以依次往前倒推，把A_t-1, A_t-2之类都算出来了 引入GAE后t时刻的实际预期收益 returns_t = A_t + V_t = delta_t + gamma * lambda * A_t+1 + V_t = r_t + gamma * V_t+1 - V_t + gamma * lambda * A_t+1 + V_t = r_t + gamma * (V_t+1 + lambda * A_t+1) 注意，这里不管是advantages还是returns，都只算response的部分 &quot;&quot;&quot; # Adopted from https://github.com/CarperAI/trlx/blob/main/trlx/models/modeling_ppo.py#L134 lastgaelam = 0 advantages_reversed = [] length = rewards.size()[-1] # 注意这里用了reversed，是采取从后往前倒推计算的方式 for t in reversed(range(start, length)): nextvalues = values[:, t + 1] if t &lt; length - 1 else 0.0 delta = rewards[:, t] + self.gamma * nextvalues - values[:, t] lastgaelam = delta + self.gamma * self.lam * lastgaelam advantages_reversed.append(lastgaelam) advantages = torch.stack(advantages_reversed[::-1], dim=1) # 优势 returns = advantages + values[:, start:] # 实际收益 # values: 预期收益 return advantages.detach(), returns 每次迭代，N个并行的参与者收集T个时间步长的数据。然后计算在这些数据上的损失，并使用minibatch SGD对K epoch进行优化。 Algorithm 1 PPO, Actor-Critic Style for iteration=1, 2, . . . do for actor=1, 2, . . . , N do Run policy πθold in environment for T timesteps Compute advantage estimates Aˆ1, ... , AˆT end for Optimize surrogate L wrt θ, with K epochs and minibatch size M ≤ NT θold ← θ end for 在强化学习里trajectory / episode / rollout 都是指“状态–动作–奖励序列” 实验与结果 在连续控制任务中，作者比较了以下几种方法： 无截断或惩罚：直接优化LCPI(θ)L^{CPI}(θ)LCPI(θ) 。 截断的目标函数：使用不同ϵ 值的LCLIP(θ)L^{CLIP}(θ)LCLIP(θ) 。 固定的KL惩罚：使用固定β 的LKLPEN(θ)L^{KLPEN}(θ)LKLPEN(θ) 。 自适应的KL惩罚：动态调整β 的LKLPEN(θ)L^{KLPEN}(θ)LKLPEN(θ) 。 实验结果表明： 使用截断的目标函数，尤其是ϵ=0.2 时，算法性能最好。 自适应KL惩罚的方法次之。 固定KL惩罚的方法表现较差。 无截断或惩罚的方法由于策略更新过大，导致性能不稳定，甚至下降。 附录 PPO其他实现： MoSS论文：Secrets of RLHF in Large Language Models Part I: PPO MoSS代码：https://github.com/OpenLMLab/MOSS-RLHF 参考文章： 【 猛猿】图解大模型RLHF系列之：人人都能看懂的PPO原理与源码解读 【 猛猿】人人都能看懂的RL-PPO理论知识 [论文阅读 粗读]- 强化学习和 RLHF 中的 PPO 算法 论文精读：Proximal Policy Optimization Algorithms近端策略优化算法 【论文解读】PPO：近端策略优化算法（Proximal Policy Optimization Algorithms） GRPO 论文：DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models 代码：https://github.com/huggingface/trl/blob/main/trl/trainer/grpo_trainer.py PPO 需加载‌策略模型、‌价值模型、‌参考模型和‌奖励模型（共4个），显存占用高。‌‌ GRPO 仅需策略模型、参考模型和奖励模型（共3个），资源消耗更低。‌‌ GRPO相比PPO优化： 无价值模型优化：GRPO 通过比较组内的响应消除了对评论模型的需求，从而显著减少了计算开销。 相对评估：GRPO 不依赖外部评估者，而是使用群体动力学来评估某个响应相对于同一批次中其他响应的表现如何。 高效训练：通过关注基于群体的优势，GRPO 简化了奖励估计过程，使其更快、更适用于大型模型。 GRPO计算流程： 采样一组输出并计算每个输出的奖励。 对组内奖励进行归一化处理。 使用归一化后的奖励计算优势函数。 通过最大化目标函数更新策略模型。 迭代训练，逐步优化策略模型。 PPO目标函数： JPPO(θ)=E[q∼P(Q),o∼πθold (O∣q)]1∣o∣∑t=1∣0∣min⁡[πθ(ot∣q,o&lt;t)πθold (ot∣q,o&lt;t)At,clip⁡(πθ(ot∣q,o&lt;t)πθold (ot∣q,o&lt;t),1−ε,1+ε)At],\\mathcal{J}_{P P O}(\\theta)=\\mathbb{E}\\left[q \\sim P(Q), o \\sim \\pi_{\\theta_{\\text {old }}}(O \\mid q)\\right] \\frac{1}{|o|} \\sum_{t=1}^{|0|} \\min \\left[\\frac{\\pi_\\theta\\left(o_t \\mid q, o_{&lt;t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_t \\mid q, o_{&lt;t}\\right)} A_t, \\operatorname{clip}\\left(\\frac{\\pi_\\theta\\left(o_t \\mid q, o_{&lt;t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_t \\mid q, o_{&lt;t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) A_t\\right], JPPO​(θ)=E[q∼P(Q),o∼πθold ​​(O∣q)]∣o∣1​t=1∑∣0∣​min[πθold ​​(ot​∣q,o&lt;t​)πθ​(ot​∣q,o&lt;t​)​At​,clip(πθold ​​(ot​∣q,o&lt;t​)πθ​(ot​∣q,o&lt;t​)​,1−ε,1+ε)At​], GRPO目标函数： JGRPO(θ)=E[q∼P(Q),{oi}i=1G∼πθold (O∣q)]1G∑i=1G1∣oi∣∑t=1∣oi∣{min⁡[πθ(oi,t∣q,oi,&lt;t)πθold (oi,t∣q,oi,&lt;t)A^i,t,clip⁡(πθ(oi,t∣q,oi,&lt;t)πθold (oi,t∣q,oi,&lt;t),1−ε,1+ε)A^i,t]−βDKL[πθ∥πref]},\\begin{aligned} \\mathcal{J}_{G R P O}(\\theta) &amp; =\\mathbb{E}\\left[q \\sim P(Q),\\left\\{o_i\\right\\}_{i=1}^G \\sim \\pi_{\\theta_{\\text {old }}}(O \\mid q)\\right] \\\\ &amp; \\frac{1}{G} \\sum_{i=1}^G \\frac{1}{\\left|o_i\\right|} \\sum_{t=1}^{\\left|o_i\\right|}\\left\\{\\min \\left[\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,&lt;t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,&lt;t}\\right)} \\hat{A}_{i, t}, \\operatorname{clip}\\left(\\frac{\\pi_\\theta\\left(o_{i, t} \\mid q, o_{i,&lt;t}\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(o_{i, t} \\mid q, o_{i,&lt;t}\\right)}, 1-\\varepsilon, 1+\\varepsilon\\right) \\hat{A}_{i, t}\\right]-\\beta \\mathbb{D}_{K L}\\left[\\pi_\\theta \\| \\pi_{r e f}\\right]\\right\\}, \\end{aligned} JGRPO​(θ)​=E[q∼P(Q),{oi​}i=1G​∼πθold ​​(O∣q)]G1​i=1∑G​∣oi​∣1​t=1∑∣oi​∣​{min[πθold ​​(oi,t​∣q,oi,&lt;t​)πθ​(oi,t​∣q,oi,&lt;t​)​A^i,t​,clip(πθold ​​(oi,t​∣q,oi,&lt;t​)πθ​(oi,t​∣q,oi,&lt;t​)​,1−ε,1+ε)A^i,t​]−βDKL​[πθ​∥πref​]},​ GRPO目标函数主要由三部分组成： 策略比值 (Policy Ratio)：衡量新旧策略的变化幅度。 裁剪目标 (Clipped Objective)：防止策略更新过大，确保稳定性。 KL 散度正则化 (KL Divergence Regularization)：防止新策略偏离参考策略过远。 优势估计 A^i,t=r~i=ri−mean⁡(r)std⁡(r)\\hat{A}_{i, t}=\\widetilde{r}_i=\\frac{r_i-\\operatorname{mean}(\\mathbf{r})}{\\operatorname{std}(\\mathbf{r})} A^i,t​=ri​=std(r)ri​−mean(r)​ 参考文章： DeepSeek 背后的数学原理：深入探究群体相对策略优化 (GRPO) 强化学习小白理解GRPO（二）：GRPO核心代码实践 GRPO 算法核心公式解析（附代码详解） Qwen_0.5b__GRPO DPO 论文：Direct Preference Optimization: Your Language Model is Secretly a Reward Model 代码：https://github.com/eric-mitchell/direct-preference-optimization DPO（Direct Preference Optimization）将 RLHF 的 2 阶段（训练 reward model和训练 PPO）多个模型的训练简化为了 1 阶段的 SFT 训练。 算法基于PPO的RLHF基础上进行了大幅简化，跳过了奖励建模步骤，直接使用偏好数据优化语言模型，解决了RLHF三个阶段的训练（SFT-&gt;RM-&gt;PPO）过程较长，更新迭代较慢的问题。DPO只有策略模型和参考模型，不再需要奖励模型。 DPO 提出了一种使用二进制交叉熵目标来精确优化大模型的方法，以替代基于RLHF的优化目标，从而大大简化偏好学习流程。 目标函数： ∇θLDPO(πθ;πref)=−βE(x,yw,yl)∼D[σ(r^θ(x,yl)−r^θ(x,yw))⏟higher weight when reward estimate is wrong [∇θlog⁡π(yw∣x)⏟increase likelihood of yw−∇θlog⁡π(yl∣x)⏟decrease likelihood of yl]],\\begin{aligned} &amp; \\nabla_\\theta \\mathcal{L}_{\\mathrm{DPO}}\\left(\\pi_\\theta ; \\pi_{\\mathrm{ref}}\\right)= \\\\ &amp; -\\beta \\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}[\\underbrace{\\sigma\\left(\\hat{r}_\\theta\\left(x, y_l\\right)-\\hat{r}_\\theta\\left(x, y_w\\right)\\right)}_{\\text {higher weight when reward estimate is wrong }}[\\underbrace{\\nabla_\\theta \\log \\pi\\left(y_w \\mid x\\right)}_{\\text {increase likelihood of } y_w}-\\underbrace{\\nabla_\\theta \\log \\pi\\left(y_l \\mid x\\right)}_{\\text {decrease likelihood of } y_l}]], \\end{aligned} ​∇θ​LDPO​(πθ​;πref​)=−βE(x,yw​,yl​)∼D​[higher weight when reward estimate is wrong σ(r^θ​(x,yl​)−r^θ​(x,yw​))​​[increase likelihood of yw​∇θ​logπ(yw​∣x)​​−decrease likelihood of yl​∇θ​logπ(yl​∣x)​​]],​ 其中： r^θ(x,y)=βlog⁡πθ(y∣x)πref(y∣x)\\hat{r}_{\\theta} ( x , y )=\\beta\\operatorname{log} \\frac{\\pi_{\\theta} ( y | x )} {\\pi_{\\mathrm{ref}} ( y | x )} r^θ​(x,y)=βlogπref​(y∣x)πθ​(y∣x)​ 参数β是控制策略模型（policy model）与参考模型（reference model）之间的 KL 散度约束强度。 为了方便分析，把log里面的分式展开，然后β设为1，并且忽略前面的log_sigmoid，那么目标函数可以简化为： [log⁡p(yw)−log⁡pref(yw)]−[log⁡p(yl)−log⁡pref(yl)][\\operatorname{log} p(y_{w})-\\operatorname{log} p_{ref}(y_{w})]-[\\operatorname{log} p(y_{l})-\\operatorname{log} p_{ref}(y_{l})] [logp(yw​)−logpref​(yw​)]−[logp(yl​)−logpref​(yl​)] 由于最初loss前面是有个负号的，所以优化目标是让本简化公式最大，即我们希望左半部分和右半部分的margin越大越好。 简化目标函数存在下面三种变化： 左边变大，右边变小，理想情况，good response概率提升，bad response概率下降 左边变小，右边更小，good response概率下降，但是bad response概率下降的更多，生成的时候还是倾向于good response 左边变的更大，右边只大了一点点，和2）同理 “Reward hacking”是指在RLHF过程中，模型发现奖励函数中的漏洞或偏差，从而通过非预期行为获得高奖励的现象。 Q：为什么DPO里Chosen和Rejected概率会同时下降（防止hacking）？ 在DPO优化过程中, 对于chosen优化的方向是有不确定性的，DPO实际上优化保证整体增大 ，而不是单一的让chosen prob增大。 解决方案： 为chosen/rejected ratio设定不同参数β β⁺·logπ(y⁺) − β⁻·logπ(y⁻)（其中 β⁺ &gt; β⁻，如 β⁺=0.6、β⁻=0.2） 我们在DPO优化时，可以确定性的优化使得正例reward&gt;0，如DPO-P max(0, −r(y⁺|x))，强制模型必须让 r (y⁺|x) &gt; 0，即logπ(y⁺|x) &gt; −C/τ（τ 为温度系数，C 为常数）。 在DPO优化时，同时对正例增加SFT优化 SFT（监督微调）的核心是最小化 y⁺的负对数似然损失（L_SFT = −logπ(y⁺|x)），其目标是直接最大化 y⁺的生成概率，具有极强的 “单向提升” 确定性。将其与 DPO 损失联合优化，可形成互补； 联合损失公式：L = L_DPO + α·L_SFT（α∈[0.1, 0.5]，控制 SFT 的权重）。 切换为 KTO：重构目标为 “确定性优化单个策略”，从根源消除比值的不确定性 Q：DPO训练的模型效果如何评估？ 核心评估维度： 偏好对齐度（核心） 人工偏好对比测试：赢率、偏好准确率 自动评估（奖励模型 RM）：平均奖励得分、排序准确率 生成质量（避免能力损失） 流畅度：Perplexity（困惑度） 任务适配性：BLEU/ROUGE/METEOR（摘要 / 翻译等） 知识准确性：FactScore、TruthfulQA 正确率 安全性（规避有害内容） 对抗性测试：有害内容拒绝率（避免过度拒绝） 基准数据集：HarmBench、RealToxicityPrompts 毒性 / 偏见分数 代码实现： def preference_loss(policy_chosen_logps: torch.FloatTensor, policy_rejected_logps: torch.FloatTensor, reference_chosen_logps: torch.FloatTensor, reference_rejected_logps: torch.FloatTensor, beta: float, label_smoothing: float = 0.0, ipo: bool = False, reference_free: bool = False) -&gt; Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor]: pi_logratios = policy_chosen_logps - policy_rejected_logps ref_logratios = reference_chosen_logps - reference_rejected_logps if reference_free: ref_logratios = 0 logits = pi_logratios - ref_logratios # also known as h_&#123;\\pi_\\theta&#125;^&#123;y_w,y_l&#125; if ipo: losses = (logits - 1/(2 * beta)) ** 2 # Eq. 17 of https://arxiv.org/pdf/2310.12036v2.pdf else: # Eq. 3 https://ericmitchell.ai/cdpo.pdf; label_smoothing=0 gives original DPO (Eq. 7 of https://arxiv.org/pdf/2305.18290.pdf) losses = -F.logsigmoid(beta * logits) * (1 - label_smoothing) - F.logsigmoid(-beta * logits) * label_smoothing chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps).detach() rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps).detach() return losses, chosen_rewards, rejected_rewards 参考文章： DPO 是如何简化 RLHF 的 人人都能看懂的DPO数学原理 LLM强化学习算法调研：PPO、DPO、KTO等","categories":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"强化学习","slug":"强化学习","permalink":"http://chiang97912.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"PPO","slug":"PPO","permalink":"http://chiang97912.github.io/tags/PPO/"},{"name":"GRPO","slug":"GRPO","permalink":"http://chiang97912.github.io/tags/GRPO/"},{"name":"DPO","slug":"DPO","permalink":"http://chiang97912.github.io/tags/DPO/"}]},{"title":"大模型激活函数","slug":"大模型激活函数","date":"2025-12-23T16:29:04.000Z","updated":"2025-12-23T17:04:46.749Z","comments":true,"path":"2025/12/24/大模型激活函数/","permalink":"http://chiang97912.github.io/2025/12/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/","excerpt":"ReLU Relu(Rectified Linear Unit)——修正线性单元函数","text":"ReLU Relu(Rectified Linear Unit)——修正线性单元函数 数学公式： ReLU(x)=max(0,x)ReLU(x) = max(0, x) ReLU(x)=max(0,x) ReLU作为激活函数的特点： 相比Sigmoid和Tanh，ReLU摒弃了复杂的计算，提高了运算速度。 解决了梯度消失问题，收敛速度快于Sigmoid和tanh函数，但要防范ReLU的梯度爆炸。 ReLU 强制将x&lt;0部分的输出置为0，可能会导致模型无法学习到有效特征，所以如果学习率设置的太大，就可能会导致网络的大部分神经元处于死亡状态，所以使用ReLU的网络，学习率不能设置太大。 LeakyReLU(x)={x,if x≥0negative_slope⋅x,otherwise\\mathrm{LeakyReLU}(x) = \\begin{cases} x, &amp; \\text{if } x \\ge 0 \\\\ \\text{negative\\_slope} \\cdot x, &amp; \\text{otherwise} \\end{cases} LeakyReLU(x)={x,negative_slope⋅x,​if x≥0otherwise​ Leaky ReLU中的公式为常数，一般设置 0.01。这个函数通常比 Relu 激活函数效果要好，但是效果不是很稳定，所以在实际中 Leaky ReLu 使用的并不多。 PRelu（参数化修正线性单元）是Leaky ReLU的一个变体，公式中参数a_i作为一个可学习的参数，会在训练的过程中更新。 RReLU（随机纠正线性单元）也是Leaky ReLU的一个变体。在RReLU中，负值的斜率a_{ji}在训练中是随机的，在测试时会固定下来。RReLU的亮点在于，在训练环节中，a_{ji}是从一个均匀分布U(I,u)中随机抽取的数值。 GeLU GeLU（Gaussian Error Linear Units）即高斯误差线性单元，它是一种高性能的神经网络激活函数，因为GeLU的非线性变化是一种符合预期的随机正则变换方式，公式如下： xP(X≤x)=xΦ(x)x P ( X \\leq x )=x \\Phi( x ) xP(X≤x)=xΦ(x) 其中Φ(x)指的是x高斯分布的累积分布，完整形式如下： xP(X≤x)=x∫−∞xe−(X−μ)22σ22πσ dXx P ( X \\leq x )=x \\int_{-\\infty}^{x} {\\frac{e^{-{\\frac{( X-\\mu)^{2}} {2 \\sigma^{2}}}}} {\\sqrt{2 \\pi} \\sigma}} \\, \\mathrm{d} X xP(X≤x)=x∫−∞x​2π​σe−2σ2(X−μ)2​​dX 计算结果如下： 0.5x(1+tanh[2π(x+0.044715x3)])0 . 5 x ( 1+t a n h [ \\sqrt{\\frac{2} {\\pi}} ( x+0 . 0 4 4 7 1 5 x^{3} ) ] ) 0.5x(1+tanh[π2​​(x+0.044715x3)]) 或者可以表示为： xσ(1.702x)x \\sigma( 1 . 7 0 2 x ) xσ(1.702x) 由此可知，概率P ( X ≤ x ) 即X的高斯正态分布ϕ(X)的累积分布Φ(x)是随着x的变化而变化的，当x增大，Φ(x)增大，当x减小，Φ(x)减小，即当x越小，在当前激活函数激活的情况下，越有可能激活结果为0，即此时神经元被dropout，而当x越大越有可能被保留。 代码实现（bert）： def gelu(x): &quot;&quot;&quot;Gaussian Error Linear Unit. This is a smoother version of the RELU. Original paper: https://arxiv.org/abs/1606.08415 Args: x: float Tensor to perform activation. Returns: `x` with the GELU activation applied. &quot;&quot;&quot; cdf = 0.5 * (1.0 + tf.tanh( (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))) return x * cdf GELU可以看作是ReLU的平滑近似，它不再是硬性地在0点截断，而是根据输入数值在高斯分布中的分位来决定输出的缩放比例，输入越小（负得越多），就越趋近于输出0，输入越大，就越趋近于输出x。 GeLU相比ReLU增加非线性因子和数据统计特性。 参考文章：https://cloud.tencent.com/developer/article/1876554 GLU/SwiGLU GLU（Gated Linear Unit）即门控线性单元，门控机制的核心思想是让网络自己学习去控制信息的流动。它不再是一个简单的、由输入值本身决定的开关，而是引入一个独立的“门控”分支，来动态决定哪些信息可以通过，以及通过多少。 GLU(x,W,V)=(xW+b)⊗σ(xV+c)\\mathrm{G L U} ( x , W , V )=( x W+b ) \\otimes\\sigma( x V+c ) GLU(x,W,V)=(xW+b)⊗σ(xV+c) 其中： x 是输入。 W,V,b,c 是可学习的权重和偏置。 (xW+b) 是内容分支，负责正常的线性变换。 σ(xV+c) 是门控分支，σ是Sigmoid函数，其输出在 (0, 1) 之间。 ⊗ 代表逐元素相乘。 SwiGLU是GLU的一个变体，它将门控分支的激活函数从Sigmoid换成了Swish函数。 Swish函数： Swish⁡(x)=x⋅σ(βx)\\operatorname{S w i s h} ( x )=x \\cdot\\sigma( \\beta x ) Swish(x)=x⋅σ(βx) 其中σ是Sigmoid函数 SwiGLU： SwiGLU(x,W,V)=Swish(xW+b)⊗(xV+c)\\mathrm{S w i G L U} ( x , W , V )=\\mathrm{S w i s h} ( x W+b ) \\otimes( x V+c ) SwiGLU(x,W,V)=Swish(xW+b)⊗(xV+c) 总结对比： 特性 ReLU GELU SwiGLU 机制 静态阈值开关 平滑的静态开关 动态、数据驱动的门控 数学形式 max(0, x) x * Φ(x) Swish(xW) ⊗ (xV) 平滑性 在0点不平滑 平滑 平滑 表达能力 较弱 中等 非常强 核心思想 简单、非线性 ReLU的平滑近似 学习如何控制信息流 代表模型 早期CNNs, RNNs BERT, GPT-2/3 LLaMA, PaLM, Mixtral, GPT-4 参考文章：https://zhuanlan.zhihu.com/p/1962167666010748360","categories":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"LLM","slug":"LLM","permalink":"http://chiang97912.github.io/tags/LLM/"},{"name":"激活函数","slug":"激活函数","permalink":"http://chiang97912.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"name":"ReLU","slug":"ReLU","permalink":"http://chiang97912.github.io/tags/ReLU/"},{"name":"GeLU","slug":"GeLU","permalink":"http://chiang97912.github.io/tags/GeLU/"},{"name":"GLU","slug":"GLU","permalink":"http://chiang97912.github.io/tags/GLU/"},{"name":"SwiGLU","slug":"SwiGLU","permalink":"http://chiang97912.github.io/tags/SwiGLU/"}]},{"title":"大模型归一化方法","slug":"大模型归一化方法","date":"2025-12-23T16:28:42.000Z","updated":"2025-12-23T17:06:47.623Z","comments":true,"path":"2025/12/24/大模型归一化方法/","permalink":"http://chiang97912.github.io/2025/12/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BD%92%E4%B8%80%E5%8C%96%E6%96%B9%E6%B3%95/","excerpt":"LayerNorm LayerNorm(层归一化)是一种在深度学习中常用的归一化技术。","text":"LayerNorm LayerNorm(层归一化)是一种在深度学习中常用的归一化技术。 计算公式： y=x−E[x]Var[x]+ϵ∗γ+βy=\\frac{x-\\mathrm{E} [ x ]} {\\sqrt{\\mathrm{V a r} [ x ]+\\epsilon}} * \\gamma+\\beta y=Var[x]+ϵ​x−E[x]​∗γ+β 其中： x：输入向量。 E[x]：输入向量均值。 Var[x]：输入向量方差。 γ：可学习的缩放参数。 β：可学习的偏移参数。 优点： 完全归一化（均值 0，方差 1），对分布偏移的矫正更彻底； 偏移参数beta可灵活调整分布中心，适配更多任务（如 CV、NLP 基础模型）。 缺点： 均值计算增加计算量，大模型场景下效率偏低； 中心化可能破坏特征的 “绝对偏移信息”（如语言模型中词向量的固有偏移）； 小批量 / 低维特征下，均值估计易受噪声影响，导致稳定性下降。 RMSNorm RMSNorm（ Root Mean Square Layer Normalization ）核心思想是通过对输入向量进行缩放归一化，以提升训练稳定性和效率。 计算公式： RMSNorm(x)=γ⊙xmean(x2)+ϵ\\mathrm{R M S N o r m} ( x )=\\gamma\\odot\\frac{x} {\\sqrt{\\mathrm{m e a n} ( x^{2} )+\\epsilon}} RMSNorm(x)=γ⊙mean(x2)+ϵ​x​ 其中： x：输入向量。 mean(x^2)：向量元素的平方均值。 ϵ：极小常数（ 如 10e−8 ），防止分母为零。 γ：可学习的缩放参数。 代码实现： class RMSNorm(nn.Module): def __init__(self, dim: int, eps: float = 1e-6): super().__init__() self.eps = eps self.gamma = nn.Parameter(torch.ones(dim)) # 可学习参数γ def _norm(self, x: torch.Tensor): # 计算平方均值的根 (RMS) return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) def forward(self, x: torch.Tensor): return self.gamma * self._norm(x.float()).type_as(x) 优点： 计算高效（少均值步骤），显存 / 算力开销更低，适配大模型（如 LLaMA、GPT-3）； 保留特征的原始均值偏移，更贴合语言模型的语义特性（词向量的偏移包含语义信息）； 无中心化带来的噪声放大，训练更稳定，收敛速度更快。 缺点： 仅归一化方差，对分布中心的偏移无矫正作用； 无偏移参数，适配性稍弱（但实践中可通过其他层补偿）。 关键区别 维度 LayerNorm RMSNorm 中心化操作 有（减均值） 无（直接用原始值计算均方根） 可学习参数 gamma（缩放）+ beta（偏移） 仅gamma（缩放） 计算复杂度 稍高（多一步均值计算） 更低（少均值计算，浮点运算量减少～20%） 数值稳定性 均值可能放大噪声（尤其小批量） 避免均值偏移，稳定性更好（尤其大模型） 保留的信息 消除均值 + 方差偏移，改变分布中心 仅消除方差偏移，保留原始分布中心 梯度传播 均值计算引入额外依赖，梯度稍复杂 梯度路径更简洁，训练更高效 💡LayerNorm 是 “中心化 + 标准化”，RMSNorm 是 “仅标准化”。 浮点运算量（FLOPs）对比： 操作类型 LayerNorm（d 维向量） RMSNorm（d 维向量） 差异（RMSNorm 减少） 加法 d（求 μ） + d（求方差）= 2d d（求平方和）= d d 次加法 减法 d（中心化 xi - mu） 0（无中心化） d 次减法 乘法（含平方） d（方差中 xi²） d（RMS 中 xi²） 0 次 除法 2 次（μ = sum/d；σ² = sum/d） 1 次（sum/d） 1 次除法 开方 1 次（sqrt (σ²)） 1 次（sqrt (RMS)） 0 次 仿射变换 d（gamma 乘） + d（beta 加）= 2d d（gamma 乘）= d d 次加法 总运算量对比： LayerNorm 总 FLOPs ≈ 4d + 3（简化后，忽略常数项） RMSNorm 总 FLOPs ≈ 2d + 2（简化后，忽略常数项） RMSNorm 比 LayerNorm 少 ≈ 2d 次运算（当 d 很大时，比如 d=1024，少 2048 次运算；d=4096 时，少 8192 次运算） 直观理解（d=1024 时的实际差异）： LayerNorm：21024（加法） + 1024（减法） + 21024（仿射）= 5120 次核心运算 RMSNorm：1024（加法） + 0（减法） + 1024（仿射）= 2048 次核心运算 计算量减少约 60%（实际工程中因内存访问优化，实测减少～20%~30%，但仍显著优于 LayerNorm）","categories":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"LLM","slug":"LLM","permalink":"http://chiang97912.github.io/tags/LLM/"},{"name":"归一化","slug":"归一化","permalink":"http://chiang97912.github.io/tags/%E5%BD%92%E4%B8%80%E5%8C%96/"},{"name":"LayerNorm","slug":"LayerNorm","permalink":"http://chiang97912.github.io/tags/LayerNorm/"},{"name":"RMSNorm","slug":"RMSNorm","permalink":"http://chiang97912.github.io/tags/RMSNorm/"}]},{"title":"Qwen系列模型笔记","slug":"Qwen系列模型笔记","date":"2025-12-19T14:58:34.000Z","updated":"2025-12-23T17:12:38.110Z","comments":true,"path":"2025/12/19/Qwen系列模型笔记/","permalink":"http://chiang97912.github.io/2025/12/19/Qwen%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/","excerpt":"Qwen 论文：Qwen Technical Report Qwen：使用了3T token, 数据包含多样化各个领域的文本和代码。","text":"Qwen 论文：Qwen Technical Report Qwen：使用了3T token, 数据包含多样化各个领域的文本和代码。 数据 网页数据：先从HTML中抽取文本，并用语言识别工具识别语种和内容。 去重技术提升数据多样性：包括归一化后的精确匹配删除，以及使用MinHash、LSH算法的模糊数据删除。 过滤低质量数据：基于规则和机器学习的方法，使用多个模型对内容进行打分（语言模型、文本质量评分模型、识别潜在攻击性或不当内容的模型等），同时也会通过人工对各种来源的文本进行采样并进行审核以确保质量。 有选择的对某些来源的数据进行上采样：确保模型接受到各种高质量内容的训练。 预训练加入高质量的多任务数据：研究表明，使用多任务质量数据可以增强zero-shot，few-shot能力。 确保评估公平性：将相关训练数据从评测集中删除。（采用13-gram overlap） 分词器 词汇表的设计显著影响模型的训练效率和下游任务的性能，Qwen采用BPE分词器方法（遵循GPT-3.5和GPT-4），从开源的快速BPE Tokenizer, tiktoken开始，并选择词汇表cl100l为基础作为起点，用常用汉字、短语以及其他语言的单词来增加词汇量以便增强多语言的性能（这里特别说明对中文词汇进行了增加），最终词汇表的大小为152K。 模型结构 Qwen是使用Transformer的修改版本设计的，具体来说，是采用了LlaMA的架构设计，Qwen对架构的修改包括以下几个方面： 嵌入层和输出映射（Embedding and output projection）：根据初步的实验结果，选择了不受限嵌入方法，而不是捆绑输入嵌入和输出投影的权重。 位置编码（Positional embedding）：选择RoPE作为将位置信息纳入模型的首选。Qwen选择使用 FP32 精度作为逆频率矩阵，而不是 BF16 或 FP16，以便优先考虑模型性能并实现更高的精度。 偏置（Bias）：在 QKV 注意力层中添加偏置以增强模型的外推能力。 归一化（Pre-Norm &amp; RMSNorm）：文中提到现代Transformer模型广泛使用前置归一化(Pre-Norm)，得益于Pre-Norm使得训练更加稳定（更容易训练），最新的研究也提出了一些提高训练稳定性的替代方法（计划在未来Qwen系列模型中进行探索），Qwen已将传统归一化方法替换为RMSNorm（RMSNorm是LayerNorm的一个变体），在相同的性能下提高了效率。 归一化方法参考：大模型归一化方法 Pre-Norm和Post-Norm： Pre Norm：xt+1=xt+Ft(Norm(xt))Post Norm:xt+1=Norm⁡(xt+Ft(xt))\\text{Pre Norm}： \\boldsymbol{x}_{t+1}=\\boldsymbol{x}_{t}+F_{t} ( \\mathrm{N o r m} ( \\boldsymbol{x}_{t} ) ) \\\\ \\text{Post Norm}: \\boldsymbol{x}_{t+1}=\\operatorname{N o r m} ( \\boldsymbol{x}_{t}+F_{t} ( \\boldsymbol{x}_{t} ) ) Pre Norm：xt+1​=xt​+Ft​(Norm(xt​))Post Norm:xt+1​=Norm(xt​+Ft​(xt​)) 激活函数（Activation function）：采用SwiGLU作为激活函数，它是Swish和门控线性单元的组合。初步实验验证，基于GLU的激活函数效果优于基于其他基线的选项，比如GeLU研究中的常见做法，将FFN的维度从隐藏层大小的4倍减少为8/3倍。 激活函数：大模型激活函数 扩展上下文 Qwen实现了简单的免于训练的技术可以在预测阶段扩展上下文长度。关键技术之一是NTK感知插值，不同于位置插值（PI）需要对RoPE的每个维度进行均等缩放，NTK感知插值调整RoPE基数防止在免训练模式下高频信息丢失。进一步提升性能，实现动态NTK-感知插值，按照块动态改变规模，以避免严重的性能下降，这些技术可以有效扩展Transformer模型的上下文长度，且不损害计算效率和准确性。 Qwen2 论文：Qwen2 Technical Report 源码：https://github.com/huggingface/transformers/tree/v4.39.3/src/transformers/models/qwen2 模型结构 模型结构基于Transformer实现，并且包括因果掩码的自注意力机制。实现4个不同大小的DenseModel和1个MOEModel。 Qwen2 Dense Model 由多个Transformer层构成，每层包含因果注意力机制和前馈神经网络FFNs, 相比之前系列主要不同有： 分组查询注意力（Grouped Query Attention，GQA）：分组查询注意力(GQA)取代传统的多头注意力(MHA)。 GQA优化推理过程中的KV缓存使用，显著提高吞吐。这种机制将多个query头组合在一起，共享同一组key和value矩阵。因此需要缓存的key和value矩阵就大大减少了，节省了内存占用。同时，推理时可以对多个query一起计算attention，显著提高了计算效率。 双块注意力结合YARN（Dual Chunk Attention with YARN）：为了扩展上下文窗口，实现双块注意力（Dual Chunk Attention, DCA）, 将长序列分割成可管理的块长度，如果输入可以在一个块中处理，DCA会产生与原始结果相同的结果注意力。DCA有利于有效捕捉块内、块间的不同Token的相对位置信息，从而提高长上下文性能。此外，还使用YARN重新调整注意力权重以获得更好的长度外推。 激活函数仍然使用SwiGLU, 位置编码使用RoPE, Attention使用QKV偏置，RMSNorm前置归一化使得训练稳定。 双块注意力：双块注意力会把很长的输入序列切割成若干个“块”（chunk）。如果输入不太长，只需一个块就能处理，那么DCA就等同于普通的注意力机制。但如果输入很长，需要很多个块来处理时，DCA就会发挥作用。它不仅能捕捉每个块内部单词之间的关系，还能捕捉不同块之间单词的相对位置关系。 YARN (NTK-aware + NTK-by-parts + Dynamic NTK)：YaRN是基于NTK-aware方法的进一步拓展，通过结合温度缩放和NTK-by-parts插值技术，全面提升长文本外推能力。 大模型位置编码：从ROPE到Yarn, 一条通用公式速通长文本大模型中的位置编码 参考文章：稳步前行的阿里云大模型——Qwen2凭什么蝉联榜首？ Qwen2 Mixture-of-exports Model Qwen2 MOE模型架构和Qwen1.5-MoE-A2.7B的架构非常相似，MoE FFN取代FFN由n个独立的FFN构成，每个作为专家服务。每个Token都被定向到特定的专家Ei进行概率（基于门控网络G分配的概率）计算： p=softmax(G(x)),y=∑i∈top⁡k(p)piEi(x).\\mathbf{p}=\\mathrm{s o f t m a x} \\left( G \\left( \\mathbf{x} \\right) \\right) , \\\\ \\mathbf{y}=\\sum_{i \\in\\operatorname{t o p}_{k} ( \\mathbf{p} )} \\mathbf{p}_{i} E_{i} ( \\mathbf{x} ) . p=softmax(G(x)),y=i∈topk​(p)∑​pi​Ei​(x). 参考文章：https://zhuanlan.zhihu.com/p/21021458859 后训练数据 自动数据合成 维持大规模指令的标注响应数据的质量是具有挑战的，特别是那些需要专家、经验、仔细耐心标注的数据。应对这些挑战，我们设计了各种自动对齐策略来大规模合成数据。 拒绝采样：对于具有明确最终答案的数学或类似任务，应用拒绝采样提高质量。LLM的任务是为每条质量生成多个响应，即推理路径。那些可以导向正确结果切被模型认为是合理的路径会被保留，作为示例数据。通过相对正确和相对错误路径生成偏好数据。 执行反馈：对于代码类的任务，用LLM生成解决方案和相关测试用例。评估这些解决方案的方法是编译和执行测试用例，据此构建示例和偏好数据。这个方法也被应用于评估指令遵循的能力，对于每条具有约束的指令，比如长度限制等，用LLM生成Python校验方法确保结果是否跟指令对齐。 数据再利用：创建有技巧性的文学类写作任务对于没有经过专业训练的标注人员是有挑战的。为处理这类问题，我们从公开领域收集了高质量的文学作品以及使用LLM开发不同程度的细节指令，这些指令和原始作品一起作为示例数据。例如，要编译生动引人入胜的角色扮演响应，我们从维基百科等知识库中获取详细的角色简介，指导大模型生成相应指令和响应。这个过程，类似做阅读理解任务，维持确保角色个人信息的完整性。 宪法反馈：宪法AI是指引导大模型生成预先定义准则的响应的过程。为确保例如安全性和价值观的遵守，编制了宪法数据集。该数据集描述需要被遵循和被规避的准则。它被用来指导大模型生产对齐响应和背离响应，从而制作示例和偏好数据。 拒绝采样：拒绝采样的核心思想是让模型生成多个答案，然后只选择最优的答案来继续训练。 参考文章：模型结构-qwen2","categories":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"LLM","slug":"LLM","permalink":"http://chiang97912.github.io/tags/LLM/"},{"name":"Qwen","slug":"Qwen","permalink":"http://chiang97912.github.io/tags/Qwen/"}]},{"title":"混合专家模型笔记","slug":"混合专家模型笔记","date":"2025-12-16T13:17:52.000Z","updated":"2025-12-23T16:33:19.714Z","comments":true,"path":"2025/12/16/混合专家模型笔记/","permalink":"http://chiang97912.github.io/2025/12/16/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/","excerpt":"论文： Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity 代码：https://github.com/jingyaogong/minimind/blob/master/model/model_minimind.py","text":"论文： Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity 代码：https://github.com/jingyaogong/minimind/blob/master/model/model_minimind.py 模型规模是提升模型性能的关键因素之一。在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。 混合专家模型 (MoE) 的一个显著优势是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。 混合专家模型 (MoEs): 与稠密模型相比， 预训练速度更快 与具有相同参数数量的模型相比，具有更快的 推理速度 需要 大量显存，因为所有专家系统都需要加载到内存中 在 微调方面存在诸多挑战，但 近期的研究 表明，对混合专家模型进行 指令调优具有很大的潜力。 模型架构 混合专家模型主要由两个关键部分组成: 稀疏 MoE 层: 这些MoE层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。 门控网络或路由: 这个部分用于决定哪些token被发送到哪个专家。一个token可以被发送到多个专家。 总结来说，在混合专家模型 (MoE) 中，将传统 Transformer 模型中的每个前馈网络 (FFN) 层替换为 MoE 层，其中 MoE 层由两个核心部分组成: 一个门控网络和若干数量的专家。 稀疏性的概念采用了条件计算的思想。在传统的稠密模型中，所有的参数都会对所有输入数据进行处理。相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，只有部分参数集合被调用和运行。 2017 年，Shazeer 等人将MoE应用于 137B 的 LSTM，GShard首次将 MoE 引入到 Transformer 模型中，Switch Transformer相比 Gshard 等方案主要做了三点改进： 简化稀疏路由 高效稀疏路由 增强的训练和微调技巧 专家选择 路由器其实是 MoE 中最核心的组件之一。它不仅决定推理阶段该选哪些专家，也决定训练阶段哪些专家要被更新。 最基础的做法是，将输入 token 表示向量 x ，乘以一个路由权重矩阵 Wg ，对打分结果做 softmax，得到一个不同专家的概率分布G( x ) ： Vπ(S)=∑aπ(a∣s)Q(s,a)V_{\\pi} ( S )=\\sum_{a} \\pi( a | s ) Q ( s , a ) Vπ​(S)=a∑​π(a∣s)Q(s,a) 然后根据这些概率值，选出和输入最相关的专家。 最后把每个路由器的输出与对应的专家相乘，加权求和，得到该 token 的最终输出： y=∑i=1nG(x)iEi(x)y=\\sum_{i=1}^{n} G ( x )_{i} E_{i} ( x ) y=i=1∑n​G(x)i​Ei​(x) 负载均衡 虽然专家选择机制看起来很简单，但实际上会出现一个问题，某些专家学得更快，导致路由器总是选它们。这会导致选择的专家不均匀，一些专家几乎不会被训练到，最终训练和推理时都容易出现偏移和性能下降的问题。为了解决这个问题，我们引入了负载均衡（Load Balancing） ：它的目的是让每个专家在训练和推理中都能被公平地使用，避免某在某几个专家上过拟合。 Q：如何解决MoE架构中负载不平衡的问题？ 如果所有的token都被发送到只有少数几个受欢迎的专家，那么训练效率将会降低。在通常的混合专家模型 (MoE) 训练中，门控网络往往倾向于主要激活相同的几个专家。这种情况可能会自我加强，因为受欢迎的专家训练得更快，因此它们更容易被选择。为了缓解这个问题，引入了一个 辅助损失（aux_loss），旨在鼓励给予所有专家相同的重要性。这个损失确保所有专家接收到大致相等数量的训练样本，从而平衡了专家之间的选择。接下来的部分还将探讨专家容量的概念，它引入了一个关于专家可以处理多少令牌的阈值。在 transformers 库中，可以通过 aux_loss 参数来控制辅助损失。 KeepTopK 策略 一种常见的路由负载均衡方法是通过一个叫做KeepTopK的简单的扩展，引入可训练的高斯噪声，以防止模型总是选中同一批专家： H(x)i=(x⋅Wg)i+StandardNormal()⋅Softplus((x⋅Wnoise)i)H ( x )_{i}=( x \\cdot W_{g} )_{i}+S t a n d a r d N o r m a l ( ) \\cdot S o f t p l u s ( ( x \\cdot W_{n o i s e} )_{i} ) H(x)i​=(x⋅Wg​)i​+StandardNormal()⋅Softplus((x⋅Wnoise​)i​) 接下来，除了你想要激活的 Top-k 个专家（比如前 2 个），其他专家的分数全部设为 -∞： KeepTopK(v,k)i={viif vi is in the top k elements of v .−∞otherwise.K e e p T o p K ( v , k )_{i}=\\begin{cases} {v_{i}} &amp; {\\mathrm{i f ~ v_i ~ i s ~ i n ~ t h e ~ t o p ~ k ~ e l e m e n t s ~ o f ~ v ~} .} \\\\ {-\\infty} &amp; {\\mathrm{o t h e r w i s e} .} \\\\ \\end{cases} KeepTopK(v,k)i​={vi​−∞​if vi​ is in the top k elements of v .otherwise.​ 这样一来，在做 SoftMax 时，分数为 -∞ 的专家对应的概率就是 0，不会被选中 ： G(x)=Softmax(KeepTopK(H(x),k))G ( x )=S o f t m a x ( K e e p T o p K ( H ( x ) , k ) ) G(x)=Softmax(KeepTopK(H(x),k)) Token Choice KeepTopK 策略是每个 token 会被送到少数几个专家那里处理。这种方法被称为 Token Choice 。Top-1 路由每个 token 只交给一个专家处理。Top-k 路由每个 token 被同时送给k个专家，然后再根据它们的输出加权合并 。这种方法的优点是：可以根据每个专家的权重贡献，融合多个专家的知识，更灵活。 辅助损失 为了让训练过程中的专家使用更加平均，研究者在主损失之外，引入了一个辅助损失（Auxiliary Loss）或者叫负载均衡损失（Load Balancing Loss）。这个损失项的作用是强制每个专家在训练中有差不多的“重要性”。 辅助损失的第一步，是对整个 batch 中每个专家的路由概率值求和也就是统计在这个 batch 中，每个专家一共“被选中的概率”加起来是多少 ： Importance(X)=∑x∈XG(x)I m p o r t a n c e ( X )=\\sum_{x \\in X} G ( x ) Importance(X)=x∈X∑​G(x) 这样就得到了每个专家的一个重要性分数（importance score），表示这个专家在整个 batch 中，在不考虑输入内容的情况下，有多大可能被选中。 接下来，我们可以利用这些重要性分数，计算每个专家重要性分数的变异系数（Coefficient of Variation, CV） ，表示各个专家的重要性差异有多大： CV=σμ.C V=\\frac{\\sigma} {\\mu} . CV=μσ​. 其中，σ表示Importance(X)的标准差，μ表示Importance(X)平均值。 如果 CV 很高，说明某些专家总被选中，而其他专家几乎没被用；如果 CV 很低，说明所有专家被使用得差不多，这正是我们想要的“负载均衡”状态。 利用这个 CV 分数，我们可以在训练过程中不断更新辅助损失，使模型的优化目标之一就是尽可能降低 CV 值，从而让每个专家的使用重要性趋于一致： Limportance(X)=wimportance⋅CV(Importance(X))2L_{i m p o r t a n c e} ( X )=w_{i m p o r t a n c e} \\cdot C V ( I m p o r t a n c e ( X ) )^{2} Limportance​(X)=wimportance​⋅CV(Importance(X))2 其中，w_{importance}是一个常量，表示缩放因子。 最后，这个辅助损失会作为一个单独的损失项，加入到整体训练目标中一起优化。 专家容量 模型中的不平衡，不仅体现在被选中的专家不平均，也体现在发送给专家的 token 分布不均。 比如：如果大量输入 token 都被不均衡地路由到了某一个专家（而其他专家几乎不会接收到 token），那可能会导致训练不充分。为了解决这个问题，可以引入一个限制机制，限制每个专家一次最多能处理多少个 token，称之为专家容量（Expert Capacity）。一旦某个专家达到容量上限，剩下的 token 就会被路由给下一个候选专家，如果所有候选专家都已满，token 将不会被任何专家处理，而是跳过当前 MoE 层，进入下一层。这种情况被称为 token overflow（token 溢出） 。 Switch Transformer允许设置专家容量因子（capacity factor），显式地控制专家容量： expert capacity=(tokens per batchnumber of experts)×capacity factor.\\mathrm{e x p e r t ~ c a p a c i t y=} \\left( \\frac{\\mathrm{t o k e n s ~ p e r ~ b a t c h}} {\\mathrm{n u m b e r ~ o f ~ e x p e r t s}} \\right) \\times\\mathrm{c a p a c i t y ~ f a c t o r} . expert capacity=(number of expertstokens per batch​)×capacity factor. 容量因子设置得越大，每个专家就能处理更多的 token。如果容量因子太大，则会浪费计算资源。相反，如果容量因子太小，则 Token 溢出会导致模型性能下降。 参考文章： https://huggingface.co/blog/zh/moe https://zhuanlan.zhihu.com/p/81886457827 https://zhuanlan.zhihu.com/p/653796685","categories":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"}],"tags":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"深度学习","slug":"深度学习","permalink":"http://chiang97912.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"MoE","slug":"MoE","permalink":"http://chiang97912.github.io/tags/MoE/"},{"name":"混合专家模型","slug":"混合专家模型","permalink":"http://chiang97912.github.io/tags/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/"}]},{"title":"Transformer系列模型笔记","slug":"Transformer系列模型笔记","date":"2025-12-13T13:43:50.000Z","updated":"2025-12-21T14:17:56.344Z","comments":true,"path":"2025/12/13/Transformer系列模型笔记/","permalink":"http://chiang97912.github.io/2025/12/13/Transformer%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E7%AC%94%E8%AE%B0/","excerpt":"Transformer 总体架构 论文：Attention Is All You Need 代码：harvardnlp/annotated-transformer","text":"Transformer 总体架构 论文：Attention Is All You Need 代码：harvardnlp/annotated-transformer Transformer的基本结构，通过模型结构图可看出Transformer是由Encoder与Decoder构成： 左边的部分是编码器Encoder，右边的部分是解码器Decoder，根据不同的任务需要，使用对应的部分，一般编码器部分常用于文本编码分类，解码器部分用于语言模型生成，Encoder和Decoder都包含6个block层，编码器和解码器并不是简单的串联关系。 编码器Encoder 每个编码器由两层结构组成： 第一层包括：多头自注意力层， 规范化层（LayerNorm）、残差连接。 第二层包括：前馈全连接层、规范化层（LayerNorm）、残差连接。 编码器Encoder 由6个相同层block组成，每一层由相同的两部分组成：一个多头注意力层和一个Feed Forward层。这两个部分后面都进行残差连接和LayerNorm归一化 LayerNorm(x +Sublayer(x))。Feed Forward层其实就是简单的MLP层，由两个线性层组成，中间用ReLU函数进行激活。 解码器Decoder 每个解码器由三层结构组成： 第一层包括：多头自注意力层（Masked Multi-Head Attention）、规范化层（LayerNorm）、残差连接。 第二层包含：多头注意力层（Multi-Head Attention）、规范化层（LayerNorm）、残差连接。 第三层包含：前馈全连接层、规范化层（LayerNorm）、残差连接。 解码器Decoder 也是由6个相同层组成，每一层由相同的三部分组成：一个带掩码的多头注意力层（Masked Multi-Head Attention）、一个多头交叉注意力层（Multi-Head Cross Attention）和一个MLP层。与编码器一样都在各部分后添加残差和LayerNorm归一化，不同的是在多头注意力层中输入的是由编码器输出的key、value和经过带掩码的多头注意力层输出的query。 Masked Multi-Head Attention： Masked Multi-Head Attention是带attention mask的MHA（Multi-Head Attention），Encoder中用的是不带attention mask的MHA，Encoder输入src只进行了padding mask，Decoder同时使用了padding mask和attention mask。 Q: Transformer 的 Encoder、Decoder 和 GPT 的架构有什么区别？ Decoder 相较于 Encoder 多了掩码机制和交叉注意力，实际上真正区分二者的是自注意力中的掩码机制，防止模型在生成时看到未来的词。交叉注意力也被称为编码器-解码器注意力（Encoder-Decoder Attention）。 输入输出层 模型输入： Transformer输入向量X由词向量Embedding和位置向量Positional Encoding相加得到。 Transformer使用的是正余弦位置编码。位置编码通过使用不同频率的正弦、余弦函数生成，位置编码公式如下： PE(pos,2i)=sin⁡(pos/100002i/dmodel )PE(pos,2i+1)=cos⁡(pos/100002i/dmodel )\\begin{aligned} P E_{(p o s, 2 i)} &amp; =\\sin \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right) \\\\ P E_{(p o s, 2 i+1)} &amp; =\\cos \\left(p o s / 10000^{2 i / d_{\\text {model }}}\\right) \\end{aligned} PE(pos,2i)​PE(pos,2i+1)​​=sin(pos/100002i/dmodel ​)=cos(pos/100002i/dmodel ​)​ 其中： i是位置向量的下标，2i和2i+1表示奇偶性，它的取值范围是[0,…,d_model / 2]； pos表示数据在序列中的绝对位置，pos=0,1,2…,max_len-1； d_{model}表示位置向量的维度（transformer论文中设置的是512维）。 参考文章：Transformer学习笔记一：Positional Encoding（位置编码） 公式实现： class PositionalEncoding(nn.Module): &quot;Implement the PE function.&quot; def __init__(self, d_model, dropout, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) # Compute the positional encodings once in log space. pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len).unsqueeze(1) div_term = torch.exp( torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model) ) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0) self.register_buffer(&quot;pe&quot;, pe) def forward(self, x): x = x + self.pe[:, : x.size(1)].requires_grad_(False) return self.dropout(x) 其中div_term没有直接进行幂运算，而是先转换为了等价的指数+对数运算，这样做是为了确保数值稳定性和计算效率： 直接使用幂运算可能会导致数值上溢或下溢。当d_model较大时，10000.0 ** (-i / d_model)中的幂可能会变得非常小，以至于在数值计算中产生下溢。通过将其转换为指数和对数运算，可以避免这种情况，因为这样可以在计算过程中保持更好的数值范围； 在许多计算设备和库中，指数和对数运算的实现通常比幂运算更快。这主要是因为指数和对数运算在底层硬件和软件中有特定的优化实现，而幂运算通常需要计算更多的中间值。 div_term公式如下： div_term=exp([024⋮(dmodel−1)]⋅(−log⁡(10000)dmodel))\\mathrm{div\\_term} = exp \\left( \\left[ \\begin{matrix} 0 \\\\ 2 \\\\ 4 \\\\ \\vdots \\\\ (d_{\\text{model}} - 1) \\end{matrix} \\right] \\cdot \\left( - \\frac{\\log(10000)}{d_{\\text{model}}} \\right) \\right) div_term=exp⎝⎜⎜⎜⎜⎜⎜⎛​⎣⎢⎢⎢⎢⎢⎢⎡​024⋮(dmodel​−1)​⎦⎥⎥⎥⎥⎥⎥⎤​⋅(−dmodel​log(10000)​)⎠⎟⎟⎟⎟⎟⎟⎞​ 其中的中括号对应的是一个从 0 到 d_{model} - 1 的等差数列(步长为 2)，设为i 且上述公式与这个公式是等价的： div_termi=10000(−idmodel)=e(i⋅(−log⁡(10000)dmodel))\\mathrm{d i v \\_t e r m}_{i}=1 0 0 0 0^{\\left(-\\frac{i} {d_{\\mathrm{m o d e l}}} \\right)}=e^{ \\left( i \\cdot \\left( - \\frac{\\log(10000)}{d_{\\text{model}}} \\right) \\right) } div_termi​=10000(−dmodel​i​)=e(i⋅(−dmodel​log(10000)​)) 可以通过下面公式求证（两边同时ln结果相等）： ax=e(x⋅ln(a))a^{x}=e^{( x \\cdot l n ( a ) )} ax=e(x⋅ln(a)) 模型输出： Transformer中输出层由一层线性层Linear和一层Softmax组成。 参考文章：https://www.cnblogs.com/tian777/p/17917392.html 自注意力机制 核心公式： Attention⁡(Q,K,V)=softmax⁡(QKTdk)V\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^T}{\\sqrt{d_k}}\\right) V Attention(Q,K,V)=softmax(dk​​QKT​)V Q 代表query，代表要查询的信息，后续会去和每个K进行匹配。 K 代表key， 代表索引，也就是被查询的向量， 后续会被每个Q匹配。 V 代表value，代表查询到的值，Q和K匹配的过程可以理解成计算两者的相关性，相关性越大对应V的权重也就越大。 代码实现： def attention(query, key, value, mask=None, dropout=None): &quot;Compute &#x27;Scaled Dot Product Attention&#x27;&quot; d_k = query.size(-1) scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k) if mask is not None: scores = scores.masked_fill(mask == 0, -1e9) p_attn = scores.softmax(dim=-1) if dropout is not None: p_attn = dropout(p_attn) return torch.matmul(p_attn, value), p_attn 多头注意力机制： 代码实现： class MultiHeadedAttention(nn.Module): &quot;&quot;&quot; 多头注意力机制模块（Multi-Headed Attention） 核心逻辑：将输入拆分为多个头，分别计算注意力，再拼接结果 &quot;&quot;&quot; def __init__(self, num_heads, d_model, dropout=0.1): super(MultiHeadedAttention, self).__init__() # 断言检查：模型维度必须能被头数整除（保证每个头的维度d_k是整数） assert d_model % num_heads == 0 # 每个注意力头的维度（d_k = d_model / h） self.d_k = d_model // num_heads # 注意力头的数量 self.num_heads = num_heads # 单独声明线性层 # Q/K/V各自的投影层：输入d_model，输出d_model（拆分为h个d_k） self.query_linear = nn.Linear(d_model, d_model) self.key_linear = nn.Linear(d_model, d_model) self.value_linear = nn.Linear(d_model, d_model) # 拼接后的输出投影层 self.output_linear = nn.Linear(d_model, d_model) # 存储注意力权重（用于可视化或后续分析） self.attn_weights = None # Dropout层（用于注意力权重的正则化） self.dropout = nn.Dropout(p=dropout) def forward(self, query, key, value, mask=None): # 处理掩码：增加头维度，使掩码能应用到所有头 if mask is not None: # mask形状：(batch, 1, seq_len_q, seq_len_k)（1对应头维度，广播到所有头） mask = mask.unsqueeze(1) # 获取批次大小 batch_size = query.size(0) # ====================== 单独处理Q/K/V的投影与维度变换 ====================== # 1. Query处理：线性投影 -&gt; 形状变换 -&gt; 维度转置 # 线性投影：(batch, seq_len_q, d_model) -&gt; (batch, seq_len_q, d_model) query = self.query_linear(query) # 形状变换：(batch, seq_len_q, d_model) -&gt; (batch, seq_len_q, num_heads, d_k) query = query.view(batch_size, -1, self.num_heads, self.d_k) # 维度转置：(batch, seq_len_q, num_heads, d_k) -&gt; (batch, num_heads, seq_len_q, d_k) # 目的：让每个头独立计算注意力（头维度在前，序列维度在后） query = query.transpose(1, 2) # 2. Key处理（与Query逻辑一致） key = self.key_linear(key) key = key.view(batch_size, -1, self.num_heads, self.d_k) key = key.transpose(1, 2) # 3. Value处理（与Query逻辑一致） value = self.value_linear(value) value = value.view(batch_size, -1, self.num_heads, self.d_k) value = value.transpose(1, 2) # ========================================================================== # 应用自注意力机制 x, self.attn_weights = attention( query, key, value, mask=mask, dropout=self.dropout ) # 拼接所有头的输出：恢复维度并应用最终线性投影 # 1. 维度转置：(batch, num_heads, seq_len_q, d_k) -&gt; (batch, seq_len_q, num_heads, d_k) x = x.transpose(1, 2) # 2. 连续化张量（保证view操作的内存连续性） x = x.contiguous() # 3. 形状变换：(batch, seq_len_q, num_heads, d_k) -&gt; (batch, seq_len_q, d_model)（拼接所有头） x = x.view(batch_size, -1, self.num_heads * self.d_k) # 应用输出线性层，得到最终结果 output = self.output_linear(x) return output Feed Forward层 Feed Forward 层是一个两层的全连接层，第一层的激活函数为 ReLU，第二层不使用激活函数。 BERT 论文：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding 源码（PyTorch版本）：codertimo/BERT-pytorch 模型配置 Google 论文中提出了Base和Large两种BERT模型。 模型 Layers Hidden Size Attention Head 参数数量 Base 12 768 12 110M Large 24 1024 16 340M 模型结构 BERT只是使用了Transformer中的Encoder部分，没有Decoder部分，因此相较于Transformer中的两种mask（key padding mask和attention mask），BERT中只有key padding mask，也就是忽略掉padding部分的信息，而在Transformer的解码阶段还需要忽略掉当前位置之后的信息所以还要使用attention mask。 💡 BERT采用和Transformer相同的Post-Norm结构。 输入表示 针对不同的任务，BERT模型的输入可以是单句或者句对。对于每一个输入的Token，它的表征由其对应的词表征（Token Embedding）、段表征（Segment Embedding）和位置表征（Position Embedding）相加产生，如下图所示： Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务，对于英文模型，使用了Wordpiece模型来产生Subword从而减小词表规模，对于中文模型，直接训练基于字的模型。 Segment Embeddings用来区别两种句子，因为预训练不仅做LM还要做以两个句子为输入的分类任务，为区别两个句子，用一个特殊标记符[SEP]进行分隔，对于单句输入，只有一种Segment Embedding，对于句对输入，会有两种Segment Embedding。 Position Embeddings和Transformer的位置编码不同，不是三角函数而是学习出来的。 预训练目标 BERT预训练过程包含两个不同的预训练任务，分别是Masked Language Model和Next Sentence Prediction任务。 Masked Language Model（MLM） 通过随机掩盖一些词（替换为统一标记符[MASK]），然后预测这些被遮盖的词来训练双向语言模型，并且使每个词的表征参考上下文信息。 这样做会产生两个缺点：（1）会造成预训练和微调时的不一致，因为在微调时[MASK]总是不可见的；（2）由于每个Batch中只有15%的词会被预测，因此模型的收敛速度比起单向的语言模型会慢，训练花费的时间会更长。对于第一个缺点的解决办法是，把80%需要被替换成[MASK]的词进行替换，10%的随机替换为其他词，10%保留原词。由于Transformer Encoder并不知道哪个词需要被预测，哪个词是被随机替换的，这样就强迫每个词的表达需要参照上下文信息。对于第二个缺点目前没有有效的解决办法，但是从提升收益的角度来看，付出的代价是值得的。 Next Sentence Prediction（NSP） 为了训练一个理解句子间关系的模型，引入一个下一句预测任务。这一任务的训练语料可以从语料库中抽取句子对包括两个句子A和B来进行生成，其中50%的概率B是A的下一个句子，50%的概率B是语料中的一个随机句子。NSP任务预测B是否是A的下一句。NSP的目的是获取句子间的信息，这点是语言模型无法直接捕捉的。 Google的论文结果表明，这个简单的任务对问答和自然语言推理任务十分有益，但是后续一些新的研究[15]发现，去掉NSP任务之后模型效果没有下降甚至还有提升。我们在预训练过程中也发现NSP任务的准确率经过1-2个Epoch训练后就能达到98%-99%，去掉NSP任务之后对模型效果并不会有太大的影响。 参考文章： ​ https://tech.meituan.com/2019/11/14/nlp-bert-practice.html ​ https://zhuanlan.zhihu.com/p/46652512 https://www.zhihu.com/question/404452350/answer/2217828686 GPT 论文： GPT-1：Improving Language Understanding by Generative Pre-Training GPT-2：Language Models are Unsupervised Multitask Learners GPT-3：Language Models are Few-Shot Learners 源码（PyTorch版本）: https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py https://github.com/graykode/gpt-2-Pytorch/blob/master/GPT2/model.py 模型细节 模型架构 💡 GPT 保留了 Decoder 的Masked Multi-Head Attention 层和 Feed Forward 层，并扩大了网络的规模。 Text &amp; Position Embed Text Embed：将输入的词转化为可训练的嵌入向量。 Position Embed：使用可学习的位置信息嵌入，这里和 Transformer 默认的正余弦位置编码不同，但 Transformer 论文的 Table 3 (E) 中有对比二者的性能差异，所以并非一个新的方法。 Transformer Block Masked Multi-Head Self-Attention：掩码多头自注意力机制，在生成任务中，每次预测一个词时，当前词只能看到左侧的上下文信息，未来的词和预测的词都会被掩盖。对应于 Transformer 架构中 Masked Multi-Head Attention。 Add &amp; Norm：Layer Norm (LN) + 残差连接 (+)。 Feed-Forward：前馈网络 (Feed-Forward Network, FFN)。 Add &amp; Norm：Layer Norm (LN) + 残差连接 (+)。 Prediction 和 Task Classifier Text Prediction：用于生成任务，预测下一个词。 Task Classifier：用于分类任务，如情感分析或文本蕴含任务。 💡 左侧的 12x 表示堆叠了12层 transformer_block。 无监督预训练 给定一个无标注样本库的token序列集合 U={u1,u2…un} ，语言模型的目标就是最大化下面的似然值。也就是通过前面的tokens，预测下一个token。 L1(U)=∑ilog⁡P(ui∣ui−k,…,ui−1;Θ)L_1(\\mathcal{U})=\\sum_i \\log P\\left(u_i \\mid u_{i-k}, \\ldots, u_{i-1} ; \\Theta\\right) L1​(U)=i∑​logP(ui​∣ui−k​,…,ui−1​;Θ) 其中，k是滑动窗口的大小，P是条件概率。模型参数使用SGD进行优化。 GPT-1使用了12层Transformer decoder结构。输入包括文本向量和位置向量。 h0=UWe+Wphl= transformer block (hl−1)∀i∈[1,n]P(u)=softmax⁡(hnWeT)\\begin{aligned} h_0 &amp; =U W_e+W_p \\\\ h_l &amp; =\\text { transformer block }\\left(h_{l-1}\\right) \\forall i \\in[1, n] \\\\ P(u) &amp; =\\operatorname{softmax}\\left(h_n W_e^T\\right) \\end{aligned} h0​hl​P(u)​=UWe​+Wp​= transformer block (hl−1​)∀i∈[1,n]=softmax(hn​WeT​)​ 其中，U表示token的文本向量， We 表示token embedding矩阵， Wp 表示位置向量矩阵。每个token会通过transformer block被编码，最后再经过一个线性层+softmax，得到下一个token的预测分布。 有监督微调 得到无监督的预训练模型后，将得到的参数值直接应用于有监督任务中。对于一个有标签的数据集C，每个实例有m个输入tokens： {x1,…xm} 。将这些tokens输入到预训练模型中，得到transformer block的输出向量 h ，再经过一个全连接+softmax，得到预测结果y： P(y∣x1,…,xm)=softmax⁡(hlmWy)P\\left(y \\mid x^1, \\ldots, x^m\\right)=\\operatorname{softmax}\\left(h_l^m W_y\\right) P(y∣x1,…,xm)=softmax(hlm​Wy​) 有监督学习的目标即最大化上述概率： L2(C)=∑(x,y)log⁡P(y∣x1,…,xm)L_2(\\mathcal{C})=\\sum_{(x, y)} \\log P\\left(y \\mid x^1, \\ldots, x^m\\right) L2​(C)=(x,y)∑​logP(y∣x1,…,xm) 值得一提的是，作者发现，把预训练目标作为辅助目标加入下游任务loss中，将会提高有监督模型的泛化性能，并加速收敛。因此，有监督任务的最终优化目标是： L3(C)=L2(C)+λ∗L1(C)L_3(\\mathcal{C})=L_2(\\mathcal{C})+\\lambda * L_1(\\mathcal{C}) L3​(C)=L2​(C)+λ∗L1​(C) GPT-2 GPT-1 和 GPT-2 的区别： GPT-2相比GPT-1拥有更大的数据集、更大的模型参数。 GPT-1：Post-Norm，层归一化放置在残差连接之后。 GPT-2：Pre-Norm，层归一化放置在残差连接之前。 Post-Norm/Pre-Norm： # Post-Normdef forward(self, x, sublayer): # 子层的输出 + 残差连接后，进行归一化 return self.norm(x + sublayer(x))# Pre-Normdef forward(self, x, sublayer): # 输入先进行归一化，再传入子层，最后进行残差连接 return x + sublayer(self.norm(x)) GPT-3 GPT-3 ：相比GPT-1和GPT-2拥有更大的数据集和更大的模型。 参考文章： https://zhuanlan.zhihu.com/p/626494749 https://blog.csdn.net/weixin_42426841/article/details/145123776","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://chiang97912.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"深度学习","slug":"深度学习","permalink":"http://chiang97912.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"Transformer","slug":"Transformer","permalink":"http://chiang97912.github.io/tags/Transformer/"},{"name":"BERT","slug":"BERT","permalink":"http://chiang97912.github.io/tags/BERT/"},{"name":"GPT","slug":"GPT","permalink":"http://chiang97912.github.io/tags/GPT/"}]},{"title":"机器学习模型评估方法及代码实现","slug":"机器学习模型评估方法及代码实现","date":"2020-01-09T16:04:46.000Z","updated":"2025-12-13T14:32:24.698Z","comments":true,"path":"2020/01/10/机器学习模型评估方法及代码实现/","permalink":"http://chiang97912.github.io/2020/01/10/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95%E5%8F%8A%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"","text":"混淆矩阵 混淆矩阵（confusion matrix）如下所示： 真实\\预测 正例 反例 正例 TP(真正例) FN(假反例) 反例 FP(假正例) TN(真反例) TP: 将正例预测为正例（预测正确）； FN: 将正例预测为负例（预测错误）； FP: 将负例预测为正例（预测错误）； TN: 将负例预测为负例（预测正确）。 正例包括 TP、FN；反例包括 TN、FP 准确率（Accuracy） 定义：指的是分类正确的样本数量占样本总数的比例 公式： A=TP+TNTP+TN+FN+FPA=\\frac{TP+TN}{TP+TN+FN+FP} A=TP+TN+FN+FPTP+TN​ scikit-learn接口： import numpy as npfrom sklearn.metrics import accuracy_scorey_pred = [0, 2, 1, 3]y_true = [0, 1, 2, 3]accuracy_score(y_true, y_pred) 精确率(Precision) 定义：精确率(Precision)，即查准率。通常我们所说的精确率是正例的精确率，它是被判定为正例的样本中，真正的正例样本的比例。我们同样可以计算负例的精确率，但是通常没有人这样做。精确率主要用来评测是否误检。举个例子，假设一个班级有10个学生，5男5女。我们目标是寻找班级中的女生，返回6个结果分别是男 、 女 、女 、 男 、女 、 男。如果返回的结果中只有3个正确，那么查准率为3/6=0.5。 公式（分类任务）： P=TPTP+FP或TNTN+FNP=\\frac{TP}{TP+FP}或\\frac{TN}{TN+FN} P=TP+FPTP​或TN+FNTN​ 在信息检索领域，精确率是返回结果中相关文档的数目与返回结果的数目的比例： precision=∣{relevant documents}∩{retrieved documents}∣{retrieved documents}precision=\\frac{|\\{relevant\\ documents\\}∩\\{retrieved\\ documents\\}|}{\\{retrieved\\ documents\\}} precision={retrieved documents}∣{relevant documents}∩{retrieved documents}∣​ scikit-learn接口： from sklearn.metrics import precision_scorey_true = [0, 1, 2, 0, 1, 2]y_pred = [0, 2, 1, 0, 0, 1]precision_score(y_true, y_pred, average=&#x27;macro&#x27;)precision_score(y_true, y_pred, average=&#x27;micro&#x27;)precision_score(y_true, y_pred, average=&#x27;weighted&#x27;)precision_score(y_true, y_pred, average=None) 注释： macro 度量：对于n个二分类混淆矩阵，在各混淆矩阵上分别计算精确率和召回率，记（P1,R1），（P2,R2）…（Pn，Rn），再计算平均值，得到宏精确率（macro-P）、宏召回率（macro-R），继而得到宏F1（macro-F1）。 micro度量：对于n个二分类混淆矩阵，先对TP、FN、FP、TN求平均值，再用均值计算得到微精确率（micro-P）、微召回率（micro-P），继而得到微F1（micro-F1）。 召回率(Recall) 定义：召回率(Recall)，即查全率。通常我们所说的召回率是正例的召回率，它是被正确分类的正例样本，占所有正例样本的比例。我们同样可以计算负例的召回率，但是通常没有人这样做。召回率主要用来评测是否漏检。举个例子，假设一个班级有10个学生，5男5女。我们目标是寻找班级中的女生，返回6个结果分别是男 、 女 、女 、 男 、女 、 男。总共有5个女生，返回结果中有3个女生，那么查全率为3/5=0.6。 公式（分类任务）： R=TPTP+FN或TNTN+FPR=\\frac{TP}{TP+FN}或\\frac{TN}{TN+FP} R=TP+FNTP​或TN+FPTN​ 在信息检索领域，召回率是返回结果中相关文档的数目与所有相关文档的数目的比例： precision=∣{relevant documents}∩{retrieved documents}∣{relevant documents}precision=\\frac{|\\{relevant\\ documents\\}∩\\{retrieved\\ documents\\}|}{\\{relevant\\ documents\\}} precision={relevant documents}∣{relevant documents}∩{retrieved documents}∣​ scikit-learn接口： from sklearn.metrics import recall_scorey_true = [0, 1, 2, 0, 1, 2]y_pred = [0, 2, 1, 0, 0, 1]recall_score(y_true, y_pred, average=&#x27;macro&#x27;)recall_score(y_true, y_pred, average=&#x27;micro&#x27;)recall_score(y_true, y_pred, average=&#x27;weighted&#x27;)recall_score(y_true, y_pred, average=None) F1_score 定义：精确率和召回率的调和平均值。 公式： 1F1=12.(1P+1R)，即F1=2×P×RP+R=2TP2TP+FP+FN\\frac{1}{F1}=\\frac{1}{2}.(\\frac{1}{P}+\\frac{1}{R})，即F1=\\frac{2\\times P\\times R}{P+R}=\\frac{2TP}{2TP+FP+FN} F11​=21​.(P1​+R1​)，即F1=P+R2×P×R​=2TP+FP+FN2TP​ scikit-learn接口： from sklearn.metrics import f1_scorey_true = [0, 1, 2, 0, 1, 2]y_pred = [0, 2, 1, 0, 0, 1]f1_score(y_true, y_pred, average=&#x27;macro&#x27;)f1_score(y_true, y_pred, average=&#x27;micro&#x27;)f1_score(y_true, y_pred, average=&#x27;weighted&#x27;)f1_score(y_true, y_pred, average=None) ROC和AUC 定义： ROC曲线，是以FPR(False Positive Rate, 召回率) 为横轴、TPR(True Positive Rate, 取伪率)为纵轴，衡量二分类系统性能的曲线。分类器对分类的置信度一般设为50%，即置信度超过50%认为是正例，低于50%认为是反例。依次改变这个置信度为10%~100%，会得到一组不同的混淆矩阵，取其中的FPR和TPR值组成坐标，连接这些值，就得到ROC曲线。ROC曲线与X轴围成的图形面积可以作为一个综合衡量指标，即AUC（Area Under Curve，曲线下面积）。AUC越大，曲线就越凸，分类器的效果也就越好。ROC曲线反映了分类器对正例的覆盖能力和对负例的覆盖能力之间的权衡。 scikit-learn接口： import numpy as npfrom sklearn.metrics import roc_curve, auc, roc_auc_scorey = np.array([1,1,2,2])pred = np.array([0.1,0.4,0.35,0.8])fpr,tpr,thresholds = roc_curve(y,pred,pos_label=2)result = auc(fpr,tpr)print(result) AP/MAP 定义：我们首先引入PR曲线概念。PR曲线（Precision-recall曲线）与ROC曲线的区别是横轴和纵轴不同，PR曲线的横轴Recall也就是TPR，反映了分类器对正例的覆盖能力。而纵轴Precision的分母是识别为正例的数目，而不是实际正例数目。Precision反映了分类器预测正例的准确程度。那么，PR曲线反映了分类器对正例的识别准确程度和对正例的覆盖能力之间的权衡。对于随机分类器而言，其Precision固定的等于样本中正例的比例，不随recall的变化而变化。与AUC相似，AP（Average Precision）就是PR曲线与X轴围成的图形面积， 若PR曲线为连续型，则： AP=∫01PR drAP=\\int_{0}^{1} PR\\, \\mathrm{d}r AP=∫01​PRdr 若PR曲线为离散型，则： AP=∑k=1nP(K)Δr(k)AP=\\sum_{k=1}^n P(K)\\Delta r(k) AP=k=1∑n​P(K)Δr(k) 在信息检索领域，我们引入MAP(Mean Average Precision, 均值平均精度)， MAP=∑q=1QAP(q)QMAP=\\frac{\\sum_{q=1}^Q AP(q)}{Q} MAP=Q∑q=1Q​AP(q)​ 其中Q为查询的总次数。 AP计算方法：前面给出的是AP的定义式，下面我们将介绍在信息检索领域计算AP的方法。首先求出每个位置上的精确率（Precision），然后求所有的位置的精确率（Precision）的平均值。如果该位置的文档是不相关的则该位置 Precision=0。 AP=∑k=1n(P(k)×rel(k))number of relevant documentsAP=\\frac{\\sum_{k=1}^n (P(k)\\times rel(k))}{number\\ of\\ relevant\\ documents} AP=number of relevant documents∑k=1n​(P(k)×rel(k))​ 注：其中k是返回结果序列的排列次序，n是返回结果的数目，P(k)是返回序列中从第k个文档处截断的精确率，rel(k)是指示函数，如果第k个文档为相关文档则rel(k)=1，否者rel(k)=0。 例如： Prediction Correctness Points 1 wrong 0 2 right 1 / 2 3 right 2 / 3 4 wrong 0 5 right 3 / 5 6 wrong 0 7 wrong 0 8 wrong 0 9 right 4 / 9 10 wrong 0 上表中Prediction列表示文档的得分排序结果，Correctness列表示结果的正确性，Points列为精确率得分。那么我们将计算得到AP=(0+1/2+2/3+0+3/5+0+0+0+4/9+0)/4=0.55。 MAP计算方法：主集合的平均准确率(MAP)是每个主题的平均准确率（AP）的平均值。假设在测试集中一共有k个类别。我们先算出我们的模型对于每个类别的AP，然后将这些AP相加在一起再除以所有类别的数量k，就可以得到最终模型的MAP。 例如：假设有两个主题，主题1有4个相关网页，主题2有3个相关网页。对于主题1检索出4个相关网页，其rank分别为1, 2, 4, 7，平均准确率为(1/1+2/2+3/4+4/7)/4=0.83；对于主题2检索出3个相关网页，其rank分别为1,3,5，平均准确率为(1/1+2/3+3/5+0+0)/5=0.45。则MAP= (0.83+0.45)/2=0.64。 scikit-learn接口： import numpy as npfrom sklearn.metrics import average_precision_scorey_true = np.array([0, 0, 1, 1])y_scores = np.array([0.1, 0.4, 0.35, 0.8])result = average_precision_score(y_true, y_scores) # 计算APprint(result) MRR 定义：MRR(Mean Reciprocal Rank, 平均倒数排名)把标准答案在搜索结果中分数的排序取倒数作为它的准确度，再对所有的问题取平均。例如： Query Results Correct response Rank Reciprocal Rank cat catten, cati, cats cats 3 1/3 torus torri, tori, toruses tori 2 1/2 virus viruses, virii, viri viruses 1 1 注：黑体为返回结果中最匹配的一项 上表中的MRR=(1/3 + 1/2 + 1)/3 = 11/18=0.61 公式： MRR=1Q∑i=1∣Q∣1rankiMRR=\\frac{1}{Q}\\sum_{i=1}^{|Q|} \\frac{1}{rank_i} MRR=Q1​i=1∑∣Q∣​ranki​1​ 其中|Q|是查询个数，rankirank_iranki​是第i个查询相对于第一个相关的结果所在的排列位置。 scikit-learn接口： import numpy as npfrom sklearn.metrics import label_ranking_average_precision_scorey_true = np.array([[1, 0, 0], [0, 0, 1]])y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])result = label_ranking_average_precision_score(y_true, y_score)print(result) NDCG 定义：NDCG(Normalized Discounted Cumulative Gain, 归一化折损累计增益)，在MAP中，四个文档和query要么相关，要么不相关，即相关度只能是0或1。NDCG对MAP进行了改进，相关度分成从0到r等级。当取r=5时，等级设定如下图所示： Relevance Rating Gain Perfect 31=25−131=2^5-131=25−1 Excellent 15=24−115=2^4-115=24−1 Good 7=23−17=2^3-17=23−1 Fair 3=22−13=2^2-13=22−1 xxx 1=21−11=2^1-11=21−1 Bad 0=20−10=2^0-10=20−1 我们将这些增益相加就是CG(Cumulative Gain，累计增益,),CG就是将每个推荐结果相关性的分支累加后作为整个推荐列表的得分。 CGk=∑i=1kreliCG_k=\\sum_{i=1}^k{rel_i} CGk​=i=1∑k​reli​ 其中relirel_ireli​表示处于位置i的推荐结果的相关性。 Relevance Rating Gain Cumulative Gain #1 http://abc.go.com/ 31 31=31x1 #2 http://www.abcteach.com/ 3 34=31+3 #3 http://abcnews.go.com/sections/scitech/ 15 49=31+3+15 #4 http://www.abc.net.au/ 15 64=31+3+15+15 #5 http://abcnews.go.com/ 15 79=31+3+15+15+15 #6 … … … 考虑到一般情况下用户会优先点选排在前面的搜索结果，所以应该引入一个折算因子(discounting factor)，这时将计算得到DCG(Discounted Cumulative Gain)值。 DCGk=∑i=1k2reli−1log2(i+1)DCG_k=\\sum_{i=1}^k \\frac{2^{rel_i}-1}{log_2 (i+1)} DCGk​=i=1∑k​log2​(i+1)2reli​−1​ Relevance Rating Gain Discounted Cumulative Gain #1 http://abc.go.com/ 31 31=31x1 #2 http://www.abcteach.com/ 3 32.9=31+3x0.63 #3 http://abcnews.go.com/sections/scitech/ 15 40.4=32.9+15x0.50 #4 http://www.abc.net.au/ 15 46.9=40.4+15x0.43 #5 http://abcnews.go.com/ 15 52.7=46.9+15x0.39 #6 … … … 最后我们对DCG进行归一化得到NDCG(Normalized Discounted Cumulative Gain,归一化折损累计增益)。 NDCGk=DCGkIDCGkNDCG_k=\\frac{DCG_k}{IDCG_k} NDCGk​=IDCGk​DCGk​​ 其中IDCG(Ideal DCG)，指推荐系统为某一用户返回的最好推荐结果列表，即假设返回结果按照相关性排序，最相关的结果放在前面，此序列的DCG为IDCG。因此DCG的值介于(0, IDCG]，故NDCG的值介于(0,1]。 评估方法实现 参考 [1]: Precision and recall [2]: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval) [3]: Precision/Recall、ROC/AUC、AP/MAP等概念区分 [4]: IR的评价指标-MAP,NDCG和MRR [5]: Mean reciprocal rank [6]: 二分类模型评估指标的计算方法与代码实现 [7]: Discounted cumulative gain [8]: 一个评测指标就是MAP(Mean Average Precision)平均精度均值。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://chiang97912.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"模型评估","slug":"模型评估","permalink":"http://chiang97912.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"},{"name":"Accuracy","slug":"Accuracy","permalink":"http://chiang97912.github.io/tags/Accuracy/"},{"name":"Precision","slug":"Precision","permalink":"http://chiang97912.github.io/tags/Precision/"},{"name":"Recall","slug":"Recall","permalink":"http://chiang97912.github.io/tags/Recall/"},{"name":"F1","slug":"F1","permalink":"http://chiang97912.github.io/tags/F1/"},{"name":"ROC","slug":"ROC","permalink":"http://chiang97912.github.io/tags/ROC/"},{"name":"AUC","slug":"AUC","permalink":"http://chiang97912.github.io/tags/AUC/"},{"name":"MAP","slug":"MAP","permalink":"http://chiang97912.github.io/tags/MAP/"},{"name":"MRR","slug":"MRR","permalink":"http://chiang97912.github.io/tags/MRR/"},{"name":"NDCG","slug":"NDCG","permalink":"http://chiang97912.github.io/tags/NDCG/"}]},{"title":"轻量级Git服务Gogs搭建教程","slug":"轻量级Git服务Gogs搭建教程","date":"2019-12-30T13:37:48.000Z","updated":"2025-12-13T12:17:39.262Z","comments":true,"path":"2019/12/30/轻量级Git服务Gogs搭建教程/","permalink":"http://chiang97912.github.io/2019/12/30/%E8%BD%BB%E9%87%8F%E7%BA%A7Git%E6%9C%8D%E5%8A%A1Gogs%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B/","excerpt":"前言 Gogs是一个类似于Gitlab的开源Git服务，它具有易安装、跨平台、轻量级等特点。相比于Gitlab它的资源占有率极低，对于个人开发者或者小型团队是非常实用的一款Git服务。","text":"前言 Gogs是一个类似于Gitlab的开源Git服务，它具有易安装、跨平台、轻量级等特点。相比于Gitlab它的资源占有率极低，对于个人开发者或者小型团队是非常实用的一款Git服务。 创建系统用户 创建新的系统用户 “git”，并切换为 “git” 用户: sudo useradd -m gitsudo su - git 创建数据库 创建新的数据库并命名为gogs： mysql&gt;create database gogs; 安装Gogs 本文以v0.11.53 版本为例，最新版本读者可以前往dl.gogs.io查看： wget https://dl.gogs.io/0.11.53/gogs_0.11.53_linux_amd64.tar.gztar xzvf gogs_0.11.53_linux_amd64.tar.gz 运行Gogs 进入解压后的文件夹，然后执行命令： ./gogs web 最后浏览器访问http://:3000完成相应的配置即可。 问题 Gogs不支持移动适配，但是它的开源分支Gitea支持，且Gitea的安装流程基本和Gogs一致。","categories":[{"name":"服务器配置","slug":"服务器配置","permalink":"http://chiang97912.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://chiang97912.github.io/tags/Git/"},{"name":"Gogs","slug":"Gogs","permalink":"http://chiang97912.github.io/tags/Gogs/"},{"name":"Gitea","slug":"Gitea","permalink":"http://chiang97912.github.io/tags/Gitea/"}]},{"title":"Pyinstaller打包Python程序攻略","slug":"Pyinstaller打包Python程序攻略","date":"2019-08-25T08:38:04.000Z","updated":"2025-12-13T12:15:59.369Z","comments":true,"path":"2019/08/25/Pyinstaller打包Python程序攻略/","permalink":"http://chiang97912.github.io/2019/08/25/Pyinstaller%E6%89%93%E5%8C%85Python%E7%A8%8B%E5%BA%8F%E6%94%BB%E7%95%A5/","excerpt":"PyInstaller基本使用方法 下面列举几个常见的可选参数： pyinstaller [options] my_script.py[options]: -h 显示帮助并退出 -D 生成一个文件夹，其中包含一个可执行文件（默认） -F 生成单个可执行文件 -w 生成一个无命令行界面的程序 -i file.ico 指定图标 --add-data SRC;DEST 在程序中用到的其他（非二进制）文件，不建议用 --hidden-import MODULENAME 在程序中隐式导入的库，可多次使用 --exclude-module MODULENAME 不希望导入的库，可多次使用 更详细的使用方法可以参看官方手册，本文不再赘述。","text":"PyInstaller基本使用方法 下面列举几个常见的可选参数： pyinstaller [options] my_script.py[options]: -h 显示帮助并退出 -D 生成一个文件夹，其中包含一个可执行文件（默认） -F 生成单个可执行文件 -w 生成一个无命令行界面的程序 -i file.ico 指定图标 --add-data SRC;DEST 在程序中用到的其他（非二进制）文件，不建议用 --hidden-import MODULENAME 在程序中隐式导入的库，可多次使用 --exclude-module MODULENAME 不希望导入的库，可多次使用 更详细的使用方法可以参看官方手册，本文不再赘述。 问题：打包后程序体积太大 由于笔者使用的是Anaconda作为Python环境，最近有个项目需要将Python代码打包成可执行文件。在之前的需求中没有使用科学计算库（例如numpy、pandas等）所以打包出来的结果也就10M左右，完全可以接受。但是最近在项目中使用了numpy等科学计算库之后，程序打包的结果接近1G左右，这样的打包结果简直令人窒息。这里主要有两个解决方案：1. 使用虚拟环境打包 2. 使用纯净版本的Python打包 推荐使用第二种方法。 使用虚拟环境打包 Python虚拟环境有Anaconda、virtualenv以及pipenv，但是Anaconda打包的程序体积太大，所以我们需要避开Anaconda。本文主要介绍的虚拟环境是pipenv，它是pip和virtualenv的结合，所以使用起来也更加方便，另外值得一提的是，pipenv和requests（Python网络请求包）是同一个作者。 Pipenv使用教程 安装pipenv pip install pipenv 创建虚拟环境 首先进入你的项目所在目录，然后输入下面的命令安装虚拟环境： pipenv install --python path\\to\\python 注： &quot;path\\to\\python&quot;是python.exe可执行程序的路径，可以是Anaconda版本的Python也可以是本地纯净版本的Python。因为我们需要避开Anaconda所以读者最好自己前往Python官网下载纯净版本的Python，然后通过上面命令安装虚拟环境。 激活虚拟环境 pipenv shell 输入上面的命令之后我们就进入了虚拟环境，然后我们可以像平时一样使用Python。 安装依赖库 在虚拟环境下安装 Pyinstaller 和你自己的脚本依赖的第三方库 pipenv install pyinstallerpipenv install numpypipenv install pandaspipenv install matplotlib 当然也可以进入虚拟环境后直接使用pip进行安装。 问题：ModuleNotFoundError pyinstaller打包成功后运行程序提示ModuleNotFoundError: No module named 'distutils’错误解决办法。 问题分析：由于pandas需要调用distutils库（Python自带），但是最新版本的virtualenv和pyinstaller存在兼容性问题，即打包的时候会遗漏distutils库，目前这个BUG还没有得到解决。当然Github上也有很多解决办法，具体可以参考这里。 解决方法：由于16.1版本的virtualenv可以正常使用，所以我们可以降级安装virtualenv。1. 首先我们需要卸载新版的virtualenv，然后安装16.1版本的virtualenv；2. 卸载原来的虚拟环境，然后使用16.1版本的virtualenv重新安装虚拟环境（使用pipenv安装）；3. 最后安装相关的依赖库重新打包即可。 使用纯净版本的Python打包 由于本地已经存在Anaconda环境，所以如果我们想再安装纯净版本的Python可以使用虚拟机，但是使用虚拟机安装Python需要安装虚拟机软件，然后安装Windows镜像，步骤过于繁琐，所以我们可以另辟蹊径——使用临时环境变量。 首先前往Python官网下载纯净的Python环境并安装到本地，然后我们可以通过下面的批处理脚本设置临时环境变量，为了方便可以将下面的脚本保存到batch脚本文件中（例如命名为set_python_path.bat），并将该脚本放到c:\\windows中，这样我们就可以在任何目录下切换Python环境了。 set_python_path.bat: @echo offset conda_path=d:\\anaconda3set python_path=d:\\program files\\python36rem Approach one: Replace the anaconda installation path with the new python installation pathcall call set path=%%path:%conda_path%=%python_path%%%rem Approach two: Let the new python installation path overwrite the anaconda installation pathrem set path=%python_path%;%python_path%\\Scripts;%path% 上面的代码中conda_path是Anaconda的安装路径，python_path是纯净版本的Python安装路径。设置临时的Python环境变量主要有两种方法：方法一是替换环境变量中Anaconda安装路径为纯净版本的Python安装路径；方法二是将纯净版本的Python安装路径放到所有路径之前这样就可以覆盖Anaconda的路径。 我们在运行完上面的脚本之后就可以切换到新的Python环境，然后安装Pyinstaller和相关的依赖包进行打包。 最后 使用上面两者方法之一，原先接近1G的程序，现在只有40M左右。 Enjoy it!","categories":[{"name":"python","slug":"python","permalink":"http://chiang97912.github.io/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://chiang97912.github.io/tags/python/"},{"name":"Pyinstaller","slug":"Pyinstaller","permalink":"http://chiang97912.github.io/tags/Pyinstaller/"},{"name":"python程序打包","slug":"python程序打包","permalink":"http://chiang97912.github.io/tags/python%E7%A8%8B%E5%BA%8F%E6%89%93%E5%8C%85/"},{"name":"pipenv","slug":"pipenv","permalink":"http://chiang97912.github.io/tags/pipenv/"},{"name":"virtualenv","slug":"virtualenv","permalink":"http://chiang97912.github.io/tags/virtualenv/"}]},{"title":"使用VIM作为IDE","slug":"使用VIM作为IDE","date":"2019-08-01T02:40:24.000Z","updated":"2025-12-13T12:18:36.570Z","comments":true,"path":"2019/08/01/使用VIM作为IDE/","permalink":"http://chiang97912.github.io/2019/08/01/%E4%BD%BF%E7%94%A8VIM%E4%BD%9C%E4%B8%BAIDE/","excerpt":"Windows环境 插件管理器 VIM的插件管理器主要有vim-plug和vundle","text":"Windows环境 插件管理器 VIM的插件管理器主要有vim-plug和vundle Vim-plug 首先下载vim-plug，可以去github下载 下载完成后解压压缩包将plug.vim复制到vim安装目录下的autoload文件夹下，即可完成vim-plug的安装。 使用管理员身份运行gvim,，然后点击gvim的“编辑”——“启动设定”，打开_vimrc配置文件。 插件安装示例 在_vimrc中添加如下的内容，这里以vim-plug下载nerdtree插件为例。设置完成后保存设置。 call plug#begin(&#x27;~/.vim/plugged&#x27;) &quot;插件保存的目录Plug &#x27;scrooloose/nerdtree&#x27;, &#123;&#x27;on&#x27;: &#x27;NERDTreeToggle&#x27;&#125; &quot;NERDTree插件call plug#end() 常用操作 命令 解释 :PlugStatus 查看插件安装状态 :PlugInstall 安装在_vimrc中配置的插件。注意命令的大小写，执行后vim-plug会自动克隆并安装插件 :PlugUpdate 更新插件 :PlugClean 清理插件（需要先在_vimrc中删除或注释） :PlugUpgrade 更新vim-plug Vundle 首先下载vundle，可以去github下载 下载完成后解压压缩包到vim安装目录下的bundle文件夹下（没有就新建），即可完成vundle的安装。 使用管理员身份运行gvim,，然后点击gvim的“编辑”——“启动设定”，打开_vimrc配置文件。 插件安装示例 在_vimrc中添加如下的内容，这里以vundle下载nerdtree插件为例。设置完成后保存设置。 set rtp+=~/.vim/bundle/Vundle.vimcall vundle#begin()Plugin &#x27;VundleVim/Vundle.vim&#x27;Plugin &#x27;scrooloose/nerdtree&#x27;call vundle#end() 常用操作 命令 解释 :PluginList 查看插件安装状态 :PluginInstall 安装在_vimrc中配置的插件。注意命令的大小写，执行后vundle会自动克隆并安装插件 :PluginUpdate 更新插件 :PluginClean 清理插件（需要先在_vimrc中删除或注释） :PluginSearch 搜索插件，例如:PluginSearch xml可以搜到xml相关的插件 Python自动补全 Python自动补全插件这里主要使用jedi-vim，具体配置步骤如下： 在_vimrc中添加如下的内容，设置完成后保存设置。 &quot;插件管理 vim-plugcall plug#begin(&#x27;~/.vim/plugged&#x27;) &quot;插件保存的目录Plug &#x27;davidhalter/jedi-vim&#x27;,call plug#end() 然后在gvim中输入:PlugInstall安装jedi-vim插件 注意: Vim和版本一定要和Python相匹配，即32位Vim配32位Python, 64位Vim配64位Python。 Vim使用Python编译的版本一定要和电脑上安装的Python版本对应，可能会出现如下错误： Error: jedi-vim failed to initialize Python: jedi-vim requires Vim with support for Python 2 or 3. (in function jedi#init_python[4]..48_init_python, line 10) 或者在gvim中输入:python3 print(&quot;Hello world&quot;)出现如下类似的错误： E370: Could not load library python37.dllE263: Sorry, this command is disabled, the Python library could not be loaded. 那么可以在_vimrc中添加如下命令指定Python3版本： set pythonthreedll=python36.dll 由于笔者安装的vim8.1使用的是Python3.7编译，但是电脑环境装的是Python3.6，我们可以通过上面配置来解决Windows环境中gvim不支持python3.6的问题。 C/C++自动补全 C/C++自动补全插件这里主要使用ctags + OmniCppComplete方案，具体配置步骤如下： Ctags 全名 Exuberant Ctags，是一个独立的程序。它可以为各种语言的源代码生成语言元素（language object）索引文件。对于 C/C++ 而言，就是把源代码中的各种宏、函数、类、类成员等等元素和它们的相关信息生成索引文件，供其它程序使用。OmniCppComplete 是专为 C/C++ 编写的OmniComplete一个补全脚本，它根据 Ctags 生成的索引文件对代码进行补全。 安装Ctags 从Ctags官网下载 Ctags 可执行文件 将下载到的文件（仅 EXE 文件即可）解压到一个目录，例如 C:/ctags 将该目录加入环境变量 PATH 生成索引文件 以生成 C++ 标准库索引文件为例： 下载专为 Ctags 修改过的 libstdc++ 头文件 将其解压到一个目录，例如 C:/ctags/cpp_src 使用命令行进入 D:/ctags/cpp_src 后执行： ctags -R --sort=1 --c++-kinds=+p --fields=+iaS --extra=+q --language-force=C++ -f cpp . 建议将上一步生成的 C:/ctags/cpp_src/cpp 文件放到一个专门放置索引文件的目录以便后面的统一设置，例如放到 C:/ctags/tags 其它库的索引文件也可以依法炮制，只需切换到该库的 include 文件夹，执行： ctags -R --sort=yes --c++-kinds=+p --fields=+iaS --extra=+q --language-force=C++ -f &lt;文件名&gt; . 安装OmniCppComplete 使用插件管理器安装OmniCppComplete 修改VIM配置文件_vimrc，加入如下内容： &quot; ctags 索引文件set tags+=C:/ctags/tags/cpp &quot; 指定tags存放路径&quot; OmniCppCompletelet OmniCpp_NamespaceSearch = 1let OmniCpp_GlobalScopeSearch = 1let OmniCpp_ShowAccess = 1let OmniCpp_ShowPrototypeInAbbr = 1 &quot; 显示函数参数列表let OmniCpp_MayCompleteDot = 1 &quot; 输入 . 后自动补全let OmniCpp_MayCompleteArrow = 1 &quot; 输入 -&gt; 后自动补全let OmniCpp_MayCompleteScope = 1 &quot; 输入 :: 后自动补全let OmniCpp_DefaultNamespaces = [&quot;std&quot;, &quot;_GLIBCXX_STD&quot;]&quot; 自动关闭补全窗口au CursorMovedI,InsertLeave * if pumvisible() == 0|silent! pclose|endifset completeopt=menuone,menu,longestfiletype plugin indent on 在插入模式编辑 C/C++ 源文件时按下 . 或 -&gt; 或 ::，或者手动按下 Ctrl+X Ctrl+O 后就会弹出自动补全窗口，此时可以用 Ctrl+N 和 Ctrl+P 上下移动光标进行选择。 自动生成tags文件 omni插件的补全是依赖于tags文件的，因此需要我们手动建立tags文件: ctags -R --sort=yes --c++-kinds=+p --fields=+iaS --extra=+q --language-force=C++ 我们可以通过下面的代码让vim在保存文件后自动生成tags文件: au BufWritePost *.c,*.cpp,*.cc,*.h silent! !ctags -R --sort=yes --c++-kinds=+p --fields=+iaS --extra=+q --language-force=C++ 其中silent!表示静默运行命令，不然每次保存文件的时候，Vim 总是会有一个”ctags 执行完毕“的提示，按任意键确认。 生产力插件 Python代码检查：flake8 实用插件管理器安装flake8 配置_vimrc文件 au BufWritePost *.py call Flake8() 注释/取消注释：vim-commentary 这个插件可以快速注释与反注释多行内容, 但是它的注释符使用的是 commentstring, 默认是 /* %s */, 但这个值满足不了Python 和 Shell这样的语言, 在 _vimrc 添加如下内容 &quot;为python和shell等添加注释autocmd FileType python,shell,coffee set commentstring=#\\ %s&quot;修改注释风格autocmd FileType java,c,cpp set commentstring=//\\ %s 普通模式下gcc 指令可以注释/取消注释 可视模式下gc 命令可以注释/撤销注释 缩进提示：indentLine indentLine是一款Vim下用于显示缩进指示线的插件。对于Python、Golang等靠代码缩进来标识代码块的语言来说，indentLine提供的缩进指示功能非常有用。indentLine安装之后即可使用，不需要额外的配置。 设置indentLine： set list lcs=tab:\\|\\ &quot; 最后面有空格let g:indentLine_leadingSpaceChar = &#x27;.&#x27;let g:indentLine_leadingSpaceEnabled = 1 实用配置 代码折叠 &quot; Enable foldingset foldmethod=syntax &quot; 语法折叠set foldlevelstart=99 &quot; 关闭自动折叠 zc 关闭折叠 zo 打开折叠 za 打开/关闭折叠互相切换 系统剪贴板 通常Vim会忽视系统剪贴板，而使用自带的剪贴板。但是有时候你想从Vim之外的程序中剪切、复制、粘贴文本。你可以通过这行代码访问你的系统剪贴板： set clipboard=unnamed 禁止生成缓存文件 每次输入保存命令之后系统都会生成以.un~和.bak结尾的文件，我们可以通过下面的配置禁止vim生成undo文件和备份文件: set noundofileset nobackupset noswapfile 主题插件 主题插件推荐使用flazz/vim-colorschemes 可以使用vim-plug安装，然后再配置_vimrc文件，例如在配置文件中增加下面的代码： colorscheme wombat Mac &amp; Linux环境 插件管理器安装 使用如下命令安装plug-vim插件: mkdir -vp ~/.vim/autoload/cd ~/.vim/autoload/wget https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim 安装示例 仍然以NERDTree插件安装为例： 编辑配置文件vim ~/.vimrc &quot;插件管理 vim-plugcall plug#begin(&#x27;~/.vim/plugged&#x27;) &quot;插件保存的目录Plug &#x27;scrooloose/nerdtree&#x27;, &#123;&#x27;on&#x27;: &#x27;NERDTreeToggle&#x27;&#125; &quot;NERDTree插件call plug#end() 其他操作和Windows系统相同。","categories":[{"name":"VIM","slug":"VIM","permalink":"http://chiang97912.github.io/categories/VIM/"}],"tags":[{"name":"Python","slug":"Python","permalink":"http://chiang97912.github.io/tags/Python/"},{"name":"VIM","slug":"VIM","permalink":"http://chiang97912.github.io/tags/VIM/"},{"name":"C/C++","slug":"C-C","permalink":"http://chiang97912.github.io/tags/C-C/"}]},{"title":"Mask矩阵在深度学习中的应用","slug":"Mask矩阵在深度学习中的应用","date":"2019-07-31T17:20:49.000Z","updated":"2025-12-13T14:02:51.261Z","comments":true,"path":"2019/08/01/Mask矩阵在深度学习中的应用/","permalink":"http://chiang97912.github.io/2019/08/01/Mask%E7%9F%A9%E9%98%B5%E5%9C%A8%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8/","excerpt":"定义 mask矩阵是一个由0和1组成的矩阵。在NLP中，一个常见的问题是输入序列长度不等，而mask可以帮助我们处理。虽然RNN等模型可以处理不定长的输入，但是在实践中，需要对输入中长度较短的句子进行填充，即在句尾填充0占位，转换成固定大小的tensor，方便矩阵操作。","text":"定义 mask矩阵是一个由0和1组成的矩阵。在NLP中，一个常见的问题是输入序列长度不等，而mask可以帮助我们处理。虽然RNN等模型可以处理不定长的输入，但是在实践中，需要对输入中长度较短的句子进行填充，即在句尾填充0占位，转换成固定大小的tensor，方便矩阵操作。 举个例子： case 1: I like cats.case 2: He does not like cats. 假设默认的序列长度是5，一般会对case 1做pad处理，变成 I like cats &lt;PAD&gt; &lt;PAD&gt; 在上述例子数字编码后，开始做embedding，而pad也会有embedding向量，但pad本身没有实际意义，参与训练可能还是有害的。因此，有必要维护一个mask tensor来记录哪些是真实的value，上述例子的两个mask如下： 1 1 1 0 01 1 1 1 1 后续再梯度传播中，mask起到了过滤的作用。 使用TensorFlow实现上述过程： import tensorflow as tfmaxlen = 5lengths = [[3, 5, 4], [1, 3, 2]]mask = tf.cast(tf.sequence_mask(lengths, maxlen), tf.float32)sess = tf.Session()mask = sess.run(mask)print(mask) 运行结果： [[[1. 1. 1. 0. 0.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 0.]] [[1. 0. 0. 0. 0.] [1. 1. 1. 0. 0.] [1. 1. 0. 0. 0.]]] 作用 使用mask矩阵是为了让那些被mask掉的tensor不会被更新。一个tensor T和同样大小的mask矩阵M相乘在梯度回传的时候，T对应mask为0的地方梯度为0。因此权重不会被更新。 语言模型中可以防止未来信息泄露 在语言模型中，常常需要从上一个词预测下一个词，而现阶段attention是标配，比如Transformer中的self attention，如果不做mask，在decoder的时候很容易把下一个词的信息泄露了，即按上诉例子，不能在预测like这个词时已经知道like后面的词了。使用mask矩阵可以很好的解决这一问题。 TensorFlow生成mask对角矩阵： import tensorflow as tfimport matplotlib.pyplot as pltdef subsequent_mask(size): &quot;Mask out subsequent positions.&quot; attn_mask = tf.ones([size, size]) mask = tf.matrix_band_part(attn_mask, -1, 0) return mask sess = tf.Session()mask = sess.run(subsequent_mask(10))print(mask)# Display matrixplt.matshow(mask)plt.show() 运行结果： [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.] [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.] [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] 参考 [1]: Mask矩阵在深度学习中有哪些应用场景？ [2]: 浅谈mask矩阵","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://chiang97912.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"NLP","slug":"深度学习/NLP","permalink":"http://chiang97912.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://chiang97912.github.io/tags/tensorflow/"},{"name":"Python","slug":"Python","permalink":"http://chiang97912.github.io/tags/Python/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chiang97912.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://chiang97912.github.io/tags/NLP/"}]},{"title":"Git常用操作记录","slug":"Git常用操作记录","date":"2019-03-24T07:26:30.000Z","updated":"2025-12-13T14:03:07.177Z","comments":true,"path":"2019/03/24/Git常用操作记录/","permalink":"http://chiang97912.github.io/2019/03/24/Git%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E8%AE%B0%E5%BD%95/","excerpt":"基本操作 Git基本配置（git config） git config可以配置git的参数，可以使用git config --list查看已经配置的git参数。其中有三个级别的保存位置，–system、–global、–local，分别表示所有用户（本系统）、当前用户（全局）、本地配置（当前目录），默认使用–local。","text":"基本操作 Git基本配置（git config） git config可以配置git的参数，可以使用git config --list查看已经配置的git参数。其中有三个级别的保存位置，–system、–global、–local，分别表示所有用户（本系统）、当前用户（全局）、本地配置（当前目录），默认使用–local。 配置用户名及邮箱 在使用Git提交前，必须配置用户名和邮箱，这些信息会永久保存到历史记录中。 git config --global user.name &quot;Peter&quot;git config --global user.email Peter@gmail.com 创建Git仓库（git init） 可以直接调用git init初始化当前目录，即创建Git仓库。 获取Git仓库（git clone） 如果需要克隆远程仓库，可以使用git clone，比如： git clone https://github.com/tensorflow/tensorflow.git 提交更新 Git中每个文件都有三种状态：committed、staged、modified。它们之间关系如下： modified=&gt; staged=&gt; committed 如果你在本地修改了文件，则文件状态就变成modified；如果使用git add命令，文件的状态变成staged；如果使用git commit命令，文件的状态就变成commited。 还有一种文件状态，未跟踪状态（unversioned/untracked），通过使用git add可以把未跟踪状态变更为staged；通过git rm可以将staged或者committed状态变为未跟踪状态。 git status: 查看在你上次提交之后是否有修改。 git add: 将想要快照的内容写入缓存区 git commit: 将缓存区内容添加到仓库中 git rm: 如果只是简单地从工作目录中手工删除文件，运行 git status 时就会在 Changes not staged for commit 的提示。要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除，然后提交。可以用以下命令完成此项工作 查看提交历史（git log） 使用git log查看当前工程的所有提交的日志。 git log --stat # 仅显示摘要选项git log --pretty=oneline # 定制记录格式git log --graph # 图像化分支和版本更新 远程仓库 可以使用git remote查看当前的远程库。 git remote -v可以显示对应的克隆地址。（对于多个远程仓库很有用） 添加远程仓库 git remote add [short_name] [url]可以添加新的远程仓库。 例如： git remote add origin https://github.com/username/projectname.git 修改远程仓库 方法一：修改命令 git remote set-url origin [url] 注：[url]表示你的Github仓库地址。 方法二：先删除后添加 git remote rm origingit remote add origin [url] 方法三：直接修改config文件 从远程仓库抓取数据 git fetch [remote-name]可以从远程仓库抓取数据到本地。 也可以使用git pull 推送数据到远程仓库 git push [remote_name] [branch_name] 默认使用origin和master。 默认使用origin和master。 ⭐强制推送数据到远程仓库⭐ 我们可以使用git push --force origin master强制推送本地代码到远程仓库，这意味着将覆盖掉远程仓库的代码。（注：–force参数也可以使用-f替代） 查看远程仓库信息 git remote show origin 远程仓库的删除和重命名 git remote rename [old_name] [new_name]git remote rm [remote_name] 修改提交信息 第一种情况 如果需要修改的提交信息是最后一次提交并且没有推送（push），那么可以使用如下命令进行修改： git commit --amend -m &quot;your new comment&quot; 第二种情况 如果需要修改的不是最后一次提交的提交信息，那么我们需要使用git rebase命令 确定修改的提交 我们可以使用git log命令查看需要修改的提交是倒数第几次提交 指定变基对象 使用命令git rebase -i HEAD~n回退到倒数第n次提交。运行命令后会进入到编辑器，出现n条commit信息，要修改哪条就将其前面pick改成edit,保存并退出。 修改提交信息 经过变基之后我们可以像第一种情况一样使用git commit --amend命令修改提交信息（可以直接输入git commit --amend然后进入编辑器修改提交内容也可以使用-m直接指定新的提交内容）。 注：变基（rebase）和合并（merge）都是对分支进行操作，它们的不同之处在于变基可以修改历史。执行git commit --amend后会产生一个新的分支，如果需要保留当前分支则需要删除之前的分支，然后将当前分支命名为之前的分支名。 执行变基 执行git rebase –continue，如果提示Successfully rebased and updated则表示变基成功。 分支操作 显示所有分支 使用git branch可显示当前所有分支。 可以使用–merged和–no-merged查看已经合并、未合并的分支。 创建及切换分支 git branch test # 创建testing 分支git checkout test # 切换到testing分支 也可以使用下面命令直接切换并创建分支 git checkout -b test 注意切换分支时请保持工作目录没有未提交的修改。Git鼓励使用分支，处理完问题之后合并分支即可。 分支合并 将test分支合并到master（主分支）上，需要通过下面命令： git checkout mastergit merge test 合并之后可以使用git branch -d test删除分支。 如果合并时存在冲突，需要手工修改。 ⭐删除分支⭐ 删除test分支 git branch -d test 主干更换 将某个分支设置为主干master，我们可以删除 master，把 dev head 打标成 master。 git branch -D mastergit checkout -b master","categories":[{"name":"Git","slug":"Git","permalink":"http://chiang97912.github.io/categories/Git/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://chiang97912.github.io/tags/Git/"}]},{"title":"Tensorflow共享变量机制理解与应用","slug":"tensorflow共享变量机制理解与应用","date":"2019-02-25T00:33:21.000Z","updated":"2025-12-13T12:14:34.008Z","comments":true,"path":"2019/02/25/tensorflow共享变量机制理解与应用/","permalink":"http://chiang97912.github.io/2019/02/25/tensorflow%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F%E6%9C%BA%E5%88%B6%E7%90%86%E8%A7%A3%E4%B8%8E%E5%BA%94%E7%94%A8/","excerpt":"创建变量 Tensorflow创建变量有两种方式： tf.get_variable() tf.Variable() 它们的区别如下： 在 tf.name_scope下时，tf.get_variable()创建的变量名不受 name_scope 的影响，而且在未指定共享变量时，如果重名会报错，tf.Variable()会自动检测有没有变量重名，如果有则会自行处理。","text":"创建变量 Tensorflow创建变量有两种方式： tf.get_variable() tf.Variable() 它们的区别如下： 在 tf.name_scope下时，tf.get_variable()创建的变量名不受 name_scope 的影响，而且在未指定共享变量时，如果重名会报错，tf.Variable()会自动检测有没有变量重名，如果有则会自行处理。 import tensorflow as tfwith tf.name_scope(&#x27;name_scope_x&#x27;): var1 = tf.get_variable(name=&#x27;var1&#x27;, shape=[1], dtype=tf.float32) var3 = tf.Variable(name=&#x27;var2&#x27;, initial_value=[2], dtype=tf.float32) var4 = tf.Variable(name=&#x27;var2&#x27;, initial_value=[2], dtype=tf.float32)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(var1.name, sess.run(var1)) print(var3.name, sess.run(var3)) print(var4.name, sess.run(var4))# 输出结果：# var1:0 [-0.30036557] 可以看到前面不含有指定的&#x27;name_scope_x&#x27;# name_scope_x/var2:0 [ 2.]# name_scope_x/var2_1:0 [ 2.] 可以看到变量名自行变成了&#x27;var2_1&#x27;，避免了和&#x27;var2&#x27;冲突 如果使用tf.get_variable()创建变量，且没有设置共享变量，重名时会报错 import tensorflow as tfwith tf.name_scope(&#x27;name_scope_1&#x27;): var1 = tf.get_variable(name=&#x27;var1&#x27;, shape=[1], dtype=tf.float32) var2 = tf.get_variable(name=&#x27;var1&#x27;, shape=[1], dtype=tf.float32)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(var1.name, sess.run(var1)) print(var2.name, sess.run(var2))# ValueError: Variable var1 already exists, disallowed. Did you mean # to set reuse=True in VarScope? Originally defined at:# var1 = tf.get_variable(name=&#x27;var1&#x27;, shape=[1], dtype=tf.float32) 共享变量 基础写法 如果要共享变量，需要使用tf.variable_scope() import tensorflow as tfwith tf.variable_scope(&#x27;variable_scope_y&#x27;) as scope: var1 = tf.get_variable(name=&#x27;var1&#x27;, shape=[1], dtype=tf.float32) scope.reuse_variables() # 设置共享变量 var1_reuse = tf.get_variable(name=&#x27;var1&#x27;) var2 = tf.Variable(initial_value=[2.], name=&#x27;var2&#x27;, dtype=tf.float32) var2_reuse = tf.Variable(initial_value=[2.], name=&#x27;var2&#x27;, dtype=tf.float32)with tf.Session() as sess: sess.run(tf.global_variables_initializer()) print(var1.name, sess.run(var1)) print(var1_reuse.name, sess.run(var1_reuse)) print(var2.name, sess.run(var2)) print(var2_reuse.name, sess.run(var2_reuse))# 输出结果：# variable_scope_y/var1:0 [-1.59682846]# variable_scope_y/var1:0 [-1.59682846] 可以看到变量var1_reuse重复使用了var1# variable_scope_y/var2:0 [ 2.]# variable_scope_y/var2_1:0 [ 2.] 或者如下形式： with tf.variable_scope(&#x27;foo&#x27;) as foo_scope: v = tf.get_variable(&#x27;v&#x27;, [1])with tf.variable_scope(&#x27;foo&#x27;, reuse=True): v1 = tf.get_variable(&#x27;v&#x27;) 还可以像下面这样编写： with tf.variable_scope(&#x27;foo&#x27;) as foo_scope: v = tf.get_variable(&#x27;v&#x27;, [1])with tf.variable_scope(foo_scope, reuse=True): v1 = tf.get_variable(&#x27;v&#x27;) 更优雅的写法 之前的几种写法是在重复使用（非第一次使用）的时候设置reuse=True来再次调用共享变量作用域（variable_scope），这是一种比较笨的方式，下面使用tf.AUTO_REUSE的写法或许更加优雅： with tf.variable_scope(&#x27;foo&#x27;, reuse=tf.AUTO_REUSE): v = tf.get_variable(&#x27;v&#x27;, [1]) v1 = tf.get_variable(&#x27;v&#x27;) 实例： import numpy as npimport tensorflow as tfdef convolution(in_put, in_channel, out_channel): with tf.variable_scope(name_or_scope=&#x27;&#x27;, reuse=tf.AUTO_REUSE): weights = tf.get_variable(name=&quot;weights&quot;, shape=[2, 2, in_channel, out_channel], initializer=tf.contrib.layers.xavier_initializer_conv2d()) output = tf.nn.conv2d(input=in_put, filter=weights, strides=[1, 1, 1, 1], padding=&quot;SAME&quot;) return outputdef main(): with tf.Graph().as_default(): input_x = tf.placeholder(dtype=tf.float32, shape=[1, 4, 4, 1]) for _ in range(5): output = convolution(input_x, 1, 1) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) _output = sess.run([output], feed_dict=&#123;input_x: np.random.uniform(low=0, high=255, size=[1, 4, 4, 1])&#125;) print(_output)if __name__ == &quot;__main__&quot;: main() reuse参数使用 当参数reuse=False，函数get_variable（）表示创建变量 import tensorflow as tfwith tf.variable_scope(&quot;foo&quot;, reuse=False): v = tf.get_variable(&quot;v&quot;, [1], initializer=tf.constant_initializer(1.0)) v1 = tf.get_variable(&quot;v&quot;, [1])# 输出结果：# ValueError: Variable foo/v already exists, disallowed. # Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? 当参数reuse=True，函数get_variable（）表示获取变量 import tensorflow as tfwith tf.variable_scope(&quot;foo&quot;): v = tf.get_variable(&quot;v&quot;, [1], initializer=tf.constant_initializer(1.0))with tf.variable_scope(&quot;foo&quot;, reuse=True): v1 = tf.get_variable(&quot;v&quot;, [1])print(v1 == v)# 输出结果：True 在tf.variable_scope()函数中，设置reuse=True时，在其命名空间&quot;foo&quot;中执行函数get_variable()时，表示获取变量&quot;v&quot;。若在该命名空间中还没有该变量，则在获取时会报错，实例如下： import tensorflow as tf with tf.variable_scope(&quot;foo&quot;, reuse=True): v1 = tf.get_variable(&quot;v&quot;,[1])# 输出结果：# ValueError: Variable foo/v does not exist, or was not created with tf.get_variable(). # Did you mean to set reuse=tf.AUTO_REUSE in VarScope? 参考 [1]: tensorflow里面name_scope, variable_scope等如何理解？ [2]: tf.AUTO_REUSE作用 [3]: TensorFlow中变量管理reuse参数的使用","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://chiang97912.github.io/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://chiang97912.github.io/tags/tensorflow/"}]},{"title":"Ubuntu环境MYSQL乱码问题修复","slug":"Ubuntu环境MYSQL乱码问题修复","date":"2018-12-09T05:10:29.000Z","updated":"2025-12-13T12:14:03.174Z","comments":true,"path":"2018/12/09/Ubuntu环境MYSQL乱码问题修复/","permalink":"http://chiang97912.github.io/2018/12/09/Ubuntu%E7%8E%AF%E5%A2%83MYSQL%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98%E4%BF%AE%E5%A4%8D/","excerpt":"首先声明笔者使用的服务器是Ubuntu16.04，数据库安装的是Mysql 5.7。初始的mysql默认字符集是latin1，如果向数据库中插入中文就会出现乱码，下面我们通过修改配置文件的方式修改mysql的默认编码。","text":"首先声明笔者使用的服务器是Ubuntu16.04，数据库安装的是Mysql 5.7。初始的mysql默认字符集是latin1，如果向数据库中插入中文就会出现乱码，下面我们通过修改配置文件的方式修改mysql的默认编码。 修改配置文件 修改[mysqld] 找到文件/etc/mysql/mysql.conf.d/mysqld.cnf 中的[mysqld]并在其最后面追加如下代码： character-set-server=utf8 修改[mysql] 找到文件/etc/mysql/conf.d/mysql.cnf中的[mysql]并在其最后面追加如下代码： default-character-set=utf8 修改[client] 找到文件/etc/mysql/debian.cnf中的[client]并在其最后面追加如下代码： default-character-set=utf8 重启MYSQL service mysql restart 查看字符集 mysql -u root -p show variables like &#x27;%character%&#x27;; 其他问题 笔者在做完如上配置之后发现通过navicat连接mysql查看内容还是出现乱码，但是其他地方返回的数据均能够正常显示。出现上述情况可以按照如下步骤进行操作： 右键–&gt;编辑连接–&gt;高级 将编码方式设置为自动","categories":[{"name":"服务器配置","slug":"服务器配置","permalink":"http://chiang97912.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://chiang97912.github.io/tags/mysql/"}]},{"title":"LAMP环境搭建以及MYSQL远程访问配置","slug":"LAMP环境搭建以及MYSQL远程访问配置","date":"2018-12-04T03:51:55.000Z","updated":"2025-12-13T12:16:28.792Z","comments":true,"path":"2018/12/04/LAMP环境搭建以及MYSQL远程访问配置/","permalink":"http://chiang97912.github.io/2018/12/04/LAMP%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%BB%A5%E5%8F%8AMYSQL%E8%BF%9C%E7%A8%8B%E8%AE%BF%E9%97%AE%E9%85%8D%E7%BD%AE/","excerpt":"首先声明笔者使用的服务器是Ubuntu16.04。下面正式进入配置教程：","text":"首先声明笔者使用的服务器是Ubuntu16.04。下面正式进入配置教程： LAMP环境搭建 安装Apache2 sudo apt-get install apache2 检查apache2是否安装成功 apache2 –v 安装PHP7 sudo apt-get install php 查看PHP版本并检查PHP是否按照成功 php –v 安装MYSQL sudo apt-get install mysql-serversudo apt-get install mysql-client 安装配置组件 sudo apt-get install libapache2-mod-phpsudo apt-get install libapache2-mod-auth-mysql # 现在这个组件作者以及放弃维护了，因为新版apache以及包含这个模块所实现的功能，可以不用安装sudo apt-get install php-mysqlsudo apt-get install php-gd # php图形库 启用mod_rewrite模块 service apache2 restart 重启Apache2和MYSQL sudo service apache2 restartsudo service mysql restart 测试PHP 创建一个php文件来测试环境是否安装成功，PHP文件（文件名为test.php）内容： &lt;?php echo phpinfo();?&gt; 通过IP/test.php查看是否连接成功。 端口允许 通过上述步骤我们可能还是不能通过IP地址访问服务器，那是因为阿里云默认是没有开放80端口的，我们需要前往阿里云的控制台将80端口添加到安全组。 MYSQL远程访问配置 修改MYSQL配置 我们通过修改MYSQL的配置文件运行允许所有主机访问服务器上的MYSQL数据库： 使用vim编辑mysql的配置文件/etc/mysql/mysql.conf.d/mysqld.cnf vim /etc/mysql/mysql.conf.d/mysqld.cnf 然后修改文件中的bind-address的值为0.0.0.0 修改MYSQL用户访问权限 首先进入MYSQL数据库，使用mysql数据库: use mysql; 然后: update user set host = &#x27;%&#x27; where user = &#x27;root&#x27;; 查看MYSQL用户权限： select host, user from user; have fun!","categories":[{"name":"服务器配置","slug":"服务器配置","permalink":"http://chiang97912.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/"}],"tags":[{"name":"sql","slug":"sql","permalink":"http://chiang97912.github.io/tags/sql/"},{"name":"lamp","slug":"lamp","permalink":"http://chiang97912.github.io/tags/lamp/"},{"name":"linux","slug":"linux","permalink":"http://chiang97912.github.io/tags/linux/"},{"name":"mysql","slug":"mysql","permalink":"http://chiang97912.github.io/tags/mysql/"}]},{"title":"PHP接口返回500错误状态码解决方法","slug":"PHP接口返回500错误状态码解决方法","date":"2018-11-17T14:12:43.000Z","updated":"2025-12-13T12:16:08.956Z","comments":true,"path":"2018/11/17/PHP接口返回500错误状态码解决方法/","permalink":"http://chiang97912.github.io/2018/11/17/PHP%E6%8E%A5%E5%8F%A3%E8%BF%94%E5%9B%9E500%E9%94%99%E8%AF%AF%E7%8A%B6%E6%80%81%E7%A0%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/","excerpt":"背景 最近在用PHP写用户登录接口，但是将PHP代码部署到生产环境却发生了错误，用浏览器访问接口产生错误状态码500如下： HTTP ERROR 500 这个错误是由于PHP代码存在错误引起的，但是默认PHP是关闭错误提示的。如果想要知道代码的错误必须先打开PHP的错误显示功能。","text":"背景 最近在用PHP写用户登录接口，但是将PHP代码部署到生产环境却发生了错误，用浏览器访问接口产生错误状态码500如下： HTTP ERROR 500 这个错误是由于PHP代码存在错误引起的，但是默认PHP是关闭错误提示的。如果想要知道代码的错误必须先打开PHP的错误显示功能。 解决方法 首先声明笔者使用的生产环境是Ubuntu 16.04。 第一步：先找到PHP的配置文件php.ini文件所在位置，这里笔者的Ubuntu中的php7的php.ini存在于路径/etc/php/7.0/apache2/下面，我们只需要使用vi/vim从上述路径下打开php.ini文件； 第二步：查找并修改php.ini文件中的display_errors和display_startup_errors中的值为On即可； 修改后的内容如下： #修改你的php.ini文件display_errors = Ondisplay_startup_errors = On 第三步：重启php-fpm。 /etc/init.d/php7.0-fpm restart 更多版本php操作详见如下链接，另外还有appache2相关操作见如下链接。 到此为止就可以查看到错误信息了。","categories":[{"name":"PHP","slug":"PHP","permalink":"http://chiang97912.github.io/categories/PHP/"}],"tags":[{"name":"PHP","slug":"PHP","permalink":"http://chiang97912.github.io/tags/PHP/"}]},{"title":"Python大数据量计数","slug":"Python大数据量计数","date":"2018-09-10T07:58:34.000Z","updated":"2025-12-13T12:15:38.079Z","comments":true,"path":"2018/09/10/Python大数据量计数/","permalink":"http://chiang97912.github.io/2018/09/10/Python%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%87%8F%E8%AE%A1%E6%95%B0/","excerpt":"和上一篇文章一样，这篇文章也是我在数学建模中碰到的，如果只是普通数据量的计数问题那么我们不妨使用counter，但是如果数据量达到一定规模，那么我们不得不考虑其他算法来解决问题了。我们这里使用hyperloglog算法来实现大数据量计数问题，这种算法是一种基于统计的计数算法，算法并不一定准确，但是足够快，如果读者将速度放在第一位那么不妨试试这种算法，而且hyperloglog算法准确率逼近100%，试问1000001和100000又有多大的差距呢，所以这种算法是有一定实用性的。","text":"和上一篇文章一样，这篇文章也是我在数学建模中碰到的，如果只是普通数据量的计数问题那么我们不妨使用counter，但是如果数据量达到一定规模，那么我们不得不考虑其他算法来解决问题了。我们这里使用hyperloglog算法来实现大数据量计数问题，这种算法是一种基于统计的计数算法，算法并不一定准确，但是足够快，如果读者将速度放在第一位那么不妨试试这种算法，而且hyperloglog算法准确率逼近100%，试问1000001和100000又有多大的差距呢，所以这种算法是有一定实用性的。 代码实现 当然作为胶水语言的Python，我们当然不必重复造轮子，这里我们可以直接使用python的bounter库来实现hyperloglog算法计数。 安装方法：pip install bounter 这里给出bounter在github上的官方教材使用的代码： 示例一： from bounter import bountercounts = bounter(size_mb=1024) # use at most 1 GB of RAMcounts.update([u&#x27;a&#x27;, &#x27;few&#x27;, u&#x27;words&#x27;, u&#x27;a&#x27;, u&#x27;few&#x27;, u&#x27;times&#x27;]) # count item frequenciesprint(counts[u&#x27;few&#x27;]) # query the counts2 示例二 from bounter import bountercounts = bounter(size_mb=200) # default version, unless you specify need_items or need_countscounts.update([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;a&#x27;, &#x27;b&#x27;])print(counts.total(), counts.cardinality()) # total and cardinality still work(5L, 3)print(counts[&#x27;a&#x27;]) # individual item frequency still works2print(list(counts)) # iterator returns keys, just like Counter[u&#x27;b&#x27;, u&#x27;a&#x27;, u&#x27;c&#x27;]print(list(counts.iteritems())) # supports iterating over key-count pairs, etc.[(u&#x27;b&#x27;, 2L), (u&#x27;a&#x27;, 2L), (u&#x27;c&#x27;, 1L)]","categories":[{"name":"算法","slug":"算法","permalink":"http://chiang97912.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"python","slug":"python","permalink":"http://chiang97912.github.io/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://chiang97912.github.io/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"Python实现最近点对求解","slug":"Python实现最近点对求解","date":"2018-09-10T07:21:27.000Z","updated":"2025-12-13T12:15:08.259Z","comments":true,"path":"2018/09/10/Python实现最近点对求解/","permalink":"http://chiang97912.github.io/2018/09/10/Python%E5%AE%9E%E7%8E%B0%E6%9C%80%E8%BF%91%E7%82%B9%E5%AF%B9%E6%B1%82%E8%A7%A3/","excerpt":"数学建模中遇到求最近点对的问题，按理说我们使用朴素法进行暴力求解答案也是可行的，但是由于数据过于庞大，我们最初使用朴素法在计算机上跑了6个小时都没有得到问题的解，最终只得作罢。后来我们利用分治法解决了最近点对问题，具体思路参看这里。","text":"数学建模中遇到求最近点对的问题，按理说我们使用朴素法进行暴力求解答案也是可行的，但是由于数据过于庞大，我们最初使用朴素法在计算机上跑了6个小时都没有得到问题的解，最终只得作罢。后来我们利用分治法解决了最近点对问题，具体思路参看这里。 分治法 分治法解决最近点对问题的算法过程读者可以参见上文给出的参考博客。分治法的思想就是先将问题分解成一个个小的问题，最后将小问题的答案合并得到问题的解。这其中关键步骤是分解与合并，具体实现方法是使用递归，核心思想是空间换时间。 代码实现 上面给出的教程中是使用C++实现的，我这里给出Python实现。 由于我这里具体的问题是给出经纬度找出最近点对，所以我采用的距离为改进的球面余弦距离。 # -*- coding: utf-8 -*-from collections import Counterfrom math import sqrt,acos, sin, cos, radiansdef nearest_dot(s): mid = int(len(s)/2) left = s[0:mid] right = s[mid:] mid_x = (left[-1][0]+right[0][0])/2.0 if len(left) &gt; 2: lmin = nearest_dot(left) #左侧部分最近点对 else: lmin = left if len(right) &gt; 2: rmin = nearest_dot(right) #右侧部分最近点对 else: rmin = right if len(lmin) &gt;1: dis_l = get_distance(lmin) else: dis_l = float(&quot;inf&quot;) if len(rmin) &gt;1: dis_r = get_distance(rmin) else: dis_r = float(&quot;inf&quot;) d = min(dis_l, dis_r) #最近点对距离 mid_min=[] for i in left: if mid_x-i[0]&lt;=d : #如果左侧部分与中间线的距离&lt;=d for j in right: if abs(i[0]-j[0])&lt;=d and abs(i[1]-j[1])&lt;=d: #如果右侧部分点在i点的(d,2d)之间 if get_distance((i,j))&lt;=d: mid_min.append([i,j]) #ij两点的间距若小于d则加入队列 if mid_min: dic=[] for i in mid_min: dic.append(&#123;get_distance(i):i&#125;) dic.sort(key=lambda x: x.keys()) return list(dic[0].values())[0] elif dis_l&gt;dis_r: return rmin else: return lmin # 求点对的距离def get_distance(m): dx = m[0][1] - m[1][1] # 经度差值 dy = m[0][0] - m[1][0] # 纬度差值 # b = (lat1 + lat2) / 2.0 # 平均纬度 # Lx = (a[3] * b*b*b + a[2]* b*b + a[1] * b + a[0]) * radians(dx) * 6367000.0 # 东西距离(单位：米) Lx = cos(dx) * radians(dx) * 6367000.0 # 东西距离(单位：米) Ly = 6367000.0 * radians(dy) # 南北距离 return sqrt(Lx * Lx + Ly * Ly)def divide_conquer(s): s.sort() nearest_dots = nearest_dot(s) return nearest_dots","categories":[{"name":"算法","slug":"算法","permalink":"http://chiang97912.github.io/categories/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"python","slug":"python","permalink":"http://chiang97912.github.io/tags/python/"},{"name":"算法","slug":"算法","permalink":"http://chiang97912.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}]},{"title":"Python实现argsort排序返回索引值","slug":"python实现argsort排序返回索引值","date":"2018-09-05T12:58:14.000Z","updated":"2025-12-13T12:14:52.359Z","comments":true,"path":"2018/09/05/python实现argsort排序返回索引值/","permalink":"http://chiang97912.github.io/2018/09/05/python%E5%AE%9E%E7%8E%B0argsort%E6%8E%92%E5%BA%8F%E8%BF%94%E5%9B%9E%E7%B4%A2%E5%BC%95%E5%80%BC/","excerpt":"插曲是这样的，之前一直在给别人写外包程序，在程序中我使用到了numpy库中的argsort排序方法，这个排序方法十分方便，对可迭代对象进行排序并返回原顺序的索引值。由于程序要传给客户，所以我将代码封装成了exe可执行程序。之前封装出来的程序最多也就40来兆，可是自从我将开发环境切换到anaconda后，封装出来的程序高达200多兆，简直吓死我了。今天客户再次需要这个程序，于是我决定抛弃numpy库，自己实现argsort算法。","text":"插曲是这样的，之前一直在给别人写外包程序，在程序中我使用到了numpy库中的argsort排序方法，这个排序方法十分方便，对可迭代对象进行排序并返回原顺序的索引值。由于程序要传给客户，所以我将代码封装成了exe可执行程序。之前封装出来的程序最多也就40来兆，可是自从我将开发环境切换到anaconda后，封装出来的程序高达200多兆，简直吓死我了。今天客户再次需要这个程序，于是我决定抛弃numpy库，自己实现argsort算法。 numpy实现 首先来看看numpy库中argsort的用法： argsort(a, axis=-1, kind=&#x27;quicksort&#x27;, order=None) 第一个参数是需要排序的可迭代对象，第二个参数是排序的维度，第三个参数是排序的算法，常见的有快排（quicksort）、堆排序（heapsort）以及归并排序（mergesort）。第四个参数是排序的次序。 实例： import numpy as npli = [1.5, 3, 15, 21, 7, 31, 5]indies = np.argsort(li, kind=&#x27;heapsort&#x27;)print(indies) 运行结果： [0 1 6 4 2 3 5] 原生python实现 其实原生python的实现主要是借用python内置函数sorted，但是sorted函数返回的结果是排序的最终结果而不是排序之前的索引值序列。但是我们可以利用sorted函数对字典排序的能力，将列表转换成字典就可以了。列表转字典的方法是先将列表通过enumerate函数转换成枚举型，然后在通过dict函数转换成字典。这样传入的列表就变为一个键为索引值为原值的一个字典了。这样我们再使用sorted函数对字典排序，再返回字典的键就可以了。 def argsort(X): d = dict(enumerate(X)) r = dict(sorted(d.items(), key=lambda x:x[1])) return list(r.keys())li = [1.5, 3, 15, 21, 7, 31, 5]indies = argsort(li)print(indies) 运行结果： [0, 1, 6, 4, 2, 3, 5]","categories":[{"name":"代码","slug":"代码","permalink":"http://chiang97912.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"python","slug":"python","permalink":"http://chiang97912.github.io/tags/python/"},{"name":"代码","slug":"代码","permalink":"http://chiang97912.github.io/tags/%E4%BB%A3%E7%A0%81/"}]},{"title":"模糊综合评价法原理及实现","slug":"模糊综合评价法原理及实现","date":"2018-09-02T09:45:12.000Z","updated":"2025-12-13T12:19:01.049Z","comments":true,"path":"2018/09/02/模糊综合评价法原理及实现/","permalink":"http://chiang97912.github.io/2018/09/02/%E6%A8%A1%E7%B3%8A%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7%E6%B3%95%E5%8E%9F%E7%90%86%E5%8F%8A%E5%AE%9E%E7%8E%B0/","excerpt":"模糊综合评价法原理 基本思想 模糊综合评价法是一种基于模糊数学的综合评价方法。该综合评价法根据模糊数学的隶属度理论把定性评价转化为定量评价，即用模糊数学对受到多种因素制约的事物或对象做出一个总体的评价。它具有结果清晰，系统性强的特点，能较好地解决模糊的、难以量化的问题，适合各种非确定性问题的解决。","text":"模糊综合评价法原理 基本思想 模糊综合评价法是一种基于模糊数学的综合评价方法。该综合评价法根据模糊数学的隶属度理论把定性评价转化为定量评价，即用模糊数学对受到多种因素制约的事物或对象做出一个总体的评价。它具有结果清晰，系统性强的特点，能较好地解决模糊的、难以量化的问题，适合各种非确定性问题的解决。 隶属度 隶属函数，也称为归属函数或模糊元函数，是模糊集合中会用到的函数，是一般集合中指示函数的一般化。指示函数可以说明一个集合中的元素是否属于特定子集合。一元素的指示函数的值可能是0或是1，而元素的隶属函数会是0到1之间的数值，表示元素属于某模糊集合的“真实程度”（degree of truth）即隶属度。 基本模型 设评判对象为P: 其因素集 $ U={ u_1, u_2, \\cdots , u_n } $ ,评判等级集 $ V={ v_1, v_2, \\cdots ,v_m } $ 。对U中每一因素根据评判集中的等级指标进行模糊评判，得到评判矩阵： R=[r11,r12,⋯ ,r1mr21,r22,⋯ ,r2mrn1,rn2,⋯ ,rnm]R=\\begin{bmatrix} r{11},r{12},\\cdots,r{1m} \\\\ r{21},r{22},\\cdots,r{2m} \\\\ r{n1},r{n2},\\cdots,r_{nm} \\end{bmatrix} R=⎣⎢⎡​r11,r12,⋯,r1mr21,r22,⋯,r2mrn1,rn2,⋯,rnm​​⎦⎥⎤​ 通常为了避免量纲的影响，我们还要进行去量纲操作。 其中，rijr_{ij}rij​表示uiu_iui​关于vjv_jvj​的隶属度。(U,V,R) 则构成了一个模糊综合评判模型。确定各因素重要性指标（也称权数）后，记为A={a1,a2,⋯ ,an}A=\\{ a_1,a_2, \\cdots,a_n \\}A={a1​,a2​,⋯,an​},满足∑i=1nai=1\\sum_{i=1}^na_i=1∑i=1n​ai​=1，合成得 B‾=A⋅R=(b1‾,b2‾,⋯ ,bm‾)\\overline B = A\\cdot R=\\left(\\overline {b_1}, \\overline {b_2}, \\cdots ,\\overline {b_m} \\right) B=A⋅R=(b1​​,b2​​,⋯,bm​​) 经归一化后，得 $ B= { b_i, b_2, \\cdots, b_m } $ ,于是可确定对象P的评判等级。 如果想得到综合得分，那么我们还可以对B进一步打分，即以同样的方式设置权数，对B进行综合评定。 对问题进行模糊综合分析之后我们通常还要考察该模型的置信度。 权数的确定： 权数可以通过数学方法来确定，也可以由具有权威性的专家及具有代表性的人按因素的重要程度来商定。不过现在通用的做法是凭经验给出权重。 合成算法： 合成算法即加权的方式，常见算法有： 主因素决定型 bi=max⁡{min⁡{a1,r1i},min⁡{a2,r2i},⋯ ,min⁡{an,rni}}b_i=\\max \\left \\{ \\min \\left \\{ a_1,r_{1i} \\right\\},\\min \\left \\{ a_2,r_{2i} \\right\\} ,\\cdots,\\min \\left \\{ a_n,r_{ni} \\right\\}\\right\\} bi​=max{min{a1​,r1i​},min{a2​,r2i​},⋯,min{an​,rni​}} 主因素突出型 bi=max⁡{a1×r1i,a2×r2i,⋯ ,an×rni}b_i=\\max \\left \\{ a_1\\times r_{1i}, a_2\\times r_{2i} ,\\cdots,a_n\\times r_{ni}\\right\\} bi​=max{a1​×r1i​,a2​×r2i​,⋯,an​×rni​} 加权平均型 bi=a1×r1i+a2×r2i+⋯+an×rnib_i=a_1\\times r_{1i} + a_2\\times r_{2i} +\\cdots+a_n\\times r_{ni} bi​=a1​×r1i​+a2​×r2i​+⋯+an​×rni​ 加权平均型算法常用在因素集很多的情形，它可以避免信息丢失；主因素突出型算法常用在所统计的模糊矩阵中的数据相差很悬殊的情形，它可以防止特殊数据的干扰。 实例：教学质量评价 我们首先按照票数结果统计了25名学生对某位老师的教学质量评价表，统计结果如下： 好（100） 较好（85） 一般（70） 较差（55） 1. 教学计划及教学内容安排（0.10） 9 14 2 0 2. 教材及参考资料状况（0.10） 3 14 7 1 3. 教师教学态度及责任心（0.15） 5 15 5 0 4. 教师讲解能力（0.10） 1 10 11 3 5. 课堂教学形式的多样化程度（0.10） 2 11 12 0 6. 理论联系实际程度及教学案例使用情况（0.10） 5 14 6 0 7. 辅助教学环节及考核情况（0.10） 4 6 13 2 8. 教学改革与创新情况（0.10） 3 8 12 2 9. 从本课程学习中所获得的收益程度（0.15） 5 12 6 2 由于我们最终要给每项指标打分，如果直接使用票数计算那么必然会导致结果收到参评人员数量的影响，即参评人数为1000和参评人数为25的结果截然不同，所以我们使用某项指标的某个等级票数的频率来代替票数来表示隶属度。各项因素的权数向量为：[0.1,0.1,0.15,0.10,0.10,0.10,0.10,0.10,0.15][0.1, 0.1, 0.15, 0.10, 0.10, 0.10, 0.10,0.10, 0.15][0.1,0.1,0.15,0.10,0.10,0.10,0.10,0.10,0.15]，评判等级的权数向量为:[100,85,70,55][100, 85,70,55][100,85,70,55]，其中两个权数向量之和为1。 好（100） 较好（85） 一般（70） 较差（55） 1. 教学计划及教学内容安排（0.10） 0.36 0.56 0.08 0.00 2. 教材及参考资料状况（0.10） 0.12 0.56 0.28 0.04 3. 教师教学态度及责任心（0.15） 0.20 0.60 0.2 0.00 4. 教师讲解能力（0.10） 0.04 0.4 0.44 0.12 5. 课堂教学形式的多样化程度（0.10） 0.08 0.44 0.48 0.00 6. 理论联系实际程度及教学案例使用情况（0.10） 0.20 0.56 0.24 0.00 7. 辅助教学环节及考核情况（0.10） 0.16 0.24 0.52 0.08 8. 教学改革与创新情况（0.10） 0.12 0.32 0.48 0.08 9. 从本课程学习中所获得的收益程度（0.15） 0.2 0.48 0.24 0.08 综合隶属度 0.168 0.47 0.318 0.044 综合隶属度由各项因素的权数向量和相应列的评判等级向量经过合成算法（这里使用加权平均）后的结果， 综合得分：0.168x100 + 0.47x85 + 0.318x70 + 0.044x55=81.43 代码实现 下面代码中的fuzzy_method为主函数，fuzzy_comprehensive_evaluation为模糊综合评价法的主要逻辑。 function fuzzy_method % 模糊综合评价模型主函数 A1=[0.1 0.2 0.3 0.4]; A2=[0.4 0.35 0.15 0.1]; R=[0.2 0.5 0.2 0.1; 0.7 0.2 0.1 0; 0 0.4 0.5 0.1; 0.2 0.3 0.5 0]; fuzzy_comprehensive_evaluation(1,A1,R) fuzzy_comprehensive_evaluation(1,A2,R)endfunction B=fuzzy_comprehensive_evaluation(model,A,R) %模糊综合评判 % 参数model用于合成算法的确定，参数A为权数向量，参数R为隶属度矩阵。 B=[]; [m,s1]=size(A); [s2,n]=size(R); if(s1~=s2) disp(&#x27;A的列不等于R的行&#x27;); else if(model==1) %主因素决定型 for(i=1:m) for(j=1:n) B(i,j)=0; for(k=1:s1) x=0; if(A(i,k)&lt;R(k,j)) x=A(i,k); else x=R(k,j); end if(B(i,j)&lt;x) B(i,j)=x; end end end end elseif(model==2) %主因素突出型 for(i=1:m) for(j=1:n) B(i,j)=0; for(k=1:s1) x=A(i,k)*R(k,j); if(B(i,j)&lt;x) B(i,j)=x; end end end end elseif(model==3) %加权平均型 for(i=1:m) for(j=1:n) B(i,j)=0; for(k=1:s1) B(i,j)=B(i,j)+A(i,k)*R(k,j); end end end elseif(model==4) %取小上界和型 for(i=1:m) for(j=1:n) B(i,j)=0; for(k=1:s1) x=0; x=min(A(i,k),R(k,j)); B(i,j)=B(i,j)+x; end B(i,j)=min(B(i,j),1); end end elseif(model==5) %均衡平均型 C=[]; C=sum(R); for(j=1:n) for(i=1:s2) R(i,j)=R(i,j)/C(j); end end for(i=1:m) for(j=1:n) B(i,j)=0; for(k=1:s1) x=0; x=min(A(i,k),R(k,j)); B(i,j)=B(i,j)+x; end end end else disp(&#x27;模型赋值不当&#x27;); end endend","categories":[{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}],"tags":[{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"模糊综合评价法","slug":"模糊综合评价法","permalink":"http://chiang97912.github.io/tags/%E6%A8%A1%E7%B3%8A%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7%E6%B3%95/"}]},{"title":"数学建模笔记","slug":"数学建模笔记","date":"2018-08-29T02:08:05.000Z","updated":"2025-12-13T12:17:31.728Z","comments":true,"path":"2018/08/29/数学建模笔记/","permalink":"http://chiang97912.github.io/2018/08/29/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AC%94%E8%AE%B0/","excerpt":"优化模型 优化模型就是给定一个目标函数，然后在约束条件下求出该目标函数的最优值。","text":"优化模型 优化模型就是给定一个目标函数，然后在约束条件下求出该目标函数的最优值。 形如： min⁡(或max⁡) z=f(x),x=(x1,...)s.t.gi(x)≤0,i=1,2,...\\min(或\\max)\\ z=f(x), x=(x_1,...)\\\\ s.t. g_i(x)\\leq0,i=1,2,... min(或max) z=f(x),x=(x1​,...)s.t.gi​(x)≤0,i=1,2,... 决策变量： x 目标函数： f(x) 约束条件： $ s.t. g_i(x)\\leq0 $ 优化问题三要素： 决策变量 目标函数 约束条件 数学规划： 线性规划（Linear Programming，简称LP） 非线性规划(Nonlinear Programming，简称NLP) 整数规划 LINGO软件求解优化模型 例题： max⁡ S{aixi=S,i=1,2,...∑i=16xi=5000,a5x6=5000.\\begin{array}{lcl} \\max \\ S\\\\ \\begin{cases} a_i x_i=S,i=1,2,... \\\\ \\sum^6_{i=1}x_i=5000, \\\\ a_5x_6=5000. \\end{cases} \\end{array} max S⎩⎪⎪⎨⎪⎪⎧​ai​xi​=S,i=1,2,...∑i=16​xi​=5000,a5​x6​=5000.​​ 存期年限 1年 2年 3年 4年 5年 最有收益 1.018 1.0432 1.0776 1.09715968 1.144 LINGO代码： !求解规划问题;model:min = S;1.018 * x1 = S;1.0432 * x2 = S;1.07776 * x3 = S;1.09715968 * x4 = S;1.144 * x5 = S;x1 + x2 + x3 + x4 + x5 + x6 = 5000;1.144 * x6 = 5000;end 运行结果： Global optimal solution found.Objective value: 135.2227Infeasibilities: 0.000000Total solver iterations: 0 Variable Value Reduced Cost S 135.2227 0.000000 X1 132.8317 0.000000 X2 129.6230 0.000000 X3 125.4664 0.000000 X4 123.2479 0.000000 X5 118.2016 0.000000 X6 4370.629 0.000000 Row Slack or Surplus Dual Price 1 135.2227 -1.000000 2 0.000000 0.2110548 3 0.000000 0.2059565 4 0.000000 0.1993522 5 0.000000 0.1958273 6 0.000000 0.1878093 7 0.000000 -0.2148538 8 0.000000 0.1878093 其中Objective value项即为所求目标值，下面的Variable即为在最优状态下各变量值。 关于LINGO教程网上有很多，在这里不再赘述。读者可以前往 这里查看。 MATLAB软件求解优化模型 常用的优化功能函数： 求解线性规划问题的主要函数是linprog。 求解二次规划问题的主要函数是quadprog。 求解无约束非线性规划问题的主要函数是fminbnd、fminunc和fminsearch。 求解约束非线性规划问题的主要函数是fgoalattain和fminimax。 一般步骤： 针对具体工程问题建立优化设计的数学模型 建立目标函数文件 建立约束函数文件 建立调用优化工具函数的命令文件 将优化设计的命令文件复制到MATLAB命令窗口中进行运算求解。 线性规划问题 数学模型形式： min⁡fTXs.t.AX≤b (线性不等式约束条件)AeqX=beq （线性等式约束条件lb≤X≤ub (边界约束条件)\\begin{array}{lcl} \\min f^TX \\\\ s.t. AX\\leq b~~~~(线性不等式约束条件) \\\\ AeqX=beq~~~~（线性等式约束条件 \\\\ lb \\leq X \\leq ub~~~~(边界约束条件) \\end{array} minfTXs.t.AX≤b (线性不等式约束条件)AeqX=beq （线性等式约束条件lb≤X≤ub (边界约束条件)​ MATLAB中函数调用格式: [xopt, fopt]=linprog(f, A, b, Aeq, beq, lb, ub, x0, options) 参数及返回值释义： xopt: 最优解 fopt: 最优值 f: 目标函数各维变量系数向量 x0：初始点 options: 可选项 注： A, b, Aeq, beq, lb, ub均和上述数学模型对应。 二次规划问题 数学模型形式： min⁡f(X)=12XTHX+CTXs.t.AX≤bAeqX=beqlb≤X≤ub\\begin{array}{lcl} \\min f(X)=\\frac{1}{2}X^THX+C^TX \\\\ s.t. AX\\leq b AeqX=beq \\\\ lb \\leq X \\leq ub \\end{array} minf(X)=21​XTHX+CTXs.t.AX≤bAeqX=beqlb≤X≤ub​ [xopt, fopt]=quadprog(H,C, A, b, Aeq, beq, lb, ub, x0, options) 参数及返回值释义： xopt: 最优解 fopt: 最优值 H: 目标函数的海塞矩阵 C： 目标函数的一次项系数向量 其余同上。 示例： 求解约束优化问题 f(X)=2X12+2x22+x32−2x1x2+x3s.t.g(X)=x1+3x2+2x3≤6h(X)=2x1−x2+x3=4x1,x2,x3≥0\\begin{array}{lcl} f(X)=2X^2_1+2x^2_2+x^2_3-2x_1x_2+x_3\\\\ s.t. g(X)=x_1+3x_2+2x_3\\leq 6\\\\ h(X)=2x_1-x_2+x_3=4\\\\ x_1,x_2,x_3\\geq 0 \\end{array} f(X)=2X12​+2x22​+x32​−2x1​x2​+x3​s.t.g(X)=x1​+3x2​+2x3​≤6h(X)=2x1​−x2​+x3​=4x1​,x2​,x3​≥0​ 解：(1)将目标函数写成二次函数的形式f(X)=12XTHX+CTXf(X)=\\frac{1}{2}X^THX+C^TXf(X)=21​XTHX+CTX,其中： X=[x1x2x3]H=[4−20−240002]C=[001]\\begin{array}{lcl} X=\\begin{bmatrix} x_1\\\\ x_2\\\\ x_3 \\end{bmatrix} H=\\begin{bmatrix} 4 &amp; -2 &amp; 0\\\\ -2 &amp; 4 &amp; 0\\\\ 0 &amp; 0 &amp; 2 \\end{bmatrix} C=\\begin{bmatrix} 0\\\\ 0\\\\ 1 \\end{bmatrix} \\end{array}X=⎣⎢⎡​x1​x2​x3​​⎦⎥⎤​H=⎣⎢⎡​4−20​−240​002​⎦⎥⎤​C=⎣⎢⎡​001​⎦⎥⎤​​ (2)编写求解二次规划的M文件： H=[4,-2,0;-2,4,0;0,0,2];C=[0,0,1];A=[1,3,2];b=[6];Aeq=[2,-1,1];beq=[4];lb=zeros(3,1);[xopt,fopt]=quadprig(H,C,A,b,Aeq,beq,lb) 其他规划问题 其他规划问题的MATLAB函数使用方法和前面两个类似，这里不再赘述，具体使用方法读者可以查阅相关资料。 微分方程及数值解 数学建模中的微分方程模型，其实就是高等数学中的微积分知识在建模中的应用。常见的微分方程模型有人口模型和种群数量模型。 实例 温度冷却 由物理学知道,物体冷却的速度与当时的物体温度和周围环境温度之差成正比.今 100℃的沸水注入杯里,放在室温为 20℃的环境冷却,5min 后测得水温为 60℃.求水温 u 与时间 t 的函数关系. 问题分析及模型的建立 设比例系数为k(k&gt;0),根据题意可得微分方程 dudt=−k(u−20)u∣t=0=100,u∣t=5=60\\begin{array}{lcl} \\frac{du}{dt}=-k(u-20)\\\\ u|_{t=0}=100,u|_{t=5}=60 \\end{array} dtdu​=−k(u−20)u∣t=0​=100,u∣t=5​=60​ 模型的求解 此为简单的一阶可分离变量微分方程,可得解析解u=20+80(0.5)t5u=20+80(0.5)^{\\frac{t}{5}}u=20+80(0.5)5t​. 使用MATLAB求解微分方程解析解(通解或特解)： dsolve(&#x27;Du+k*(u-20)=0&#x27;,&#x27;u(0)=100&#x27;,&#x27;t&#x27;)%dsolve 为求常微分方程的符号解函数 运算结果为： u =20+80*exp(-k*t) 使用MATLAB求解微分方程数值解： 由给定条件u∣t=5=60u|_{t=5}=60u∣t=5​=60,可得k=ln⁡25k=\\frac{\\ln 2}{5}k=5ln2​,即u=20+80(0.5)t5u=20+80(0.5)^\\frac{t}{5}u=20+80(0.5)5t​. MATLAB代码： f=inline(&#x27;-0.2*log(2)*(u-20)&#x27;,&#x27;t&#x27;,&#x27;u&#x27;);[t,u]=ode45(f,[0,100],100);%ode45 为龙格库塔法求微分方程的数值解plot(t,u)%绘制 0 到 100 分钟的温度随时间变化的图形 MATLAB求解的函数微分方程(组) 那些不可以用积分方法求解的微分方程初值问题，可以用 MATLAB 的函数，如二三阶龙格-库塔法ode23 或四五阶龙格-库塔法 ode45 命令来求其数值解． 对于微分方程(组)的初值问题 x(t)=f(t,x),x=(x1,...,xn)T,f=(f1,...,fn)Tx(t0)=x0,x0(x01,...,x0n)T\\begin{array}{lcl} x(t)=f(t,x),x=(x_1,...,x_n)^T,f=(f_1,...,f_n)^T\\\\ x(t_0)=x_0, x_0(x_{01},...,x_{0n})^T \\end{array} x(t)=f(t,x),x=(x1​,...,xn​)T,f=(f1​,...,fn​)Tx(t0​)=x0​,x0​(x01​,...,x0n​)T​ 可用下面的 MATLAB 命令实现计算： [t,x]= =ode23(odefun ，ts ，x0 ，options)[t,x]= =ode45(odefun ，ts ，x0 ，options) 这里 ode23 用的是 3 级 2 阶的龙格-库塔法公式，ode45 用的是 5 级 4 阶的龙格-库塔公式．输入参数 odefun 是待解方程写成的函数 M 文件或inline 格式的函数f(t,x)f(t,x)f(t,x)。 ts=[t0，t1，…，tf]，则输出为在指定时刻 t0，t1，…，tf 的函数值．如果输入 t0∶k∶tf，则输出为在[t0，tf]内以 k 为间隔的等分点处的函数值。 x0 为函数初值( n 维向量)。 options 可用于设定误差限（options 默认时设定相对误差10−3{10}^{-3}10−3绝对误差10−6{10}^{-6}10−6)，命令为： options=odeset(‘reltol’，rt，‘abstol’，at) 这里 rt，at 分别为设定的相对误差和绝对误差。 命令的输出 t 为由输入指定的 ts，x 为相应的函数值( n维向量)。 其他MATLAB函数 MATLAB中还有诸如求解偏微分方程的函数，读者可以自行查询相关资料。 机器学习 由于机器学习内容很多这里我只将常用学习算法进行归类。机器学习分为监督学习和无监督学习。其中监督学习又分为分类、集成学习、降维。 分类： K最近邻 决策树 贝叶斯分类器 逻辑回归 支持向量机 集成学习： bagging：随机森林 boosting：AdaBoost、GBDT、XGBoost（工业级） 无监督学习： K-Means DBSCAN（基于密度的聚类） 概率统计模型 聚类 {K−Means聚类（快速聚类）分层聚类（系统聚类）hierarchical cluster判别分析 {根据距离判别fisher判别法逐步判别法（选择变量）时间序列分析 {指数平滑Box−Jenkins方法\\begin{array}{lcl} 聚类~~\\begin{cases} K-Means聚类（快速聚类）\\\\ 分层聚类（系统聚类）hierarchical\\ cluster \\end{cases}\\\\ 判别分析~~\\begin{cases} 根据距离判别\\\\ fisher判别法\\\\ 逐步判别法（选择变量） \\end{cases}\\\\ 时间序列分析~~\\begin{cases} 指数平滑\\\\ Box-Jenkins方法 \\end{cases} \\end{array} 聚类 {K−Means聚类（快速聚类）分层聚类（系统聚类）hierarchical cluster​判别分析 ⎩⎪⎪⎨⎪⎪⎧​根据距离判别fisher判别法逐步判别法（选择变量）​时间序列分析 {指数平滑Box−Jenkins方法​​ 注：聚类分析对类及类的数量是未知的，而判别分析是已知的。 评价模型 常用评价模型： 层次分析法：定性与定量相结合的多准则决策 灰色综合评价法：灰色关联度分析 模糊综合评价法： 根据模糊数学的隶属度评价 BP神经网络综合评价法 数据包络（DEA）： 是一个对多投入/多产出的多决策单元的效率评价方法，广泛使用于业绩评价 综合评价模型： 线性加权综合法 y=∑j=1mwixjy=\\sum^m_{j=1}w_ix_jy=∑j=1m​wi​xj​ 非线性加权综合法 y=∏j=1mxjwjy=\\prod^m_{j=1}x_j^{w_j}y=∏j=1m​xjwj​​ 逼近理想点（TOPSIS）方法 动态加权 动态加权函数： 分段变幂函数 偏大型正态分布函数 S型分布函数 智能计算方法 模拟退火算法（SA）：寻找全局最优思想 自动转换: 满足条件自动转换到另外一种状态 概率转换： 当不满足条件时按照一定的概率转换到另外一种状态 遗传算法（GA）： 1. 选择 2. 交叉 3. 变异 粒子群算法（PSO）：信息共享思想","categories":[{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}],"tags":[{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}]},{"title":"灰色关联分析模型及其改进","slug":"灰色关联分析模型及其改进","date":"2018-08-28T13:48:03.000Z","updated":"2025-12-13T12:18:52.036Z","comments":true,"path":"2018/08/28/灰色关联分析模型及其改进/","permalink":"http://chiang97912.github.io/2018/08/28/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90%E6%A8%A1%E5%9E%8B%E5%8F%8A%E5%85%B6%E6%94%B9%E8%BF%9B/","excerpt":"灰色关联分析基本原理 对于两个系统之间的因素，其随时间或不同对象而变化的关联性大小的度量，称为关联度。而根据因素之间发展趋势的相似或相异程度作为衡量因素间关联程度的方法称为灰色关联分析方法。对于本题衡量客户接受的程度与哪些因素关联度更高，我们可以使用灰色关联分析来衡量购买价格、维护成本、车门数、人员携带能力、行李箱大小、汽车安全性能等调查项和汽车可接受性之间的关联程度。","text":"灰色关联分析基本原理 对于两个系统之间的因素，其随时间或不同对象而变化的关联性大小的度量，称为关联度。而根据因素之间发展趋势的相似或相异程度作为衡量因素间关联程度的方法称为灰色关联分析方法。对于本题衡量客户接受的程度与哪些因素关联度更高，我们可以使用灰色关联分析来衡量购买价格、维护成本、车门数、人员携带能力、行李箱大小、汽车安全性能等调查项和汽车可接受性之间的关联程度。 首先，我们定义数据矩阵的最后一列l为标准要素，其余各列为需要比较的要素，分别用x‘l和x’i表示。 第一步：数据标准化 Xi′=Xi−min⁡Ximax⁡Xi−min⁡Xi=(xi′(1),xi′(2),xi′(3),xi′(4)...)X_i^\\prime=\\frac{X_i-\\min X_i}{\\max X_i-\\min X_i}=(x_i^\\prime(1),x_i^\\prime(2),x_i^\\prime(3),x_i^\\prime(4)...) Xi′​=maxXi​−minXi​Xi​−minXi​​=(xi′​(1),xi′​(2),xi′​(3),xi′​(4)...) 第二步：求差序列 Δi(k)=∣xl′−xi′∣;i=1,2,3,4...\\Delta_i(k)=|x_l^\\prime-x_i^\\prime|;i=1,2,3,4... Δi​(k)=∣xl′​−xi′​∣;i=1,2,3,4... 第三步：求两极差 M=max⁡imax⁡kΔi(k)m=min⁡imin⁡kΔi(k)M=\\max_i \\max_k \\Delta_i(k) \\\\ m=\\min_i \\min_k \\Delta_i(k) M=imax​kmax​Δi​(k)m=imin​kmin​Δi​(k) 第四步：计算关联系数 γli=m+ρMΔi(k)+ρM,i=2,3,4...\\gamma_{li}=\\frac{m+\\rho M}{\\Delta_i(k)+\\rho M},i=2,3,4... γli​=Δi​(k)+ρMm+ρM​,i=2,3,4... 第五步：求灰色关联度 γln=1N∑k=1Nγln(k),i=1,2,3,4...\\gamma_{ln}=\\frac{1}{N}\\sum^N_{k=1}\\gamma_{ln}(k),i=1,2,3,4... γln​=N1​k=1∑N​γln​(k),i=1,2,3,4... 灰色关联分析改进模型 灰色关联分析的核心是计算关联度，但是原始的关联度计算方法对各样本平等看待，即采用平权处理。但是由于各个特征对结果的影响程度是不同的，如果采用平权处理，必然会丧失数据中隐藏的潜在特征，所以本文中我们对各特征进行加权处理。此处我们采用基于距离分析法对灰色关联分析算法进行改进。 改进后的公式为: γln=∑k=1Nα(k)γln(k),i=1,2,3,4...\\gamma_{ln}=\\sum^N_{k=1}\\alpha(k)\\gamma_{ln}(k),i=1,2,3,4... γln​=k=1∑N​α(k)γln​(k),i=1,2,3,4... 其中权重α的计算方法如下： 我们以最优要素和最劣要素为参考要素。计算各个要素与参考要素的距离，离最优要素点近并且离最劣样本远的样本为总体较好的要素。 设数据为m x n矩阵 [a11⋯a1n⋮⋱⋮am1⋯amn]\\begin{bmatrix} a_{11} &amp; \\cdots &amp; a_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{m1} &amp; \\cdots &amp; a_{mn} \\end{bmatrix} ⎣⎢⎢⎡​a11​⋮am1​​⋯⋱⋯​a1n​⋮amn​​⎦⎥⎥⎤​ 第一步：确定最优要素和最劣要素 A+=(a1+,a2+,...,an+)TA−=(a1−,a2−,...,an−)TA^+=(a^+_1,a^+_2,...,a^+_n)^T \\\\ A^-=(a^-_1,a^-_2,...,a^-_n)^T A+=(a1+​,a2+​,...,an+​)TA−=(a1−​,a2−​,...,an−​)T 其中， al+=max⁡(a1l,a2l,...,aml)al−=max⁡(a1l,a2l,...,aml),l=1,2,3,...n\\begin{array}{lcl} a^+_l=\\max(a_{1l},a_{2l},...,a_{ml})\\\\ a^-_l=\\max(a_{1l},a_{2l},...,a_{ml}), l=1,2,3,...n \\end{array} al+​=max(a1l​,a2l​,...,aml​)al−​=max(a1l​,a2l​,...,aml​),l=1,2,3,...n​ 第二步：计算各要素点到参考要素点的距离。这里采用欧式距离， Dk+=∑l=1n(akl−al+)2D^+_k=\\sqrt{\\sum^n_{l=1}{(a_{kl}-a^+_l)}^2} Dk+​=l=1∑n​(akl​−al+​)2​ 第三步：综合正负向距离 Sk=Dk−Dk+,k=1,2,...,m.S_k=\\frac{D^-_k}{D^+_k},k=1,2,...,m. Sk​=Dk+​Dk−​​,k=1,2,...,m. 通过上式对各要素的距离进行综合评价，即当要素与最优要素点之间的距离越小，与最劣要素点之间的距离越大，那么该要素得到的分数越高。 但是为了防止Sk的分子和分母出现零，我们将Sk的分子和分母同时加上一个偏置β（这里的β一般取1）。得到， Sk=Dk−+βDk++β,k=1,2,...,m.S_k=\\frac{D^-_k+\\beta}{D^+_k+\\beta},k=1,2,...,m. Sk​=Dk+​+βDk−​+β​,k=1,2,...,m. 第四步：数据归一化 αk=Sk∑k=1mSk\\alpha_k=\\frac{S_k}{\\sum^m_{k=1}S_k} αk​=∑k=1m​Sk​Sk​​ 则 α=(α1,α2,...,αm)T\\alpha=(\\alpha_1,\\alpha_2,...,\\alpha_m)^T α=(α1​,α2​,...,αm​)T 即为所有权重向量。","categories":[{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"}],"tags":[{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"灰色关联分析","slug":"灰色关联分析","permalink":"http://chiang97912.github.io/tags/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"}]},{"title":"Tensorflow中矩阵乘法matmul和multiply详解","slug":"tensorflow中矩阵乘法matmul和multiply详解","date":"2018-04-09T12:01:13.000Z","updated":"2025-12-13T14:03:22.130Z","comments":true,"path":"2018/04/09/tensorflow中矩阵乘法matmul和multiply详解/","permalink":"http://chiang97912.github.io/2018/04/09/tensorflow%E4%B8%AD%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95matmul%E5%92%8Cmultiply%E8%AF%A6%E8%A7%A3/","excerpt":"在机器学习或者神经网络编程过程中，我们的运算对象通常是矩阵，而常用的矩阵操作就是点乘(dot product)和元素相乘(elementwise multiplication)。学过线性代数的读者肯定对点乘不会陌生，但是元素相乘就不一定知道了。其实elementwise multiplication就是将两个shape一样的矩阵按照对应元素相乘。","text":"在机器学习或者神经网络编程过程中，我们的运算对象通常是矩阵，而常用的矩阵操作就是点乘(dot product)和元素相乘(elementwise multiplication)。学过线性代数的读者肯定对点乘不会陌生，但是元素相乘就不一定知道了。其实elementwise multiplication就是将两个shape一样的矩阵按照对应元素相乘。 点乘matmul 在tensorflow中的点乘使用matmul方法，其中matmul分两种情况。 首先我们通过一个例子来了解matmul方法的使用 ： 代码示例: import numpy as npimport tensorflow as tf&quot;&quot;&quot; 2-D &quot;&quot;&quot;print(&quot;2-D:&quot;)sess = tf.Session()a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3]) # 2-D tensor `a`print(&quot;a = &quot;, sess.run(a))b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2]) # 2-D tensor `b`print(&quot;b = &quot;, sess.run(a))c = tf.matmul(a, b)print(&quot;c = &quot;, sess.run(c))&quot;&quot;&quot; 3-D &quot;&quot;&quot;print(&quot;3-D:&quot;)a = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3]) # 3-D tensor `a`print(&quot;a = &quot;, sess.run(a))b = tf.constant(np.arange(13, 25, dtype=np.int32), shape=[2, 3, 2]) # 3-D tensor `b`print(&quot;b = &quot;, sess.run(a))c = tf.matmul(a, b)print(&quot;c = &quot;, sess.run(c))sess.close() 运行结果： 2-D:a = [[1 2 3] [4 5 6]]b = [[1 2 3] [4 5 6]]c = [[ 58 64] [139 154]]3-D:a = [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]]]b = [[[ 1 2 3] [ 4 5 6]] [[ 7 8 9] [10 11 12]]]c = [[[ 94 100] [229 244]] [[508 532] [697 730]]] 如果做点乘的两个矩阵的shape维度为2维，那么就按照一般矩阵点乘来计算。 如果点乘的两个矩阵的shape维度为3维，那么我们通常将第一维定义为batch_size，那么matmul方法就会逐个数据的将对应的维度按照第一种情况对两个矩阵做点乘。 元素相乘multiply 两个矩阵中对应元素各自相乘 例如有矩阵 M1 = [ a b c e f g h i j ] 和 M2 = [ k l m n o p q r s ] 那么multiply(M1,M2)的结果为： [ a*k b*l c*m e*n f*o g*p h*q i*r j*s ] 代码示例： import tensorflow as tfa = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])b = tf.constant([[10., 11., 12.], [13., 14., 15.], [16., 17., 18.]])c = tf.multiply(a, b)sess = tf.Session()print(sess.run(c))sess.close() 运行结果： [[ 10. 22. 36.] [ 52. 70. 90.] [112. 136. 162.]]","categories":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://chiang97912.github.io/categories/tensorflow/"}],"tags":[{"name":"tensorflow","slug":"tensorflow","permalink":"http://chiang97912.github.io/tags/tensorflow/"}]},{"title":"Python爬虫入门实践","slug":"Python爬虫入门实践","date":"2018-01-23T15:55:18.000Z","updated":"2025-12-13T12:15:23.488Z","comments":true,"path":"2018/01/23/Python爬虫入门实践/","permalink":"http://chiang97912.github.io/2018/01/23/Python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/","excerpt":"写这篇博客，一来是整理我最近一两个月的数据抓取经验，二来是帮助新手快速入门爬虫。听到爬虫这个词很多人可能会联想到谷歌，百度，必应等搜索引擎，它们拥有强大的数据检索能力，为我们查找资料提供了极大的帮助。这些搜索引擎之所以强大就是因为它们有一个强大的数据抓取系统。下面我们从0到1逐层展开讲解。","text":"写这篇博客，一来是整理我最近一两个月的数据抓取经验，二来是帮助新手快速入门爬虫。听到爬虫这个词很多人可能会联想到谷歌，百度，必应等搜索引擎，它们拥有强大的数据检索能力，为我们查找资料提供了极大的帮助。这些搜索引擎之所以强大就是因为它们有一个强大的数据抓取系统。下面我们从0到1逐层展开讲解。 基础 其实爬虫很简单，归结起来就是下面两个步骤： 获取网页源代码 解析源代码获取需要的信息 获取网页源代码 在Python3中，获取网页源代码通常有两种方式：第一种是通过Python3自带的urllib库中的request.urlopen函数来请求网页源代码；第二种方式也是最常使用的方式就是使用requests库来实现网页的抓取。 urllib 首先我先来简单的介绍一下urllib库请求网页的方法： from urllib.request import urlopen response = urlopen('https://pixabay.com/en/') #抓取全球著名免费矢量图网站pixabay html = response.read() requests 下面开始介绍本教程获取网页源代码主要使用的工具：requests requests的使用很简单和urllib类似： import requests response = requests.get('https://pixabay.com/en/') #注意一定要加上url前面的安全协议https或者http不然系统会报错。 response.encoding = 'utf-8' #设置网页的源代码编码格式为utf-8，你可以根据具体情况设置诸如gbk（国标）等编码格式。 html = response.text #获取源代码 解析HTML 关于HTML我简要的说明一下：对于整个网页来说，它是使用html标记语言编写的，所以网页中的每一个元素都是一个节点，其中html标签用于包裹整个网页，head标签用于一些不需要可视化的代码编写其中包括网页的标题，引用的css文件和js文件等。而网站的主体被body标签包裹。body标签中有图层标签div（主要的标签）、img标签（图片标签）等。而每个标签都有一些属性比如class(类)、id(编号)等。其中id在整个网页中是唯一指定的，而class属性并不唯一。我们可以通过标签的层次关系以及这些标签的属性来定位我们需要抓取的数据。学习网络爬虫需要掌握一定的网页知识，如果读者欠缺这方面的知识可以前往菜鸟教程学习有关html以及css有关的知识。 解析HTML或者XML的工具或框架有很多，最常用的有: BeautifulSoup：使用正则表达式编写的html解析库 lxml：通过xml的节点查找信息的网页解析库（速度很快） scrapy：集成爬虫框架 pyquery：Python中对jquery的实现，对于熟悉前端的人来说非常容易上手。 BeautifulSoup BeautifulSoup有多种使用方法，对于查找元素可以使用find_all方法，但是笔者习惯使用select方法（css选择器语法）搜索文档。关于css选择器如果读者对前端足够熟悉的话应该能够轻松上手。经常使用到的标签包括div、span、ul、li、img、a等。我们使用点（.）表示标签的class属性，使用井（#）表示标签的id属性。在使用select方法的时候可以通过他们定位标签数据的位置。关于BeautifulSoup的具体使用方法，读者可以前往官方文档学习。 这里我们以抓取pixabay网站首页图片的url为例： 对于爬虫我建议大家使用chrome浏览器的控制台对页面进行分析（F12调出控制台）。 打开控制台后，选中Elements选项卡我们可以查看网页源代码，通过对网页的分析我们会发现图片的url为img标签的src属性，而img标签的父标签为a,a标签的父标签为div标签，div的父标签为带有class属性为flex_grid和credits的div标签，而该标签的父标签是id属性为gallery的标签。由于id属性是唯一的我们可以以该标签为源点，查找目标标签。 import requests from bs4 import BeautifulSoup response = requests.get('https://pixabay.com/en/') #请求pixabay网站 response.encoding = 'utf-8' #设置utf-8编码 html = response.text #网页源代码 soup = BeautifulSoup(html, 'lxml') #使用BeautifulSoup解析网页 imgs = soup.select('#gallery &gt; div.flex_grid.credits &gt; div &gt; a &gt; img') #使用css选择器定位链接，返回结果为包含所有图片标签的列表 for img in imgs: print(img['src']) #打印图片标签的src属性 运行结果： https://cdn.pixabay.com/photo/2018/06/23/16/22/romanesco-3493007__340.jpg https://cdn.pixabay.com/photo/2018/07/08/14/16/cat-3523992__340.jpg https://cdn.pixabay.com/photo/2018/05/30/15/31/rustic-3441673__340.jpg https://cdn.pixabay.com/photo/2016/06/29/09/28/golf-1486354__340.jpg https://cdn.pixabay.com/photo/2017/11/10/08/10/son-2935723__340.jpg https://cdn.pixabay.com/photo/2018/07/06/13/30/statue-3520416__340.jpg https://cdn.pixabay.com/photo/2018/07/08/15/32/dahlia-3524115__340.jpg https://cdn.pixabay.com/photo/2018/05/07/22/08/sydney-opera-house-3381786__340.jpg https://cdn.pixabay.com/photo/2018/07/05/23/31/ivy-3519431__340.jpg https://cdn.pixabay.com/photo/2017/06/05/14/55/glass-2374311__340.jpg https://cdn.pixabay.com/photo/2018/04/12/11/44/apple-3313225__340.jpg https://cdn.pixabay.com/photo/2017/09/22/09/48/desert-2774945__340.jpg https://cdn.pixabay.com/photo/2017/07/12/22/51/couple-2498660__340.jpg https://cdn.pixabay.com/photo/2016/06/20/03/15/pier-1467984__340.jpg https://cdn.pixabay.com/photo/2018/01/29/10/40/shower-of-sparks-3115784__340.jpg https://cdn.pixabay.com/photo/2017/09/06/20/35/massage-2722936__340.jpg https://cdn.pixabay.com/photo/2018/07/01/20/01/mercedes-3510327__340.jpg https://cdn.pixabay.com/photo/2018/06/28/15/23/soft-fruits-3504149__340.jpg https://cdn.pixabay.com/photo/2018/06/28/17/02/water-lily-3504363__340.jpg https://cdn.pixabay.com/photo/2018/06/29/01/47/piano-3505109__340.jpg lxml 总的来说使用BeautifulSoup已经能够解决大部分爬虫问题了，但是由于BeautifulSoup的另辟蹊径，独创了一套正则表达式的查询方法导致了爬虫的爬取速度非常的慢，而且BeautifulSoup对于列表项的爬取是非常的糟糕的。但是lxml就不存在着两个问题，lxml由于底层采用c语言代码编写，所以速度上是非常快的。而且由于lxml采用xpath语法解析HTML，对列表项的爬取是非常轻松的。 关于lxml框架和xpath语法，读者可以参看崔庆才的博客。 下面使用lxml爬取pixabay网站图片url: import requests from lxml import etree # 获取网页的源代码 response = requests.get('https://pixabay.com/en/') response.encoding = 'utf-8' html = response.text # 解析网页 parser = etree.HTML(html) for i in range(1,21): #打印前20张图片url link = parser.xpath('//*[@id=&quot;gallery&quot;]/div[1]/div['+ str(i) +']/a/img/@src')[0] print(link) 运行结果： https://cdn.pixabay.com/photo/2018/06/23/16/22/romanesco-3493007__340.jpg https://cdn.pixabay.com/photo/2018/07/08/14/16/cat-3523992__340.jpg https://cdn.pixabay.com/photo/2018/05/30/15/31/rustic-3441673__340.jpg https://cdn.pixabay.com/photo/2016/06/29/09/28/golf-1486354__340.jpg https://cdn.pixabay.com/photo/2017/11/10/08/10/son-2935723__340.jpg https://cdn.pixabay.com/photo/2018/07/06/13/30/statue-3520416__340.jpg https://cdn.pixabay.com/photo/2018/07/08/15/32/dahlia-3524115__340.jpg https://cdn.pixabay.com/photo/2018/05/07/22/08/sydney-opera-house-3381786__340.jpg https://cdn.pixabay.com/photo/2018/07/05/23/31/ivy-3519431__340.jpg https://cdn.pixabay.com/photo/2017/06/05/14/55/glass-2374311__340.jpg https://cdn.pixabay.com/photo/2018/04/12/11/44/apple-3313225__340.jpg https://cdn.pixabay.com/photo/2017/09/22/09/48/desert-2774945__340.jpg https://cdn.pixabay.com/photo/2017/07/12/22/51/couple-2498660__340.jpg https://cdn.pixabay.com/photo/2016/06/20/03/15/pier-1467984__340.jpg https://cdn.pixabay.com/photo/2018/01/29/10/40/shower-of-sparks-3115784__340.jpg https://cdn.pixabay.com/photo/2017/09/06/20/35/massage-2722936__340.jpg https://cdn.pixabay.com/photo/2018/07/01/20/01/mercedes-3510327__340.jpg https://cdn.pixabay.com/photo/2018/06/28/15/23/soft-fruits-3504149__340.jpg https://cdn.pixabay.com/photo/2018/06/28/17/02/water-lily-3504363__340.jpg https://cdn.pixabay.com/photo/2018/06/29/01/47/piano-3505109__340.jpg 进阶 动态数据爬取 占位 百度地图数据爬取 占位 微博数据爬取实战 占位 球探网数据爬取 占位 未完待续。。。","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://chiang97912.github.io/categories/%E7%88%AC%E8%99%AB/"}],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://chiang97912.github.io/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"自然语言处理入门实践","slug":"自然语言处理入门实践","date":"2017-11-16T09:20:51.000Z","updated":"2025-12-13T12:16:52.857Z","comments":true,"path":"2017/11/16/自然语言处理入门实践/","permalink":"http://chiang97912.github.io/2017/11/16/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%85%A5%E9%97%A8%E5%AE%9E%E8%B7%B5/","excerpt":"自然语言处理入门实践 人工智能领域中有两个主流方向即机器视觉（CV）和自然语言处理（NLP）。其中自然语言处理（英语：Natural Language Processing，缩写作 NLP）是人工智能和语言学领域的分支学科。自然语言处理领域主要探讨如何处理及运用自然语言。自然语言处理包括多方面和步骤，基本有认知、理解、生成等部分。下面介绍一些NLP的基础知识。","text":"自然语言处理入门实践 人工智能领域中有两个主流方向即机器视觉（CV）和自然语言处理（NLP）。其中自然语言处理（英语：Natural Language Processing，缩写作 NLP）是人工智能和语言学领域的分支学科。自然语言处理领域主要探讨如何处理及运用自然语言。自然语言处理包括多方面和步骤，基本有认知、理解、生成等部分。下面介绍一些NLP的基础知识。 分词 提到自然语言处理当然不得不提大名鼎鼎的自然语言处理工具包NLTK了。NLTK在国外很流行，它提供包括英文分词、词性标注、命名实体的识别等等自然语言处理中的基本操作。如果各位想体验NLTK你可以通过pip install nltk来安装nltk。 ​ 但是nltk只支持英文分词，如果想对中文进行切词还得另谋他法。英文分词很方便，直接按照空格分割字符串就可以切出token字串，但是汉语是没有使用空格来分割词语。所以我给大家提供了一下几种汉语分词解决方案。 使用斯坦福大学分词器+nltk包，完美解决nltk对汉语的兼容。 使用jieba分词。 使用哈工大语言技术平台的python封装包pyltp 本人经常用的是jieba分词工具和哈工大的分词器pyltp。首先jieba分词工具分词的速度非常快，这一点可谓是完胜pyltp，而且jieba分词的准确率也和pyltp非常接近。但是jieba分词工具终究只是一个分词工具。它不支持一些高级的功能比如命名实体识别，语义角色标注等。所以如果只需要分词而不需要命名实体识别那就尽量使用jieba分词，有其它需求再使用pyltp。 你可以通过pip安装jieba和pyltp，jieba的安装非常简单，使用一个pip安装就可以了。但是pyltp的安装就没那么容易了，你除了需要安装pyltp还需要下载一个数据包，这个数据包提供了分词需要的一些词典。为了下载这个数据包你需要去Pyltp在Github主页下载整个项目，然后把项目中的ltp_data文件夹存放到一个固定位置方便以后调用。另外pyltp的具体调用方法可以前往Pyltp的官网学习。jieba分词的调用方法可以直接前往它在Github主页查看。 下面我简单介绍一下jieba分词工具的使用： 代码： # coding:utf-8 import jieba text = &quot;青年一代有理想、有本领、有担当，国家就有前途，民族就有希望。&quot; tokens = jieba.lcut(text) print(tokens) 运行结果： Building prefix dict from the default dictionary ... Loading model from cache C:\\temp\\jieba.cache Loading model cost 1.020 seconds. Prefix dict has been built succesfully. ['青年一代', '有', '理想', '、', '有', '本领', '、', '有', '担当', '，', '国家', '就', '有', '前途', '，', '民族', '就', '有', '希望', '。'] 停用词 什么是停用词呢？停用词就是在信息检索中，为节省存储空间和提高搜索效率，在处理自然语言数据（或文本）之前或之后会自动过滤掉某些字或词，这些字或词即被称为停用词（Stop Words）。 这些停用词都是人工输入、非自动化生成的，生成后的停用词会形成一个停用词表。但是，并没有一个明确的停用词表能够适用于所有的工具。甚至有一些工具是明确地避免使用停用词来支持短语搜索的。 简单来说就是对我们处理自然语言过程中没有帮助的词汇，例如“哈哈”、“不仅”、“如果”等。这些词蕴含的信息很少，所以在NLP中我们通常会将它们从分词结果中去掉。 那么怎么获取停用词表呢？首先你可以谷歌一下，网上有很多停用词表。不过如果你想为了方便的话可以使用我提供给你的停用词表。https://pan.baidu.com/s/1o8VFnTK 说明： 停用词表每一行就是一个停用词且该停用词表即包括标点符号也包括普通停用词。 那么有怎么去除停用词呢？ 你可以通过下面代码加载停用词，其中stopwords.txt就是停用词文件。 stopwords = [line.strip() for line in open(&#x27;stopwords.txt&#x27;, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;).readlines()] TF-IDF tf-idf（英语：term frequency–inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。tf-idf是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。tf-idf加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了tf-idf以外，互联网上的搜索引擎还会使用基于链接分析的评级方法，以确定文件在搜索结果中出现的顺序。 tf是term frequency的缩写，也就是词频的意思；而idf是inverse document frequency的缩写，也就是逆词频的意思。 我大概解释一下，通常一个句子中不同的词蕴含着不同的信息，且他们的重要性是不相同的。通常一个词语在一个文本中出现的次数越高说明它就越重要，反之越不重要。这是有些细心的读者就发现了，我们前面介绍的那些停用词怎么解释，例如“不仅”，它在一个句子中出现的顺序其实也不低呀，如果我们认为它很重要的话，那么就大错特错了，因为它是不包含任何信息的。所以我们此时就引入下面一个概念，也就是逆词频。通常在多个文本中，我们认为当一个词在同一篇文本中出现的次数越高那么它就越重要，可是如果它即出现在本篇文档中又在其他文档中的频次很高，那么我们就认为它不那么重要了。我们需要降低它的重要程度。 对于在某一特定文件里的词语来说，它的重要性可表示为： tfij=ni,j∑knk,jtf_{ij}=\\frac{n_{i,j}}{\\sum_kn_{k,j}} tfij​=∑k​nk,j​ni,j​​ 逆向文件频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到： idfi=log⁡∣D∣∣j:ti∈dj∣idf_i=\\log \\frac{|D|}{|{j:t_i\\in d_j}|} idfi​=log∣j:ti​∈dj​∣∣D∣​ 最后计算综合词频和逆词频得到最终的tfidf值 tfidfi,j=tfi,j×idfitfidf_{i,j}=tf_{i,j}\\times idf_i tfidfi,j​=tfi,j​×idfi​ Word2vec One-hot 自然语言在计算机中是以字符或者字符串形式存在的，但是作为一个基于统计的机器学习系统，我们的运算对象是数值型的数据，所以我们需要对字符型数据进行处理。 表征单词的方式是首先建立一个较大的词汇表（例如10000），然后使用one-hot的方式对每个单词进行编码。例如单词Man，Woman，King，Queen，Apple，Orange分别出现在词汇表的第5391，9853，4914，7157，456，6257的位置，则它们分别用O5391,O9853,O4914,O7157,O456,O6257表示。其中O5391表示一个维度为10000，第5391维度的值为1其余位置值全为0的向量。 one-hot表征单词的方法最大的缺点就是每个单词都是独立的、正交的，无法知道不同单词之间的相似程度。例如Apple和Orange都是水果，词性相近，但是单从one-hot编码上来看，内积为零，无法知道二者的相似性。在NLP中，我们更希望能掌握不同单词之间的相似程度。 词嵌入 为了解决One-hot向量的缺点我们可以使用特征表征（Featurized representation）的方法对每个单词进行编码。也就是使用一个特征向量表征单词，特征向量的每个元素都是对该单词某一特征的量化描述，量化范围可以是[-1,1]之间。特征表征的例子如下图所示： Man(5391) Woman(9853) King(4914) Queen(7157) Apple(456) Orange(6257) Gender -1 1 -0.95 0.97 0.00 0.01 Royal 0.01 0.02 0.93 0.95 -0.01 0.00 Age 0.03 0.02 0.7 0.65 0.03 -0.02 Food 0.09 0.01 0.02 0.01 0.95 0.97 … … … … … … … 特征向量的长度依情况而定，特征元素越多则对单词表征得越全面。这里的特征向量长度设定为300。使用特征表征之后，词汇表中的每个单词都可以使用对应的300 x 1的向量来表示，该向量的每个元素表示该单词对应的某个特征值。每个单词用e+词汇表索引的方式标记，例如e5391, e9853, e4914, e7157, e456, e6257。 特征表征的优点是根据特征向量能清晰知道不同单词之间的相似程度，例如Apple和Orange之间的相似度较高，很可能属于同一类别。这种单词“类别”化的方式，大大提高了有限词汇量的泛化能力。这种特征化单词的操作被称为Word Embeddings，即单词嵌入。 这里特征向量的每个特征元素含义是具体的，对应到实际特征，例如性别、年龄等。而在实际应用中，特征向量很多特征元素并不一定对应到有物理意义的特征，是比较抽象的。但是，这并不影响对每个单词的有效表征，同样能比较不同单词之间的相似性。 学习词嵌入 我们可以通过构建自然语言模型（神经网络），运用梯度下降算法得到embedding。举个简单的例子，输入样本是下面这句话： I want a glass of orange ——. 通过这句话的前6个单词，预测最后的单词“juice”。 为了让神经网络输入层数目固定，可以选择只取预测单词的前4个单词作为输入，例如该句中只选择“a glass of orange”四个单词作为输入。当然，这里的4是超参数，可调。 一般地，我们把输入叫做context，输出叫做target。对应到上面这句话里： context: a glass of orange target: juice 关于context的选择有多种方法： target前n个单词或后n个单词，n可调 target前1个单词 target附近某1个单词（Skip-Gram） 关于context和target的选择，比较流行的模型有Skip-Gram和Cbow。 情感分析 pass 机器翻译 pass Attention机制 pass","categories":[{"name":"深度学习","slug":"深度学习","permalink":"http://chiang97912.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"NLP","slug":"深度学习/NLP","permalink":"http://chiang97912.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/"}],"tags":[{"name":"NLP","slug":"NLP","permalink":"http://chiang97912.github.io/tags/NLP/"}]},{"title":"二级域名指向网站子目录的配置","slug":"二级域名指向网站子目录的配置","date":"2017-11-11T03:52:00.000Z","updated":"2025-12-13T12:18:13.028Z","comments":true,"path":"2017/11/11/二级域名指向网站子目录的配置/","permalink":"http://chiang97912.github.io/2017/11/11/%E4%BA%8C%E7%BA%A7%E5%9F%9F%E5%90%8D%E6%8C%87%E5%90%91%E7%BD%91%E7%AB%99%E5%AD%90%E7%9B%AE%E5%BD%95%E7%9A%84%E9%85%8D%E7%BD%AE/","excerpt":"第一步：添加二级域名 首先声明的是，我的wordpress保存在网站的根目录下的wordpress子目录，所以我现在想把wordpress目录指向二级域名blog.figurinn.xyz.经过多次尝试终于成功了。","text":"第一步：添加二级域名 首先声明的是，我的wordpress保存在网站的根目录下的wordpress子目录，所以我现在想把wordpress目录指向二级域名blog.figurinn.xyz.经过多次尝试终于成功了。 首先，你需要在你申请域名的服务商那里新增二级域名。如果你是在阿里云申请的域名，那么你需要到阿里云的DNS解析里面找到新增DNS的按钮，纪录类型选择A，然后主机纪录填你想要申请的名称，例如我的就填blog，记录值就是你服务器的IP地址，其他的默认就好了。 第二步：服务器设置 这一步你需要在你的网站根目录下增加一个名叫.htaccess的文件，然后填入一下的内容 &lt;IfModule mod_rewrite.c&gt; RewriteEngine On RewriteCond %{HTTP_HOST} ^blog.figurinn.xyz$ RewriteCond %{REQUEST_URI} !^/wordpress/ RewriteRule ^(.*)$ /wordpress/$1?Rewrite [L,QSA] &lt;/IfModule&gt; 上面的内容是根据我的网站设定的，你可以根据你的需求进行更改 如果你此时访问的二级域名不成功的话，很可能你没有开启服务器url重写功能。 如果这样，你可以按一下步骤打开服务器的url重写功能(以我的ubuntu服务器为例子) 首先确保/etc/apache2/mods-enabled/rewrite.load文件中的 LoadModule rewrite_module /usr/lib/apache2/modules/mod_ rewrite.so 没有被注释掉（如果有#号表示就被注释），如果被注释了取消注释（只需要删除前面的#）即可，然后vim保存退出。 然后编辑/etc/apache2/apache2.conf 文件，找到 &lt;Directory /var/www/&gt; Options Indexes FollowSymLinks AllowOverride None Require all granted &lt;/Directory&gt; 然后将AllowOverride None中的None改为All 到此为止二级域名指向服务器子目录就成功了 一些问题及解决办法 我通过blog.figurinn.xyz设置二级域名指向子目录成功后，该二级域名可以访问，可是发现当访问某一篇文章时，却发现文章的前缀还是www.figurinn.xyz/wordpress，后来发现是wordpress设置的问题 如果你也有同样的问题，你可以进入wordpress后台，然后点击设置，你会发现有“WordPress地址”、“站点地址”，如果你希望访问wordpress中某一篇文章时url的前缀也是你设置的二级域名的话，你只需要将上文提到的两个设置选项设置为你的二级域名即可。","categories":[{"name":"服务器配置","slug":"服务器配置","permalink":"http://chiang97912.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/"}],"tags":[{"name":"服务器配置","slug":"服务器配置","permalink":"http://chiang97912.github.io/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/"}]},{"title":"Python3连接MySQL数据库","slug":"Python3连接MySQL数据库","date":"2017-09-17T05:20:25.000Z","updated":"2025-12-13T12:15:47.754Z","comments":true,"path":"2017/09/17/Python3连接MySQL数据库/","permalink":"http://chiang97912.github.io/2017/09/17/Python3%E8%BF%9E%E6%8E%A5MySQL%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"Python2连接数据库一般使用mysqldb库，而到了Python3已经完全不支持mysqldb库了，所以要使用其他的替代库。而Django中又使用了mysqldb库，所以我们的解决方案就是使用pymysql库来模拟mysqldb库。","text":"Python2连接数据库一般使用mysqldb库，而到了Python3已经完全不支持mysqldb库了，所以要使用其他的替代库。而Django中又使用了mysqldb库，所以我们的解决方案就是使用pymysql库来模拟mysqldb库。 先在python3中安装好pymysql库。然后在Django项目的app包的__init__.py文件中添加如下语句： import pymysql pymysql.install_as_MySQLdb() 这样我们就可以在Django项目中调用mysql数据库了。","categories":[{"name":"代码","slug":"代码","permalink":"http://chiang97912.github.io/categories/%E4%BB%A3%E7%A0%81/"}],"tags":[{"name":"mysql","slug":"mysql","permalink":"http://chiang97912.github.io/tags/mysql/"},{"name":"python","slug":"python","permalink":"http://chiang97912.github.io/tags/python/"},{"name":"django","slug":"django","permalink":"http://chiang97912.github.io/tags/django/"},{"name":"代码","slug":"代码","permalink":"http://chiang97912.github.io/tags/%E4%BB%A3%E7%A0%81/"}]},{"title":"谈谈我对于维度的理解","slug":"谈谈我对于维度的理解","date":"2017-08-25T08:14:48.000Z","updated":"2025-12-16T13:56:52.742Z","comments":true,"path":"2017/08/25/谈谈我对于维度的理解/","permalink":"http://chiang97912.github.io/2017/08/25/%E8%B0%88%E8%B0%88%E6%88%91%E5%AF%B9%E4%BA%8E%E7%BB%B4%E5%BA%A6%E7%9A%84%E7%90%86%E8%A7%A3/","excerpt":"前不久和朋友聊到了维度的话题，朋友给我讲了他在一本杂志上看到的有关空间维度的文章。身处在三维空间的我们是很难理解四维空间的，我们人类理解四维空间就好比二维空间的生物理解三维空间的生物一样困难。假如蚂蚁是一种二维生物，它的活动范围只能是地面（平面），而苍蝇就是生活在三维空间的生物，它可以向上下、左右、前后中的任意一个方向移动。假设某一天苍蝇突然从空中落下停到地面休息了几秒，这时二维生物蚂蚁发现了三维生物苍蝇，但是苍蝇转瞬即逝。这种现象其实有点类似人类发现不明飞行物。我们假设外星人生活在四维空间，它可以在时间轴上任意移动，而我们人类只能从某个时间点出发并且沿着时间增大的方向匀速移动。相比于四维空间的生物能够在时间轴上以任意方向任意速度移动而言，我们三维空间的生物可能显得有点笨拙。经过这样的解释，那么经常出现在新闻版面中的不明飞行物转瞬即逝的现象就能够很好解释了。","text":"前不久和朋友聊到了维度的话题，朋友给我讲了他在一本杂志上看到的有关空间维度的文章。身处在三维空间的我们是很难理解四维空间的，我们人类理解四维空间就好比二维空间的生物理解三维空间的生物一样困难。假如蚂蚁是一种二维生物，它的活动范围只能是地面（平面），而苍蝇就是生活在三维空间的生物，它可以向上下、左右、前后中的任意一个方向移动。假设某一天苍蝇突然从空中落下停到地面休息了几秒，这时二维生物蚂蚁发现了三维生物苍蝇，但是苍蝇转瞬即逝。这种现象其实有点类似人类发现不明飞行物。我们假设外星人生活在四维空间，它可以在时间轴上任意移动，而我们人类只能从某个时间点出发并且沿着时间增大的方向匀速移动。相比于四维空间的生物能够在时间轴上以任意方向任意速度移动而言，我们三维空间的生物可能显得有点笨拙。经过这样的解释，那么经常出现在新闻版面中的不明飞行物转瞬即逝的现象就能够很好解释了。 关于维度的认知霍金曾在他的著作《时间简史》中做出过这样的理解：维度可以比喻成驾驶火车，一维世界的火车只能笔直的往前行驶，二维的火车除了可以向前行驶之外还可以转弯，而三维世界的火车除了具有以上的特质外，遇到有坡度的山还可以向上行驶。如果存在四维空间的火车，那它就可以做时间旅行了，所以我认为外星人如果存在的话应该就存在于四维空间。","categories":[{"name":"随笔","slug":"随笔","permalink":"http://chiang97912.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"物理","slug":"物理","permalink":"http://chiang97912.github.io/tags/%E7%89%A9%E7%90%86/"},{"name":"空间维度","slug":"空间维度","permalink":"http://chiang97912.github.io/tags/%E7%A9%BA%E9%97%B4%E7%BB%B4%E5%BA%A6/"}]}],"categories":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/categories/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"深度学习","slug":"深度学习","permalink":"http://chiang97912.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"http://chiang97912.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"服务器配置","slug":"服务器配置","permalink":"http://chiang97912.github.io/categories/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/"},{"name":"python","slug":"python","permalink":"http://chiang97912.github.io/categories/python/"},{"name":"VIM","slug":"VIM","permalink":"http://chiang97912.github.io/categories/VIM/"},{"name":"NLP","slug":"深度学习/NLP","permalink":"http://chiang97912.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/"},{"name":"Git","slug":"Git","permalink":"http://chiang97912.github.io/categories/Git/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://chiang97912.github.io/categories/tensorflow/"},{"name":"PHP","slug":"PHP","permalink":"http://chiang97912.github.io/categories/PHP/"},{"name":"算法","slug":"算法","permalink":"http://chiang97912.github.io/categories/%E7%AE%97%E6%B3%95/"},{"name":"代码","slug":"代码","permalink":"http://chiang97912.github.io/categories/%E4%BB%A3%E7%A0%81/"},{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/categories/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"爬虫","slug":"爬虫","permalink":"http://chiang97912.github.io/categories/%E7%88%AC%E8%99%AB/"},{"name":"随笔","slug":"随笔","permalink":"http://chiang97912.github.io/categories/%E9%9A%8F%E7%AC%94/"}],"tags":[{"name":"大模型","slug":"大模型","permalink":"http://chiang97912.github.io/tags/%E5%A4%A7%E6%A8%A1%E5%9E%8B/"},{"name":"位置编码","slug":"位置编码","permalink":"http://chiang97912.github.io/tags/%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"},{"name":"相对位置编码","slug":"相对位置编码","permalink":"http://chiang97912.github.io/tags/%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"},{"name":"旋转位置编码","slug":"旋转位置编码","permalink":"http://chiang97912.github.io/tags/%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/"},{"name":"RoPE","slug":"RoPE","permalink":"http://chiang97912.github.io/tags/RoPE/"},{"name":"强化学习","slug":"强化学习","permalink":"http://chiang97912.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"},{"name":"PPO","slug":"PPO","permalink":"http://chiang97912.github.io/tags/PPO/"},{"name":"GRPO","slug":"GRPO","permalink":"http://chiang97912.github.io/tags/GRPO/"},{"name":"DPO","slug":"DPO","permalink":"http://chiang97912.github.io/tags/DPO/"},{"name":"LLM","slug":"LLM","permalink":"http://chiang97912.github.io/tags/LLM/"},{"name":"激活函数","slug":"激活函数","permalink":"http://chiang97912.github.io/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"},{"name":"ReLU","slug":"ReLU","permalink":"http://chiang97912.github.io/tags/ReLU/"},{"name":"GeLU","slug":"GeLU","permalink":"http://chiang97912.github.io/tags/GeLU/"},{"name":"GLU","slug":"GLU","permalink":"http://chiang97912.github.io/tags/GLU/"},{"name":"SwiGLU","slug":"SwiGLU","permalink":"http://chiang97912.github.io/tags/SwiGLU/"},{"name":"归一化","slug":"归一化","permalink":"http://chiang97912.github.io/tags/%E5%BD%92%E4%B8%80%E5%8C%96/"},{"name":"LayerNorm","slug":"LayerNorm","permalink":"http://chiang97912.github.io/tags/LayerNorm/"},{"name":"RMSNorm","slug":"RMSNorm","permalink":"http://chiang97912.github.io/tags/RMSNorm/"},{"name":"Qwen","slug":"Qwen","permalink":"http://chiang97912.github.io/tags/Qwen/"},{"name":"深度学习","slug":"深度学习","permalink":"http://chiang97912.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"},{"name":"MoE","slug":"MoE","permalink":"http://chiang97912.github.io/tags/MoE/"},{"name":"混合专家模型","slug":"混合专家模型","permalink":"http://chiang97912.github.io/tags/%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B/"},{"name":"Transformer","slug":"Transformer","permalink":"http://chiang97912.github.io/tags/Transformer/"},{"name":"BERT","slug":"BERT","permalink":"http://chiang97912.github.io/tags/BERT/"},{"name":"GPT","slug":"GPT","permalink":"http://chiang97912.github.io/tags/GPT/"},{"name":"模型评估","slug":"模型评估","permalink":"http://chiang97912.github.io/tags/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"},{"name":"Accuracy","slug":"Accuracy","permalink":"http://chiang97912.github.io/tags/Accuracy/"},{"name":"Precision","slug":"Precision","permalink":"http://chiang97912.github.io/tags/Precision/"},{"name":"Recall","slug":"Recall","permalink":"http://chiang97912.github.io/tags/Recall/"},{"name":"F1","slug":"F1","permalink":"http://chiang97912.github.io/tags/F1/"},{"name":"ROC","slug":"ROC","permalink":"http://chiang97912.github.io/tags/ROC/"},{"name":"AUC","slug":"AUC","permalink":"http://chiang97912.github.io/tags/AUC/"},{"name":"MAP","slug":"MAP","permalink":"http://chiang97912.github.io/tags/MAP/"},{"name":"MRR","slug":"MRR","permalink":"http://chiang97912.github.io/tags/MRR/"},{"name":"NDCG","slug":"NDCG","permalink":"http://chiang97912.github.io/tags/NDCG/"},{"name":"Git","slug":"Git","permalink":"http://chiang97912.github.io/tags/Git/"},{"name":"Gogs","slug":"Gogs","permalink":"http://chiang97912.github.io/tags/Gogs/"},{"name":"Gitea","slug":"Gitea","permalink":"http://chiang97912.github.io/tags/Gitea/"},{"name":"python","slug":"python","permalink":"http://chiang97912.github.io/tags/python/"},{"name":"Pyinstaller","slug":"Pyinstaller","permalink":"http://chiang97912.github.io/tags/Pyinstaller/"},{"name":"python程序打包","slug":"python程序打包","permalink":"http://chiang97912.github.io/tags/python%E7%A8%8B%E5%BA%8F%E6%89%93%E5%8C%85/"},{"name":"pipenv","slug":"pipenv","permalink":"http://chiang97912.github.io/tags/pipenv/"},{"name":"virtualenv","slug":"virtualenv","permalink":"http://chiang97912.github.io/tags/virtualenv/"},{"name":"Python","slug":"Python","permalink":"http://chiang97912.github.io/tags/Python/"},{"name":"VIM","slug":"VIM","permalink":"http://chiang97912.github.io/tags/VIM/"},{"name":"C/C++","slug":"C-C","permalink":"http://chiang97912.github.io/tags/C-C/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://chiang97912.github.io/tags/tensorflow/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"http://chiang97912.github.io/tags/Deep-Learning/"},{"name":"NLP","slug":"NLP","permalink":"http://chiang97912.github.io/tags/NLP/"},{"name":"mysql","slug":"mysql","permalink":"http://chiang97912.github.io/tags/mysql/"},{"name":"sql","slug":"sql","permalink":"http://chiang97912.github.io/tags/sql/"},{"name":"lamp","slug":"lamp","permalink":"http://chiang97912.github.io/tags/lamp/"},{"name":"linux","slug":"linux","permalink":"http://chiang97912.github.io/tags/linux/"},{"name":"PHP","slug":"PHP","permalink":"http://chiang97912.github.io/tags/PHP/"},{"name":"算法","slug":"算法","permalink":"http://chiang97912.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"数学建模","slug":"数学建模","permalink":"http://chiang97912.github.io/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"代码","slug":"代码","permalink":"http://chiang97912.github.io/tags/%E4%BB%A3%E7%A0%81/"},{"name":"模糊综合评价法","slug":"模糊综合评价法","permalink":"http://chiang97912.github.io/tags/%E6%A8%A1%E7%B3%8A%E7%BB%BC%E5%90%88%E8%AF%84%E4%BB%B7%E6%B3%95/"},{"name":"灰色关联分析","slug":"灰色关联分析","permalink":"http://chiang97912.github.io/tags/%E7%81%B0%E8%89%B2%E5%85%B3%E8%81%94%E5%88%86%E6%9E%90/"},{"name":"爬虫","slug":"爬虫","permalink":"http://chiang97912.github.io/tags/%E7%88%AC%E8%99%AB/"},{"name":"服务器配置","slug":"服务器配置","permalink":"http://chiang97912.github.io/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE/"},{"name":"django","slug":"django","permalink":"http://chiang97912.github.io/tags/django/"},{"name":"物理","slug":"物理","permalink":"http://chiang97912.github.io/tags/%E7%89%A9%E7%90%86/"},{"name":"空间维度","slug":"空间维度","permalink":"http://chiang97912.github.io/tags/%E7%A9%BA%E9%97%B4%E7%BB%B4%E5%BA%A6/"}]}